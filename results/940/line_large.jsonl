{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)  # Remove special characters and punctuation marks", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "  ", "#", " Remove", " special", " characters", " and", " punct", "uation", " marks", "\n"], "logprobs": [-0.023689216002821922, -0.00025722055579535663, -0.0020058290101587772, -0.1281573623418808, -0.04926752299070358, -0.4472964107990265, -0.029927097260951996, -0.3847354054450989, -0.0007465674425475299, -0.01202806830406189, -3.0397906812140718e-05, -0.013603131286799908, -0.011825812980532646, -0.000179036331246607, -0.08449075371026993, -0.00019059749320149422, -0.005078036803752184, -5.3881147323409095e-05, -0.00010847456724150106, -0.015737971290946007, -2.2574832439422607, -0.0003131137927994132, -0.6578453779220581, -0.11590974777936935, -0.0064210533164441586, -0.807940661907196, -0.07614858448505402, -0.003730839118361473, -0.10118154436349869, -0.22519490122795105], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 299.37450580978395, "energy_per_token": 5.987490116195679, "time": 10.226061, "time_per_token": 0.20452122}}
{"outputs": [{"text": " re.sub(r'[^a-zA-Z0-9]+', ' ', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^", "a", "-", "z", "A", "-", "Z", "0", "-", "9", "]+", "'", ",", " '", " '", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -1.199073314666748, -0.5877083539962769, -0.00014911970356479287, -0.0002544794406276196, -0.007544240448623896, -5.61460001335945e-05, -0.0015324526466429234, -0.5383650660514832, -0.00022218143567442894, -3.158996332786046e-05, -2.7890639305114746, -0.0034963686484843493, -0.00027509720530360937, -0.15052171051502228, -0.0009246364934369922, -0.00014757021563127637, -0.005153822712600231, -9.655486064730212e-05, -0.00019798702851403505, -0.047172218561172485, -0.38338503241539], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 181.66021013832093, "energy_per_token": 3.7845877112150195, "time": 6.320167, "time_per_token": 0.13167014583333334}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)  # Remove special characters", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "  ", "#", " Remove", " special", " characters", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -2.1393115520477295, -0.00026770823751576245, -0.6644008755683899, -0.15878842771053314, -0.005243240389972925, -0.7137592434883118], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05], "finish_reason": "stop"}], "metrics": {"energy": 176.7028603878021, "energy_per_token": 3.9267302308400467, "time": 6.1106825, "time_per_token": 0.13579294444444445}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -0.30388143658638], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict<|endoftext|># Copyright 2021 Huawei Technologies Co., Ltd", "tokens": [" fre", "q", "_", "dict", "<", "|", "end", "of", "text", "|>", "#", " Copyright", " ", "2", "0", "2", "1", " H", "uawei", " Technologies", " Co", ".", ",", " Ltd", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -1.8244831562042236, -6.198863957251888e-06, -1.9073468138230965e-06, -2.9802276912960224e-06, -1.4185804502631072e-05, -0.14681214094161987, -1.2786263227462769, -3.266659736633301, -0.6799730658531189, -0.03887123987078667, -0.0003475538978818804, -0.3150072991847992, -1.802467703819275, -2.4209868907928467, -0.2259562760591507, -0.0014723424101248384, -8.904537389753386e-05, -1.6331539882230572e-05, -8.868777513271198e-05, -2.9802276912960224e-06, -0.022418983280658722], "finish_reason": "stop"}], "metrics": {"energy": 244.36930529403682, "energy_per_token": 4.006054185148145, "time": 7.3647795, "time_per_token": 0.12073409016393442}}
{"outputs": [{"text": " re.sub(r'[^a-zA-Z0-9\\s]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^", "a", "-", "z", "A", "-", "Z", "0", "-", "9", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -1.199073314666748, -0.5877083539962769, -0.00014911970356479287, -0.0002544794406276196, -0.007544240448623896, -5.61460001335945e-05, -0.0015324526466429234, -0.5383650660514832, -0.00022218143567442894, -3.158996332786046e-05, -1.6791130304336548, -0.004698071163147688, -0.08223661035299301, -0.0009641766082495451, -0.00019822540343739092, -0.12347855418920517, -0.00026639728457666934, -0.004045161418616772, -7.497983460780233e-05, -0.0001778444420779124, -0.01623622328042984, -0.20162636041641235], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 187.0984479799271, "energy_per_token": 3.8183356730597366, "time": 6.526337, "time_per_token": 0.13319055102040817}}
{"outputs": [{"text": " re.sub(r'\\W+', ' ', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'\\", "W", "+'", ",", " '", " '", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -1.6769564151763916, -0.022418633103370667, -0.17651697993278503, -0.00036745471879839897, -0.03512629121541977, -0.0004857790481764823, -0.0001431601122021675, -0.0037910512182861567, -5.113947918289341e-05, -7.629103492945433e-05, -0.018660351634025574, -0.6423712968826294], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05], "finish_reason": "stop"}], "metrics": {"energy": 157.5885842895508, "energy_per_token": 4.377460674709744, "time": 5.267671, "time_per_token": 0.14632419444444444}}
{"outputs": [{"text": " re.sub(r'[^A-Za-z0-9 ]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^", "A", "-", "Z", "a", "-", "z", "0", "-", "9", " ]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -1.199073314666748, -0.9374302625656128, -0.0002315968304174021, -0.009623917751014233, -0.0006267252028919756, -8.4638240878121e-06, -2.825220326485578e-05, -0.3252539336681366, -0.0005482124397531152, -2.4437606043647975e-05, -1.0646443367004395, -0.3472878634929657, -0.00015078838623594493, -0.08354729413986206, -0.00028355870745144784, -0.0028903395868837833, -8.523101132595912e-05, -0.0005783553933724761, -0.011083230376243591, -0.1494341790676117], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 177.45340890789032, "energy_per_token": 3.7756044448487303, "time": 6.323872, "time_per_token": 0.13455046808510637}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -0.30388143658638], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 171.22002960395815, "energy_per_token": 4.17609828302337, "time": 5.682249, "time_per_token": 0.13859143902439022}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -0.30388143658638], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 167.69229274082184, "energy_per_token": 4.09005592050785, "time": 5.6889906, "time_per_token": 0.13875586829268294}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -0.30388143658638], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(cleaned_str)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-2.7800307273864746, -0.00012838016846217215, -0.0016105312388390303, -0.0034310303162783384, -0.005301701836287975, -0.0001851148990681395, -0.00020275443966966122, -2.8013790142722428e-05, -0.0012741313548758626, -0.004142395220696926, -0.00016962042718660086, -0.0003134713042527437, -0.011676774360239506, -0.0024901593569666147], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.03845377638936043, -0.0033945576287806034, -0.008408382534980774, -0.0003672163875307888, -0.0008690156391821802], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02140853926539421, -2.3841855067985307e-07, -2.8609820219571702e-05, -4.815939246327616e-05, -0.4663054347038269], "finish_reason": "stop"}], "metrics": {"energy": 195.24026866817474, "energy_per_token": 4.33867263707055, "time": 6.3207273, "time_per_token": 0.14046060666666665}}
{"outputs": [{"text": " re.sub('[^a-zA-Z]', ' ', input_str)", "tokens": [" re", ".", "sub", "('", "[", "^", "a", "-", "z", "A", "-", "Z", "]", "'", ",", " '", " '", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -2.3294432163238525, -0.23118238151073456, -0.10565362870693207, -0.7664939165115356, -0.00026842328952625394, -0.00021145492792129517, -0.0037569671403616667, -3.290122185717337e-05, -0.0026534132193773985, -1.9943374395370483, -0.011132745072245598, -0.0006623458466492593, -0.10798150300979614, -0.000653530121780932, -0.0003510097449179739, -0.006906446535140276, -6.711257447022945e-05, -6.210611172718927e-05, -0.006177142728120089, -0.22296710312366486], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 169.12990719318393, "energy_per_token": 3.9332536556554403, "time": 5.904092, "time_per_token": 0.13730446511627908}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -0.30388143658638], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(cleaned_str)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-2.7800307273864746, -0.00012838016846217215, -0.0016105312388390303, -0.0034310303162783384, -0.005301701836287975, -0.0001851148990681395, -0.00020275443966966122, -2.8013790142722428e-05, -0.0012741313548758626, -0.004142395220696926, -0.00016962042718660086, -0.0003134713042527437, -0.011676774360239506, -0.0024901593569666147], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.03845377638936043, -0.0033945576287806034, -0.008408382534980774, -0.0003672163875307888, -0.0008690156391821802], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02140853926539421, -2.3841855067985307e-07, -2.8609820219571702e-05, -4.815939246327616e-05, -0.4663054347038269], "finish_reason": "stop"}], "metrics": {"energy": 186.2390285215378, "energy_per_token": 4.1386450782563955, "time": 6.3279443, "time_per_token": 0.14062098444444446}}
{"outputs": [{"text": " re.sub(r'\\W+', ' ', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'\\", "W", "+'", ",", " '", " '", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -1.6769564151763916, -0.022418633103370667, -0.17651697993278503, -0.00036745471879839897, -0.03512629121541977, -0.0004857790481764823, -0.0001431601122021675, -0.0037910512182861567, -5.113947918289341e-05, -7.629103492945433e-05, -0.018660351634025574, -0.6423712968826294], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 162.36535291290284, "energy_per_token": 4.388252781429807, "time": 5.4819365, "time_per_token": 0.14816044594594593}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -0.30388143658638], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict<|endoftext|># This code is so you can run the samples without installing the package", "tokens": [" fre", "q", "_", "dict", "<", "|", "end", "of", "text", "|>", "#", " This", " code", " is", " so", " you", " can", " run", " the", " samples", " without", " installing", " the", " package", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -1.8244831562042236, -6.198863957251888e-06, -1.9073468138230965e-06, -2.9802276912960224e-06, -1.4185804502631072e-05, -0.14681214094161987, -1.2786263227462769, -3.724331855773926, -1.7778977155685425, -0.3632614314556122, -3.024362325668335, -0.0007728926721028984, -3.421248038648628e-05, -0.0190336462110281, -4.8874615458771586e-05, -0.006161266937851906, -0.0020137999672442675, -0.0005455912905745208, -0.00015031162183731794, -0.0011965028243139386, -0.00163433444686234], "finish_reason": "stop"}], "metrics": {"energy": 242.082826040268, "energy_per_token": 3.968570918692918, "time": 7.3721223, "time_per_token": 0.12085446393442623}}
{"outputs": [{"text": " re.sub(r\"[^\\w\\s]\", '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "\"", "[", "^\\", "w", "\\", "s", "]\"", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -1.8100244998931885, -0.09824926406145096, -0.9836709499359131, -0.0009502424509264529, -0.015332569368183613, -0.0004017737810499966, -0.009927777573466301, -0.0001463782973587513, -2.142855644226074, -0.00039593485416844487, -0.0070340619422495365, -6.735097849741578e-05, -8.201262971851975e-05, -0.013886621221899986, -0.3303817808628082], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict<|endoftext|>import re", "tokens": [" fre", "q", "_", "dict", "<", "|", "end", "of", "text", "|>", "import", " re", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -1.8244831562042236, -6.198863957251888e-06, -1.9073468138230965e-06, -2.9802276912960224e-06, -1.4185804502631072e-05, -0.14681214094161987, -1.136198878288269, -1.4905208349227905, -0.011777039617300034], "finish_reason": "stop"}], "metrics": {"energy": 202.44327994728084, "energy_per_token": 4.217568332235017, "time": 6.305943, "time_per_token": 0.1313738125}}
{"outputs": [{"text": " re.sub(r\"[^\\w\\s]\", \" \", input_str).lower()", "tokens": [" re", ".", "sub", "(", "r", "\"", "[", "^\\", "w", "\\", "s", "]\"", ",", " \"", " \"", ",", " input", "_", "str", ")", ".", "lower", "()", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -1.8100244998931885, -0.09824926406145096, -0.9836709499359131, -0.0009502424509264529, -0.015332569368183613, -0.0004017737810499966, -0.009927777573466301, -0.0001463782973587513, -0.15701186656951904, -2.6573474407196045, -0.0004633783537428826, -0.004492903593927622, -9.107174992095679e-05, -6.329813186312094e-05, -0.0117806913331151, -2.147152900695801, -1.836012363433838, -0.00016735584358684719, -0.2679479718208313], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05], "finish_reason": "stop"}], "metrics": {"energy": 185.59120970344543, "energy_per_token": 4.316074644266172, "time": 5.896792, "time_per_token": 0.1371346976744186}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -0.30388143658638], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(cleaned_str)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-2.7800307273864746, -0.00012838016846217215, -0.0016105312388390303, -0.0034310303162783384, -0.005301701836287975, -0.0001851148990681395, -0.00020275443966966122, -2.8013790142722428e-05, -0.0012741313548758626, -0.004142395220696926, -0.00016962042718660086, -0.0003134713042527437, -0.011676774360239506, -0.0024901593569666147], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.03845377638936043, -0.0033945576287806034, -0.008408382534980774, -0.0003672163875307888, -0.0008690156391821802], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict"], "logprobs": [-0.02140853926539421, -2.3841855067985307e-07, -2.8609820219571702e-05, -4.815939246327616e-05], "finish_reason": "stop"}], "metrics": {"energy": 194.3729787311554, "energy_per_token": 4.41756769843535, "time": 6.317521, "time_per_token": 0.14358002272727274}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)  # Remove special characters and punctuation marks", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "  ", "#", " Remove", " special", " characters", " and", " punct", "uation", " marks", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -2.1393115520477295, -0.00026770823751576245, -0.6644008755683899, -0.15878842771053314, -0.005243240389972925, -0.9585445523262024, -0.05236160010099411, -0.0036499565467238426, -0.11578595638275146, -0.23245030641555786], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 241.63890450859077, "energy_per_token": 4.832778090171815, "time": 6.738865, "time_per_token": 0.1347773}}
{"outputs": [{"text": " re.sub(r'\\W+', ' ', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'\\", "W", "+'", ",", " '", " '", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -1.6769564151763916, -0.022418633103370667, -0.17651697993278503, -0.00036745471879839897, -0.03512629121541977, -0.0004857790481764823, -0.0001431601122021675, -0.0037910512182861567, -5.113947918289341e-05, -7.629103492945433e-05, -0.018660351634025574, -0.6423712968826294], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 157.32299945735934, "energy_per_token": 4.251972958307009, "time": 5.4798007, "time_per_token": 0.14810272162162164}}
{"outputs": [{"text": " re.sub(r'\\W', ' ', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'\\", "W", "'", ",", " '", " '", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -1.6769564151763916, -0.022418633103370667, -1.9458575248718262, -0.0002535260282456875, -0.03011809103190899, -0.0003846143954433501, -0.0003591130953282118, -0.00409490754827857, -3.7788631743751466e-05, -4.541770613286644e-05, -0.02362576685845852, -0.4313729703426361], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(cleaned_str)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-2.7800307273864746, -0.00012838016846217215, -0.0016105312388390303, -0.0034310303162783384, -0.005301701836287975, -0.0001851148990681395, -0.00020275443966966122, -2.8013790142722428e-05, -0.0012741313548758626, -0.004142395220696926, -0.00016962042718660086, -0.0003134713042527437, -0.011676774360239506, -0.0024901593569666147], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.03845377638936043, -0.0033945576287806034, -0.008408382534980774, -0.0003672163875307888, -0.0008690156391821802], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02140853926539421, -2.3841855067985307e-07, -2.8609820219571702e-05, -4.815939246327616e-05, -0.4663054347038269], "finish_reason": "stop"}], "metrics": {"energy": 187.00551893997195, "energy_per_token": 4.561110218048096, "time": 6.1112556, "time_per_token": 0.14905501463414633}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -0.30388143658638], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 193.75552731704707, "energy_per_token": 4.725744568708465, "time": 5.7061787, "time_per_token": 0.13917509024390243}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)  # Remove special characters and punctuation marks using regex", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "  ", "#", " Remove", " special", " characters", " and", " punct", "uation", " marks", " using", " regex", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -2.1393115520477295, -0.00026770823751576245, -0.6644008755683899, -0.15878842771053314, -0.005243240389972925, -0.9585445523262024, -0.05236160010099411, -0.0036499565467238426, -0.11578595638275146, -3.022592306137085, -0.02262808009982109, -0.11116120219230652], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 215.87860672664655, "energy_per_token": 4.151511667820126, "time": 6.738074, "time_per_token": 0.12957834615384617}}
{"outputs": [{"text": " re.sub(r\"[^a-zA-Z0-9]\", \" \", input_str)", "tokens": [" re", ".", "sub", "(", "r", "\"", "[", "^", "a", "-", "z", "A", "-", "Z", "0", "-", "9", "]\"", ",", " \"", " \"", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -1.8100244998931885, -0.09824926406145096, -0.5014379024505615, -0.14326457679271698, -0.00021419614495243877, -0.0003149013500660658, -0.0051516881212592125, -5.125868119648658e-05, -0.0018067717319354415, -0.20169593393802643, -0.0002212279650848359, -4.815939246327616e-05, -0.8235354423522949, -0.00013433984713628888, -0.04685012251138687, -0.12011406570672989, -0.00028534632292576134, -0.005463311914354563, -7.259582343976945e-05, -0.00033563701435923576, -0.015164914540946484, -0.25943711400032043], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 183.01330090522762, "energy_per_token": 3.8939000192601623, "time": 6.330839, "time_per_token": 0.13469870212765958}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str).strip()", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", ".", "strip", "()", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -2.2151248455047607, -0.3719938099384308, -0.0005187834613025188, -0.11050070822238922], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 167.90951357746127, "energy_per_token": 3.8161253085786653, "time": 6.114569, "time_per_token": 0.1389674772727273}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -0.30388143658638], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 167.24783360671992, "energy_per_token": 4.079215453822437, "time": 5.687582, "time_per_token": 0.13872151219512194}}
{"outputs": [{"text": " re.sub(r'[^\\w\\s]', '', input_str).strip()", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " ''", ",", " input", "_", "str", ")", ".", "strip", "()", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -0.37079378962516785, -0.00086198840290308, -0.015631647780537605, -2.3483953555114567e-05, -0.01343283336609602, -0.014252417720854282, -0.00016640232934150845, -0.09557600319385529, -0.0001746263587847352, -0.004008592572063208, -4.970903682988137e-05, -6.329813186312094e-05, -0.015377648174762726, -2.2151248455047607, -0.3719938099384308, -0.0005187834613025188, -0.11050070822238922], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 177.70314918136594, "energy_per_token": 4.038707935940135, "time": 5.8912, "time_per_token": 0.1338909090909091}}
{"outputs": [{"text": " re.sub(r'\\W+', ' ', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'\\", "W", "+'", ",", " '", " '", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -1.6769564151763916, -0.022418633103370667, -0.17651697993278503, -0.00036745471879839897, -0.03512629121541977, -0.0004857790481764823, -0.0001431601122021675, -0.0037910512182861567, -5.113947918289341e-05, -7.629103492945433e-05, -0.018660351634025574, -0.6423712968826294], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 161.7264003791809, "energy_per_token": 4.370983794031916, "time": 5.4665904, "time_per_token": 0.1477456864864865}}
{"outputs": [{"text": " re.sub(r'[^A-Za-z0-9 ]', '', input_str)", "tokens": [" re", ".", "sub", "(", "r", "'", "[", "^", "A", "-", "Z", "a", "-", "z", "0", "-", "9", " ]", "'", ",", " ''", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -0.4342042803764343, -0.03244341164827347, -1.199073314666748, -0.9374302625656128, -0.0002315968304174021, -0.009623917751014233, -0.0006267252028919756, -8.4638240878121e-06, -2.825220326485578e-05, -0.3252539336681366, -0.0005482124397531152, -2.4437606043647975e-05, -1.0646443367004395, -0.3472878634929657, -0.00015078838623594493, -0.08354729413986206, -0.00028355870745144784, -0.0028903395868837833, -8.523101132595912e-05, -0.0005783553933724761, -0.011083230376243591, -0.1494341790676117], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -0.4488956928253174], "finish_reason": "stop"}], "metrics": {"energy": 180.35362845706936, "energy_per_token": 3.837311243767433, "time": 6.3293533, "time_per_token": 0.1346670914893617}}
{"outputs": [{"text": " re.sub(r\"[^\\w\\s]\", \"\", input_str)", "tokens": [" re", ".", "sub", "(", "r", "\"", "[", "^\\", "w", "\\", "s", "]\"", ",", " \"", "\"", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -1.8100244998931885, -0.09824926406145096, -0.9836709499359131, -0.0009502424509264529, -0.015332569368183613, -0.0004017737810499966, -0.009927777573466301, -0.0001463782973587513, -0.15701186656951904, -0.07297386974096298, -0.00039986721822060645, -0.005273835267871618, -6.4490144723095e-05, -0.0001102625101339072, -0.010978532955050468, -0.339874267578125], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict<|endoftext|># -*- coding: utf-8 -*-", "tokens": [" fre", "q", "_", "dict", "<", "|", "end", "of", "text", "|>", "#", " -", "*-", " coding", ":", " utf", "-", "8", " -", "*-", "\n"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05, -1.8244831562042236, -6.198863957251888e-06, -1.9073468138230965e-06, -2.9802276912960224e-06, -1.4185804502631072e-05, -0.14681214094161987, -1.2786263227462769, -2.4953184127807617, -0.007728437427431345, -0.017524754628539085, -0.003705304116010666, -0.03899461030960083, -0.006081054452806711, -7.915183232398704e-05, -0.0026376002933830023, -0.00428674416616559, -0.004305616952478886], "finish_reason": "stop"}], "metrics": {"energy": 223.29167800807954, "energy_per_token": 3.917397859790869, "time": 7.166389, "time_per_token": 0.12572612280701753}}
{"outputs": [{"text": " re.sub(r\"[^a-zA-Z0-9]+\", \" \", input_str)", "tokens": [" re", ".", "sub", "(", "r", "\"", "[", "^", "a", "-", "z", "A", "-", "Z", "0", "-", "9", "]+", "\"", ",", " \"", " \"", ",", " input", "_", "str", ")", "\n"], "logprobs": [-0.019698821008205414, -0.00025912740966305137, -0.0022751418873667717, -0.10295312106609344, -0.05093313381075859, -1.8100244998931885, -0.09824926406145096, -0.5014379024505615, -0.14326457679271698, -0.00021419614495243877, -0.0003149013500660658, -0.0051516881212592125, -5.125868119648658e-05, -0.0018067717319354415, -0.20169593393802643, -0.0002212279650848359, -4.815939246327616e-05, -2.6845240592956543, -0.0055599333718419075, -0.0002060916303889826, -0.21904093027114868, -0.12652550637722015, -0.00015293381875380874, -0.0042719063349068165, -8.225102646974847e-05, -0.0002138386043952778, -0.033250220119953156, -0.6020905375480652], "finish_reason": "stop"}, {"text": " word_tokenize(cleaned_str)", "tokens": [" word", "_", "token", "ize", "(", "cleaned", "_", "str", ")", "\n"], "logprobs": [-0.1321386843919754, -2.95634672511369e-05, -8.583032467868179e-06, -7.152555099310121e-07, -0.00033885458833537996, -0.0018475858960300684, -1.3828182090946939e-05, -4.291525328881107e-06, -0.012240410782396793, -0.0018730491865426302], "finish_reason": "stop"}, {"text": " Counter(words)", "tokens": [" Counter", "(", "words", ")", "\n"], "logprobs": [-0.041920557618141174, -0.0036095723044127226, -0.009240951389074326, -0.0004114258335903287, -0.0008500776602886617], "finish_reason": "stop"}, {"text": " freq_dict", "tokens": [" fre", "q", "_", "dict"], "logprobs": [-0.02310182899236679, -2.3841855067985307e-07, -2.658331868587993e-05, -4.434487345861271e-05], "finish_reason": "stop"}], "metrics": {"energy": 188.17146630287166, "energy_per_token": 4.003648219210035, "time": 6.3235664, "time_per_token": 0.1345439659574468}}
