{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -0.4014422297477722, -0.00205163168720901, -0.021972665563225746, -0.014374163001775742, -0.11593862622976303, -0.00022182388056535274, -0.0029114973731338978], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.0596445798873901, -0.025069916620850563], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts()", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-0.8544065356254578, -8.630380034446716e-05, -0.4608425199985504, -0.08176787942647934, -0.3675312101840973, -0.00010024998482549563, -0.00036459474358707666, -0.016897281631827354, -0.01130437757819891, -0.05606747046113014, -6.949660019017756e-05, -0.0007761088781990111, -0.034683309495449066, -0.15717507898807526], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.items(), columns=['Word','Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "items", "()", ",", " columns", "=['", "Word", "'", ",", "'", "F", "requency", "'])", "\n"], "logprobs": [-0.019807400181889534, -5.113947918289341e-05, -0.015020717866718769, -0.551566481590271, -0.38968905806541443, -0.08469504117965698, -0.6827731728553772, -0.06297004222869873, -0.0007695574313402176, -0.02842019870877266, -0.42232319712638855, -0.008088333532214165, -0.0004351384413894266, -0.0003620922507252544, -2.3862853050231934, -0.004211960360407829, -0.004253507591784, -0.009569605812430382, -0.0378115251660347], "finish_reason": "stop"}, {"text": " df<|endoftext|>#!/usr/bin/env python3", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "#", "!", "/", "usr", "/", "bin", "/", "env", " python", "3", "\n"], "logprobs": [-0.045987728983163834, -1.5901813507080078, -5.602820692729438e-06, -1.5497195136049413e-06, -3.71926071238704e-05, -3.671578815556131e-05, -0.07485758513212204, -1.1896486282348633, -0.983961284160614, -0.05369268357753754, -0.08728794753551483, -0.0001699779968475923, -0.004240687470883131, -0.0004720765573438257, -0.05594167858362198, -0.004944952670484781, -0.7635889053344727, -0.023933548480272293], "finish_reason": "stop"}], "metrics": {"energy": 148.59515022659303, "energy_per_token": 1.5807994704956705, "time": 5.4740214, "time_per_token": 0.058234270212765955}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -0.4014422297477722, -0.00205163168720901, -0.021972665563225746, -0.014374163001775742, -0.11593862622976303, -0.00022182388056535274, -0.0029114973731338978], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " list(map(str.lower, tk.tokenize(text.lower())))", "tokens": [" list", "(", "map", "(", "str", ".", "lower", ",", " t", "k", ".", "token", "ize", "(", "text", ".", "lower", "()", ")))", "\n"], "logprobs": [-2.9335834980010986, -0.035638678818941116, -1.290351152420044, -0.011918052099645138, -1.539429783821106, -0.4693792462348938, -0.2678067088127136, -0.0028456454165279865, -0.16536253690719604, -0.0002236116270069033, -0.00016675988445058465, -0.0048531354404985905, -0.0012178155593574047, -0.003018587129190564, -0.006462390068918467, -2.068366050720215, -0.2266046404838562, -0.017912408336997032, -0.07278246432542801, -0.03460570052266121], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith(\"$\")]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-0.7044394016265869, -0.6327607035636902, -0.23968376219272614, -0.002373380120843649, -0.0005994430393911898, -0.004096925724297762, -0.042246416211128235, -0.12772656977176666, -0.541154682636261, -0.18355722725391388, -9.667406266089529e-05, -7.152555099310121e-07, -0.8364623785018921, -0.00827089324593544, -0.0005620330339297652, -0.0006808108882978559, -0.19494688510894775, -0.02132137306034565], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts()", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-0.8023592829704285, -8.189342770492658e-05, -0.5465512275695801, -0.0737992525100708, -0.3500606119632721, -8.725739462533966e-05, -0.0003164505760651082, -0.019534118473529816, -0.012167632579803467, -0.05938250198960304, -8.594620157964528e-05, -0.0007238152320496738, -0.05177270993590355, -0.15320360660552979], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.items(), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "items", "()", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.016325123608112335, -5.495397272170521e-05, -0.014808260835707188, -0.6104250550270081, -0.4009978175163269, -0.0834890604019165, -0.5317944288253784, -0.06023959815502167, -0.0007902836659923196, -0.030943652614951134, -0.3863666355609894, -0.009148228913545609, -0.0004654039512388408, -0.00031728477915748954, -0.13158179819583893, -0.002362200990319252, -0.002602882217615843, -0.008568072691559792, -0.05337776616215706], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.0427679605782032, -0.851069986820221], "finish_reason": "stop"}], "metrics": {"energy": 151.32948331069952, "energy_per_token": 1.4412331743876143, "time": 5.69165, "time_per_token": 0.05420619047619048}}
{"outputs": [{"text": " type(text) == str:", "tokens": [" type", "(", "text", ")", " ==", " str", ":", "\n"], "logprobs": [-1.9378771781921387, -0.0004161922261118889, -0.0004612335760612041, -0.00020132421923335642, -2.5706870555877686, -0.07887531816959381, -0.025967257097363472, -0.015640920028090477], "finish_reason": "stop"}, {"text": " ValueError", "tokens": [" Value", "Error", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -2.9025511741638184], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.16092899441719055, -0.0068033249117434025, -1.7165990357170813e-05, -0.00658804876729846, -0.5803220272064209, -0.37004923820495605, -0.0003687655262183398, -0.0002361257211305201, -0.0027393694035708904, -0.003709698561578989, -0.04438398778438568, -0.026280242949724197], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4608280062675476, -0.00022075122979003936, -0.0009211826254613698, -0.007431126665323973, -0.00022790218645241112, -0.0026290398091077805, -0.0041859629563987255, -0.03510914370417595, -0.012430338189005852], "finish_reason": "stop"}, {"text": " [word for word in words if word.lower().startswith('$')]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "lower", "()", ".", "start", "sw", "ith", "('", "$", "')", "]", "\n"], "logprobs": [-0.7689715027809143, -0.5372699499130249, -0.25305527448654175, -0.0028795229736715555, -0.0005937243695370853, -0.0030452092178165913, -0.05147530511021614, -0.10661162436008453, -0.5623965263366699, -2.539865016937256, -0.18036703765392303, -0.02131308801472187, -0.14255042374134064, -2.610649426060263e-05, -1.1920928244535389e-07, -0.522165834903717, -0.008791785687208176, -0.00026806574896909297, -0.297360360622406, -0.017439592629671097], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=[[word, words.count(word)] for word in set(dollar_words)], columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=[", "[", "word", ",", " words", ".", "count", "(", "word", ")]", " for", " word", " in", " set", "(", "dollar", "_", "words", ")]", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.8908650279045105, -8.976056415122002e-05, -0.9270489811897278, -0.39763545989990234, -2.980330467224121, -1.6869251728057861, -0.513715922832489, -0.32093194127082825, -0.08962970972061157, -0.8873909711837769, -0.0001784403866622597, -0.0002936885575763881, -0.0014173472300171852, -0.0018018929986283183, -0.018292654305696487, -0.03907795995473862, -0.0026965707074850798, -0.0005082983989268541, -0.2348507195711136, -0.000601349223870784, -0.008495145477354527, -3.2186455882765586e-06, -2.9444261599564925e-05, -0.022801131010055542, -0.14215247333049774, -0.29430440068244934, -0.29030686616897583, -0.011751591227948666, -0.00046850196667946875, -0.0005156855913810432, -0.09675776213407516, -0.0022670540492981672, -0.0031827054917812347, -0.01664934866130352, -0.04125719517469406], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.most_common(), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "most", "_", "common", "()", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.015400656498968601, -6.294052582234144e-05, -0.014945323579013348, -0.5732138752937317, -0.39879581332206726, -0.09539581090211868, -0.9121494293212891, -1.156323378381785e-05, -0.0012457951670512557, -0.10516496002674103, -0.00177345285192132, -0.03936142474412918, -0.409132182598114, -0.008691221475601196, -0.0005421360838226974, -0.00032431588624604046, -0.12991030514240265, -0.0014778178883716464, -0.0020919598173350096, -0.006845830008387566, -0.025383880361914635], "finish_reason": "stop"}, {"text": " df<|endoftext|>from math import *", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "from", " math", " import", " *", "\n"], "logprobs": [-0.03984369337558746, -1.6676278114318848, -8.4638240878121e-06, -1.7881377516459906e-06, -3.290122185717337e-05, -7.223821739898995e-05, -0.08215887099504471, -1.188005805015564, -3.509276866912842, -0.016782408580183983, -2.8362953662872314, -0.011746760457754135], "finish_reason": "stop"}], "metrics": {"energy": 139.18197953796385, "energy_per_token": 1.1598498294830322, "time": 6.319219, "time_per_token": 0.05266015833333334}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"The input must be a string\")", "tokens": [" Value", "Error", "(", "\"", "The", " input", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -2.60115909576416, -0.6513035297393799, -1.1789894104003906, -0.0015711355954408646, -0.03012734465301037, -0.022021645680069923, -1.0107924938201904, -0.00032824851223267615, -0.0033551135566085577], "finish_reason": "stop"}, {"text": " nltk.tokenize.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.1583298295736313, -0.007430653553456068, -1.8954096958623268e-05, -0.0075366683304309845, -1.256964921951294, -0.0004953111056238413, -0.0014174662064760923, -0.40124326944351196, -1.6169027090072632, -0.003003374207764864, -0.0008579387213103473, -0.003548873821273446, -0.00483035808429122, -0.07880061864852905, -0.033921100199222565], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4035109579563141, -0.00019834458362311125, -0.001212219474837184, -0.006853762548416853, -0.0002636561985127628, -0.0032059960067272186, -0.0034860337618738413, -0.0359879806637764, -0.0121212312951684], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.052741289138794, -0.024003613740205765], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts()", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-0.8422386050224304, -8.940297266235575e-05, -0.4434922933578491, -0.07045775651931763, -0.3287331759929657, -0.00011669908417388797, -0.00031442465842701495, -0.018067894503474236, -0.011181667447090149, -0.049145184457302094, -8.487341256113723e-05, -0.0006677066558040679, -0.045699186623096466, -0.16811895370483398], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word':list(freq.keys()),", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", "list", "(", "freq", ".", "keys", "())", ",", "\n"], "logprobs": [-0.018001511693000793, -5.686121585313231e-05, -0.014613850973546505, -1.7621917724609375, -0.05813220515847206, -0.009511865675449371, -0.04071139916777611, -3.3505585193634033, -0.00045599075383506715, -0.03254334628582001, -0.032389283180236816, -0.021724587306380272, -0.0006675875629298389, -0.009027381427586079, -2.229334831237793], "finish_reason": "stop"}, {"text": " df.reset_index(drop=True)", "tokens": [" df", ".", "reset", "_", "index", "(", "drop", "=", "True", ")", "\n"], "logprobs": [-0.04029115289449692, -3.5851922035217285, -3.266392707824707, -2.312633478140924e-05, -6.341733387671411e-05, -0.25490543246269226, -0.0015324526466429234, -0.029929758980870247, -0.005489513278007507, -0.1589210480451584, -0.29076439142227173], "finish_reason": "stop"}], "metrics": {"energy": 107.88340495681763, "energy_per_token": 1.2400391374346853, "time": 5.272643, "time_per_token": 0.06060509195402299}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -0.4014422297477722, -0.00205163168720901, -0.021972665563225746, -0.014374163001775742, -0.11593862622976303, -0.00022182388056535274, -0.0029114973731338978], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.0596445798873901, -0.025069916620850563], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-2.9798853397369385, -0.022045550867915154], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.items(), columns=['Word','Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "items", "()", ",", " columns", "=['", "Word", "'", ",", "'", "F", "requency", "'])", "\n"], "logprobs": [-0.019807400181889534, -5.113947918289341e-05, -0.015020717866718769, -0.551566481590271, -0.38968905806541443, -0.08469504117965698, -0.6827731728553772, -0.06297004222869873, -0.0007695574313402176, -0.02842019870877266, -0.42232319712638855, -0.008088333532214165, -0.0004351384413894266, -0.0003620922507252544, -2.3862853050231934, -0.004211960360407829, -0.004253507591784, -0.009569605812430382, -0.0378115251660347], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.045987728983163834, -0.8022155165672302], "finish_reason": "stop"}], "metrics": {"energy": 128.4970647792816, "energy_per_token": 1.946925223928509, "time": 4.219776, "time_per_token": 0.063936}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError", "tokens": [" Value", "Error", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -2.9025511741638184], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.16092899441719055, -0.0068033249117434025, -1.7165990357170813e-05, -0.00658804876729846, -0.5803220272064209, -0.37004923820495605, -0.0003687655262183398, -0.0002361257211305201, -0.0027393694035708904, -0.003709698561578989, -0.04438398778438568, -0.026280242949724197], "finish_reason": "stop"}, {"text": " nk.word_tokenize(text)", "tokens": [" n", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-2.585994005203247, -0.9361774921417236, -0.03693739324808121, -1.2131465673446655, -0.2544408142566681, -0.004577515181154013, -0.0006348263123072684, -0.0032737255096435547, -0.01622719131410122, -0.06765580177307129, -0.016833283007144928], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith('$')]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "('", "$", "')", "]", "\n"], "logprobs": [-0.738824725151062, -0.5723260045051575, -0.26579952239990234, -0.002368147252127528, -0.0007476395112462342, -0.0030036119278520346, -0.04976976662874222, -0.12861040234565735, -0.46934354305267334, -0.1652052104473114, -0.00011455356434453279, -7.152555099310121e-07, -0.5202310681343079, -0.008430133573710918, -0.0003995097358711064, -0.343437135219574, -0.017158981412649155], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=[dollar_words], index=[\"$dollar\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=[", "dollar", "_", "words", "]", ",", " index", "=[", "\"", "$", "dollar", "\"", "])", "\n"], "logprobs": [-0.816577672958374, -8.368142152903602e-05, -1.0302048921585083, -0.44575411081314087, -3.0198895931243896, -1.8578202724456787, -2.0281240940093994, -0.00025018901214934886, -0.00421682745218277, -1.090948224067688, -0.021729836240410805, -2.8264172077178955, -0.6625315546989441, -0.8337564468383789, -2.2807302474975586, -1.3427684307098389, -2.2672109603881836, -0.273495078086853, -0.22224554419517517], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.items(), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "items", "()", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.02063123695552349, -6.55629628454335e-05, -0.01695600338280201, -0.5294541716575623, -0.38773566484451294, -0.07500402629375458, -0.5705124735832214, -0.049590542912483215, -0.0007711059297434986, -0.029796354472637177, -0.3656507134437561, -0.008106189779937267, -0.0004412154376041144, -0.0003644755925051868, -0.11510609835386276, -0.0022998806089162827, -0.0025518732145428658, -0.008433797396719456, -0.04327382147312164], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.04354206100106239, -0.8261560201644897], "finish_reason": "stop"}], "metrics": {"energy": 149.87936809349057, "energy_per_token": 1.6470260230053908, "time": 5.060644, "time_per_token": 0.055611472527472526}}
{"outputs": [{"text": " type(text) != str:", "tokens": [" type", "(", "text", ")", " ", "!", "=", " str", ":", "\n"], "logprobs": [-1.9378771781921387, -0.0004161922261118889, -0.0004612335760612041, -0.00020132421923335642, -0.5202892422676086, -4.172316494077677e-06, -7.188061863416806e-05, -0.03330383822321892, -0.011490946635603905, -0.015304863452911377], "finish_reason": "stop"}, {"text": " ValueError(\"The text argument must be a string.\")", "tokens": [" Value", "Error", "(", "\"", "The", " text", " argument", " must", " be", " a", " string", ".", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -2.60115909576416, -1.063096284866333, -3.0777316093444824, -0.18963105976581573, -0.0018254535971209407, -0.05105423182249069, -0.016200566664338112, -0.4188319444656372, -0.014744949527084827, -0.0007764662150293589, -0.003990070428699255], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.15022796392440796, -0.007025065366178751, -1.7046782886609435e-05, -0.00555791798979044, -0.5660759806632996, -0.4805006980895996, -0.0004020121123176068, -0.00022885564249008894, -0.00303914793767035, -0.00455473130568862, -0.057331498712301254, -0.03153188154101372], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4272163212299347, -0.0001908358681248501, -0.0007404923089779913, -0.007050160318613052, -0.0002329078270122409, -0.0027372294571250677, -0.004305616952478886, -0.03553042188286781, -0.013138508424162865], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith(\"$\")]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-0.7140969038009644, -0.5500982403755188, -0.26411113142967224, -0.0028267446905374527, -0.0006865289760753512, -0.00316226645372808, -0.05843539535999298, -0.13575011491775513, -0.5051314830780029, -0.1917395442724228, -0.00010585224663373083, -4.768370445162873e-07, -1.025939702987671, -0.008289691992104053, -0.0004542034293990582, -0.0007134514744393528, -0.19285233318805695, -0.018092364072799683], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-3.014357566833496, -0.022192934527993202], "finish_reason": "stop"}, {"text": " pd.DataFrame([[word, freq.count(word)] for word, _ in freq.items()]).sort_values(by='Frequency', ascending=False)", "tokens": [" pd", ".", "DataFrame", "([", "[", "word", ",", " fre", "q", ".", "count", "(", "word", ")]", " for", " word", ",", " _", " in", " fre", "q", ".", "items", "()", "])", ".", "sort", "_", "values", "(", "by", "='", "F", "requency", "'", ",", " ascending", "=", "False", ")", "\n"], "logprobs": [-0.019027329981327057, -4.8397800128441304e-05, -0.013414249755442142, -2.736283302307129, -1.2760241031646729, -0.8159797787666321, -0.10624422878026962, -0.13222569227218628, -4.51792984677013e-05, -1.6173609495162964, -0.7286640405654907, -0.000579546787776053, -0.002187480451539159, -0.00555791798979044, -0.03396834805607796, -0.005489513278007507, -2.866323232650757, -2.0463786125183105, -0.013151567429304123, -0.026208247989416122, -0.00023600654094479978, -0.002375164069235325, -0.2093321979045868, -0.002075781114399433, -0.6734580397605896, -2.681708335876465, -1.7674322128295898, -0.0016820580931380391, -0.03181841969490051, -0.13836786150932312, -0.27653977274894714, -0.6031626462936401, -0.2373497635126114, -9.059495641849935e-05, -0.03861917555332184, -0.0002885640424210578, -0.07456386834383011, -0.02532472275197506, -0.006531557533890009, -0.04778359830379486, -0.11400794982910156], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.035784762352705], "finish_reason": "stop"}], "metrics": {"energy": 127.86700979423527, "energy_per_token": 1.1839537943910674, "time": 5.8932047, "time_per_token": 0.054566710185185184}}
{"outputs": [{"text": " type(text) != str:", "tokens": [" type", "(", "text", ")", " ", "!", "=", " str", ":", "\n"], "logprobs": [-1.9378771781921387, -0.0004161922261118889, -0.0004612335760612041, -0.00020132421923335642, -0.5202892422676086, -4.172316494077677e-06, -7.188061863416806e-05, -0.03330383822321892, -0.011490946635603905, -0.015304863452911377], "finish_reason": "stop"}, {"text": " ValueError(\"Input must be a string.\")", "tokens": [" Value", "Error", "(", "\"", "Input", " must", " be", " a", " string", ".", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -1.2210054397583008, -0.9398485422134399, -0.001859008800238371, -0.061045609414577484, -0.016411321237683296, -1.0943067073822021, -0.013644995167851448, -0.0003054867556784302, -0.002829003380611539], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " list(filter(lambda x: x[0]==\"$\", words))", "tokens": [" list", "(", "filter", "(", "lambda", " x", ":", " x", "[", "0", "]", "==", "\"", "$", "\"", ",", " words", "))", "\n"], "logprobs": [-2.3058834075927734, -0.16409772634506226, -0.11457588523626328, -0.014257471077144146, -0.028058158233761787, -0.39924702048301697, -0.027749788016080856, -0.15805087983608246, -0.6381251811981201, -0.0022597985807806253, -0.006680890452116728, -2.515072822570801, -1.5460985898971558, -0.0007130940794013441, -3.9219088648678735e-05, -0.12054212391376495, -0.0872693732380867, -0.01066390611231327, -0.0700996145606041], "finish_reason": "stop"}, {"text": " pd.Series(words).value_counts()", "tokens": [" pd", ".", "Series", "(", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-0.8544065356254578, -8.630380034446716e-05, -0.4608425199985504, -0.08176787942647934, -2.8542027473449707, -0.02982933074235916, -0.030208658427000046, -0.06151484325528145, -5.924526340095326e-05, -0.0011536378879100084, -0.037891410291194916, -0.21208061277866364], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.most_common(), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "most", "_", "common", "()", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.019807400181889534, -5.113947918289341e-05, -0.015020717866718769, -0.551566481590271, -0.38968905806541443, -0.08469504117965698, -0.8234420418739319, -1.6569954823353328e-05, -0.0013017522869631648, -0.12999634444713593, -0.0017534609651193023, -0.039327044039964676, -0.41796648502349854, -0.007381665054708719, -0.0005607224884442985, -0.0002640137099660933, -0.1356949508190155, -0.0016000575851649046, -0.0018442542059347034, -0.006157830823212862, -0.025632794946432114], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.045987728983163834], "finish_reason": "stop"}], "metrics": {"energy": 113.81467551994325, "energy_per_token": 1.1733471703086933, "time": 5.682451, "time_per_token": 0.05858196907216495}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -0.4014422297477722, -0.00205163168720901, -0.021972665563225746, -0.014374163001775742, -0.11593862622976303, -0.00022182388056535274, -0.0029114973731338978], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " [x for x in tk.tokenize(text) if x[0] == \"$\"]", "tokens": [" [", "x", " for", " x", " in", " t", "k", ".", "token", "ize", "(", "text", ")", " if", " x", "[", "0", "]", " ==", " \"", "$", "\"", "]", "\n"], "logprobs": [-1.9032509326934814, -2.384746789932251, -0.6634833216667175, -0.0038262030575424433, -0.0013635394861921668, -0.045637574046850204, -0.00010907054820563644, -0.0002057340752799064, -0.006606641225516796, -0.0005209281225688756, -0.0008509114268235862, -0.0039747534319758415, -0.13847331702709198, -0.03343804180622101, -0.34299683570861816, -0.5466611981391907, -0.002303924411535263, -0.0056595089845359325, -0.12117616832256317, -0.8276610970497131, -0.0004068977141287178, -2.2172682292875834e-05, -0.1462772637605667, -0.015910105779767036], "finish_reason": "stop"}, {"text": " [word[1:] for word in words if word.startswith(\"$\") and word[1:].isalpha()]", "tokens": [" [", "word", "[", "1", ":]", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")", " and", " word", "[", "1", ":]", ".", "is", "alpha", "()", "]", "\n"], "logprobs": [-0.7044394016265869, -0.6327607035636902, -2.7700278759002686, -0.04372579604387283, -0.011077924631536007, -0.1363886296749115, -0.003418080974370241, -0.0006193388253450394, -0.009950203821063042, -0.055230818688869476, -0.12180178612470627, -0.8024547100067139, -0.0521048828959465, -5.173549288883805e-05, -4.768370445162873e-07, -0.784042239189148, -0.003259823424741626, -0.0004190520558040589, -0.00048756631440483034, -1.5813977718353271, -0.15979033708572388, -1.0135459899902344, -0.7255528569221497, -0.031132256612181664, -0.1462055742740631, -0.2540540099143982, -0.8925880789756775, -0.09722460061311722, -0.0029122107662260532, -0.05208892747759819, -0.04251322150230408], "finish_reason": "stop"}, {"text": " Counter(dollar_words)", "tokens": [" Counter", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-1.4766840934753418, -0.023888271301984787, -0.01190792117267847, -1.9907753085135482e-05, -5.2569914259947836e-05, -0.0015550674870610237, -0.06013128161430359], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.most_common(), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "most", "_", "common", "()", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.019362138584256172, -5.769562994828448e-05, -0.012766890227794647, -0.5710367560386658, -0.32536742091178894, -0.0808197632431984, -1.0483709573745728, -1.3708974620385561e-05, -0.0011449456214904785, -0.1282566636800766, -0.00176821683999151, -0.04310302063822746, -0.41859525442123413, -0.007659707684069872, -0.0005951540661044419, -0.00032860602368600667, -0.1380925327539444, -0.0018352109473198652, -0.002022008877247572, -0.006067547015845776, -0.017855968326330185], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.0343206524848938], "finish_reason": "stop"}], "metrics": {"energy": 138.99519581222535, "energy_per_token": 1.198234446657115, "time": 6.5245166, "time_per_token": 0.05624583275862069}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -0.4014422297477722, -0.00205163168720901, -0.021972665563225746, -0.014374163001775742, -0.11593862622976303, -0.00022182388056535274, -0.0029114973731338978], "finish_reason": "stop"}, {"text": " nltk.tokenize.wordpunct_tokenize(text)", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "punct", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -1.2952587604522705, -0.00038413776201196015, -0.001291870721615851, -0.4738922715187073, -0.2712269425392151, -2.074220174108632e-05, -0.006677574943751097, -0.010126877576112747, -0.0021709464490413666, -0.006514739710837603, -0.04698707163333893, -0.040879976004362106], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " list(filter(lambda word: word.startswith(\"$\"), words))", "tokens": [" list", "(", "filter", "(", "lambda", " word", ":", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")", ",", " words", "))", "\n"], "logprobs": [-2.3058834075927734, -0.16409772634506226, -0.11457588523626328, -0.014257471077144146, -0.028058158233761787, -2.126354932785034, -0.014294606633484364, -0.17507629096508026, -0.6368419528007507, -0.11307842284440994, -0.0001641377166379243, -1.9073468138230965e-06, -1.007190227508545, -0.008575519546866417, -0.00047946401173248887, -0.0010760000441223383, -0.0009003399754874408, -0.2597295343875885, -0.023961132392287254, -0.007857958786189556, -0.05226747319102287], "finish_reason": "stop"}, {"text": " nltk.FreqDist(dollar_words)", "tokens": [" n", "lt", "k", ".", "F", "req", "Dist", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-3.2175076007843018, -0.03729209676384926, -0.0006466205231845379, -0.0005006728461012244, -0.03774689882993698, -0.014581308700144291, -0.00044216870446689427, -0.007980840280652046, -0.02831498347222805, -1.3589766240329482e-05, -6.0794889577664435e-05, -0.0011083895806223154, -0.023451460525393486], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.most_common(), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "most", "_", "common", "()", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.017967088147997856, -5.221230458118953e-05, -0.01162917260080576, -0.5673636198043823, -0.34866371750831604, -0.07908406108617783, -0.9417056441307068, -1.490105023549404e-05, -0.0011062461417168379, -0.1153118759393692, -0.0018617454916238785, -0.03771188482642174, -0.410745769739151, -0.007523654028773308, -0.0005957497633062303, -0.00029952809563837945, -0.1314956694841385, -0.0014188947388902307, -0.0019456282025203109, -0.00579759432002902, -0.023301467299461365], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.03578821197152138, -0.7378689050674438], "finish_reason": "stop"}], "metrics": {"energy": 156.67794259834295, "energy_per_token": 1.5360582607680682, "time": 5.8804917, "time_per_token": 0.05765187941176471}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"Input is not a string.\")", "tokens": [" Value", "Error", "(", "\"", "Input", " is", " not", " a", " string", ".", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -1.2210054397583008, -2.0947861671447754, -0.02243436872959137, -0.06945313513278961, -0.05268622562289238, -1.1349486112594604, -0.02521301992237568, -0.00036554806865751743, -0.0031339838169515133], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " nk.pos_tag(tk.tokenize(text))", "tokens": [" n", "k", ".", "pos", "_", "tag", "(", "tk", ".", "token", "ize", "(", "text", "))", "\n"], "logprobs": [-2.6781437397003174, -1.0055696964263916, -0.04580771550536156, -1.009155511856079, -0.022869527339935303, -0.015415681526064873, -0.08541800081729889, -0.01529594138264656, -0.001738585764542222, -0.0030702855437994003, -0.007465795613825321, -0.0015549485106021166, -0.004129692446440458, -0.10628753155469894, -0.025219762697815895], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.0693132877349854, -0.02257167361676693], "finish_reason": "stop"}, {"text": " [words.count(word) for word in dollar_words]", "tokens": [" [", "words", ".", "count", "(", "word", ")", " for", " word", " in", " dollar", "_", "words", "]", "\n"], "logprobs": [-2.212735652923584, -0.9903066754341125, -0.0003626880934461951, -0.0009118927991949022, -0.004553188569843769, -0.38242024183273315, -0.01671699434518814, -0.0063419281505048275, -0.006139229983091354, -0.0002779574424494058, -0.08825680613517761, -1.156323378381785e-05, -2.777537883957848e-05, -0.022201795130968094, -0.008126409724354744], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.items(), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "items", "()", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.02047918550670147, -5.6622808187967166e-05, -0.015324234031140804, -0.5410475134849548, -0.347957968711853, -0.07596275955438614, -0.7086973786354065, -0.0587654747068882, -0.0007730118231847882, -0.03363896906375885, -0.37614959478378296, -0.00874877255409956, -0.00044955636258237064, -0.00031859567388892174, -0.1028311550617218, -0.0026062114629894495, -0.0026423560921102762, -0.008078637532889843, -0.0360199473798275], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.03926641121506691, -0.8176043033599854], "finish_reason": "stop"}], "metrics": {"energy": 115.32304909706117, "energy_per_token": 1.3409656871751299, "time": 5.0540643, "time_per_token": 0.058768189534883726}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(f\"text should be a string, but received {type(text)}\")", "tokens": [" Value", "Error", "(", "f", "\"", "text", " should", " be", " a", " string", ",", " but", " received", " {", "type", "(", "text", ")}", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -2.2335028648376465, -0.15621794760227203, -1.6882880926132202, -2.1250741481781006, -0.00861026719212532, -0.32730045914649963, -0.153161883354187, -0.35994282364845276, -1.3694950342178345, -2.154806137084961, -0.3933418393135071, -0.11389870941638947, -0.0006469779182225466, -0.0009135602158494294, -0.05209175497293472, -0.23246023058891296, -0.0007091629668138921, -0.0038829660043120384], "finish_reason": "stop"}, {"text": " nltk.tokenize.RegexpTokenizer('\\w+')", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "Re", "gex", "p", "Token", "izer", "('", "\\", "w", "+", "')", "\n"], "logprobs": [-0.1462714970111847, -0.006203206721693277, -2.5987286790041253e-05, -0.00629039853811264, -1.4247721433639526, -0.0004912600270472467, -0.0013766345800831914, -2.3553287982940674, -4.95898348162882e-05, -0.013637939468026161, -0.0034218826331198215, -0.0009496469865553081, -2.281810998916626, -0.7070974111557007, -0.13653641939163208, -0.15446163713932037, -0.056338727474212646, -0.4069693386554718], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.41451114416122437, -0.0002470903273206204, -0.0010207447921857238, -0.005811817012727261, -0.0002847504511009902, -0.002401327481493354, -0.004451010376214981, -0.034948572516441345, -0.014336325228214264], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.1251342296600342, -0.026746345683932304], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts()", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-0.858333170413971, -8.201262971851975e-05, -0.42381206154823303, -0.06962805241346359, -0.2866764962673187, -0.00010752100206445903, -0.0002971446083392948, -0.018070939928293228, -0.011840303428471088, -0.04853475093841553, -9.047575440490618e-05, -0.0008317348547279835, -0.04476364701986313, -0.16010628640651703], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq, columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.01984269544482231, -5.8530047681415454e-05, -0.014031804166734219, -0.5230844020843506, -0.4176231324672699, -3.0608091354370117, -0.17619147896766663, -0.41951075196266174, -0.011457359418272972, -0.0012516292044892907, -0.00032789100077934563, -0.14079447090625763, -0.0012326984433457255, -0.002380040008574724, -0.010909201577305794, -0.09249930828809738], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.04090354964137077], "finish_reason": "stop"}], "metrics": {"energy": 102.79609688758849, "energy_per_token": 1.155012324579646, "time": 5.284334, "time_per_token": 0.0593745393258427}}
{"outputs": [{"text": " isinstance(text, str):", "tokens": [" isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-2.714087963104248, -0.0005810956354252994, -0.0005077026435174048, -0.0005529781919904053, -0.04566764086484909, -0.09333935379981995, -0.020168351009488106], "finish_reason": "stop"}, {"text": " ValueError(\"The input should be a string.\")", "tokens": [" Value", "Error", "(", "\"", "The", " input", " should", " be", " a", " string", ".", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -2.60115909576416, -0.6513035297393799, -2.460956573486328, -0.007268296089023352, -0.09842116385698318, -0.07071835547685623, -0.5721896290779114, -0.032615117728710175, -0.001150541938841343, -0.004297308158129454], "finish_reason": "stop"}, {"text": " nltk.Tokenizer()", "tokens": [" n", "lt", "k", ".", "Token", "izer", "()", "\n"], "logprobs": [-0.16528736054897308, -0.007084014359861612, -1.4543427823809907e-05, -0.005629044491797686, -3.630119800567627, -0.014995940029621124, -1.6019798517227173, -0.07184286415576935], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4230636656284332, -0.00020287363440729678, -0.0011325619416311383, -0.007014529779553413, -0.0003815161471720785, -0.002261820714920759, -0.004397603217512369, -0.040676720440387726, -0.01307273842394352], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.1018126010894775, -0.02556830644607544], "finish_reason": "stop"}, {"text": " pd.DataFrame()", "tokens": [" pd", ".", "DataFrame", "()", "\n"], "logprobs": [-0.8551533222198486, -8.940297266235575e-05, -1.0750174522399902, -3.254673480987549, -0.020570745691657066], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq, columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.018443282693624496, -6.508615479106084e-05, -0.012555483728647232, -0.5759567022323608, -0.44258272647857666, -2.8599135875701904, -0.19489793479442596, -0.4276416003704071, -0.011347516439855099, -0.001085883704945445, -0.00033504117163829505, -0.13786864280700684, -0.0014728185487911105, -0.0024763653054833412, -0.010912738740444183, -0.09498731046915054], "finish_reason": "stop"}, {"text": " df<|endoftext|>#!/bin/python3", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "#", "!", "/", "bin", "/", "python", "3", "\n"], "logprobs": [-0.041091885417699814, -1.6737830638885498, -5.722029527532868e-06, -1.7881377516459906e-06, -3.611976353568025e-05, -4.8636207793606445e-05, -0.08847244083881378, -1.3091106414794922, -1.063405990600586, -0.062279168516397476, -2.5505597591400146, -0.00016378014697693288, -0.324544221162796, -0.1186932772397995, -0.0010582567192614079], "finish_reason": "stop"}], "metrics": {"energy": 129.55568669605253, "energy_per_token": 1.7046800881059543, "time": 4.6316123, "time_per_token": 0.060942267105263154}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"Text is not a string\")", "tokens": [" Value", "Error", "(", "\"", "Text", " is", " not", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -2.4910898208618164, -2.294532537460327, -0.014638873748481274, -0.0603204108774662, -0.03195340186357498, -0.5215756893157959, -0.00029952809563837945, -0.003170703537762165], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " list(filter(lambda w: w.startswith('$'), words))", "tokens": [" list", "(", "filter", "(", "lambda", " w", ":", " w", ".", "start", "sw", "ith", "('", "$", "')", ",", " words", "))", "\n"], "logprobs": [-2.3058834075927734, -0.16409772634506226, -0.11457588523626328, -0.014257471077144146, -0.028058158233761787, -1.960091471672058, -0.023219473659992218, -0.13881027698516846, -0.728214681148529, -0.1483502984046936, -0.00013541258522309363, -8.344646857949556e-07, -0.46337956190109253, -0.002953574061393738, -0.0007200032705441117, -0.22867830097675323, -0.018769511952996254, -0.006152380723506212, -0.047971211373806], "finish_reason": "stop"}, {"text": " Counter(dollar_words)", "tokens": [" Counter", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-1.4841654300689697, -0.026528239250183105, -0.011906037107110023, -2.062299427052494e-05, -6.294052582234144e-05, -0.0012721073580905795, -0.058514103293418884], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.items(), columns=['Word','Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "items", "()", ",", " columns", "=['", "Word", "'", ",", "'", "F", "requency", "'])", "\n"], "logprobs": [-0.019807400181889534, -5.113947918289341e-05, -0.015020717866718769, -0.551566481590271, -0.38968905806541443, -0.08469504117965698, -0.6827731728553772, -0.06297004222869873, -0.0007695574313402176, -0.02842019870877266, -0.42232319712638855, -0.008088333532214165, -0.0004351384413894266, -0.0003620922507252544, -2.3862853050231934, -0.004211960360407829, -0.004253507591784, -0.009569605812430382, -0.0378115251660347], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.045987728983163834, -0.8022155165672302], "finish_reason": "stop"}], "metrics": {"energy": 102.03255883598328, "energy_per_token": 1.1594608958634465, "time": 5.0486, "time_per_token": 0.05737045454545455}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"Input text is not a string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " text", " is", " not", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -1.2210054397583008, -1.1282564401626587, -1.8182264566421509, -0.0190336462110281, -0.047774165868759155, -0.02909977175295353, -0.5917839407920837, -0.00026246439665555954, -0.003242594189941883], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.0596445798873901, -0.025069916620850563], "finish_reason": "stop"}, {"text": " pd.DataFrame(dollar_words).groupby(0, as_index=False).size().reset_index(name='Frequency')", "tokens": [" pd", ".", "DataFrame", "(", "dollar", "_", "words", ")", ".", "group", "by", "(", "0", ",", " as", "_", "index", "=", "False", ")", ".", "size", "()", ".", "reset", "_", "index", "(", "name", "='", "F", "requency", "')", "\n"], "logprobs": [-0.8544065356254578, -8.630380034446716e-05, -1.0436118841171265, -0.4462294280529022, -3.5585827827453613, -0.0002208704245276749, -0.0009947833605110645, -0.5019869804382324, -0.33461320400238037, -0.42470648884773254, -0.0003499372396618128, -0.5086165070533752, -1.115607738494873, -2.529510974884033, -0.24658210575580597, -0.000346362212439999, -0.00029118589009158313, -0.07746042311191559, -0.011256285011768341, -0.05875176191329956, -0.0678354874253273, -0.5449211001396179, -0.013374610804021358, -0.2776826322078705, -0.7989972829818726, -0.00014649749209638685, -0.00017009719158522785, -0.10195372253656387, -0.03707306459546089, -0.32268860936164856, -0.11670692265033722, -0.00792762078344822, -0.0042930347844958305, -0.08839518576860428], "finish_reason": "stop"}, {"text": " pd.DataFrame([(word, freq[word]) for word in dollar_words])", "tokens": [" pd", ".", "DataFrame", "([", "(", "word", ",", " fre", "q", "[", "word", "])", " for", " word", " in", " dollar", "_", "words", "])", "\n"], "logprobs": [-0.019807400181889534, -5.113947918289341e-05, -0.015020717866718769, -2.690305233001709, -1.2277723550796509, -0.6807767748832703, -0.08524954319000244, -0.11990345269441605, -5.876845170860179e-05, -0.3410834074020386, -0.0027518521528691053, -0.012869163416326046, -0.017380546778440475, -0.004536100197583437, -0.01265848521143198, -1.9573115110397339, -2.264974000354414e-06, -5.6265202147187665e-05, -1.1525540351867676, -0.13185139000415802], "finish_reason": "stop"}, {"text": " df<|endoftext|>from unittest import TestCase", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "from", " un", "ittest", " import", " Test", "Case", "\n"], "logprobs": [-0.034479137510061264, -1.508948802947998, -5.364403477869928e-06, -1.7881377516459906e-06, -3.6238969187252223e-05, -4.303362584323622e-05, -0.08280037343502045, -1.2089080810546875, -2.8816144466400146, -0.11006651073694229, -0.07580916583538055, -0.09558272361755371, -0.00014172980445437133, -0.568681538105011], "finish_reason": "stop"}], "metrics": {"energy": 130.74315143012998, "energy_per_token": 1.1673495663404463, "time": 6.3074093, "time_per_token": 0.056316154464285714}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -0.4014422297477722, -0.00205163168720901, -0.021972665563225746, -0.014374163001775742, -0.11593862622976303, -0.00022182388056535274, -0.0029114973731338978], "finish_reason": "stop"}, {"text": " nltk.tokenize.wordpunct_tokenize(text)", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "punct", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -1.2952587604522705, -0.00038413776201196015, -0.001291870721615851, -0.4738922715187073, -0.2712269425392151, -2.074220174108632e-05, -0.006677574943751097, -0.010126877576112747, -0.0021709464490413666, -0.006514739710837603, -0.04698707163333893, -0.040879976004362106], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.0596445798873901, -0.025069916620850563], "finish_reason": "stop"}, {"text": " Counter(dollar_words)", "tokens": [" Counter", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-1.4841654300689697, -0.026528239250183105, -0.011906037107110023, -2.062299427052494e-05, -6.294052582234144e-05, -0.0012721073580905795, -0.058514103293418884], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.items(), columns = ['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "items", "()", ",", " columns", " =", " ['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.019807400181889534, -5.113947918289341e-05, -0.015020717866718769, -0.551566481590271, -0.38968905806541443, -0.08469504117965698, -0.6827731728553772, -0.06297004222869873, -0.0007695574313402176, -0.02842019870877266, -1.929473876953125, -0.40043407678604126, -0.010733844712376595, -0.0006644901586696506, -0.0017429888248443604, -0.16560035943984985, -0.0025913489516824484, -0.002845050999894738, -0.008512875996530056, -0.03507484495639801], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.045987728983163834], "finish_reason": "stop"}], "metrics": {"energy": 88.46152824306488, "energy_per_token": 1.179487043240865, "time": 4.6380463, "time_per_token": 0.061840617333333334}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"Input is not a string.\")", "tokens": [" Value", "Error", "(", "\"", "Input", " is", " not", " a", " string", ".", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -1.2210054397583008, -2.0947861671447754, -0.02243436872959137, -0.06945313513278961, -0.05268622562289238, -1.1349486112594604, -0.02521301992237568, -0.00036554806865751743, -0.0031339838169515133], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " [i.lower().strip(punctuation) for i in tk.tokenize(text) if len(i)>0]", "tokens": [" [", "i", ".", "lower", "()", ".", "strip", "(", "punct", "uation", ")", " for", " i", " in", " t", "k", ".", "token", "ize", "(", "text", ")", " if", " len", "(", "i", ")>", "0", "]", "\n"], "logprobs": [-1.9032509326934814, -3.137756586074829, -0.671993613243103, -0.19033069908618927, -0.001210671616718173, -1.7819123268127441, -0.10958416014909744, -0.07961735129356384, -0.045508187264204025, -5.8410845667822286e-05, -0.02303263358771801, -0.08856180310249329, -0.006166953593492508, -0.0021808191668242216, -0.046775609254837036, -9.583967766957358e-05, -0.00024375328212045133, -0.004770445637404919, -0.0013227055314928293, -0.0009495278936810791, -0.0071886456571519375, -0.39160865545272827, -0.11284992843866348, -1.412241816520691, -0.0016601602546870708, -0.009304611943662167, -1.9626975059509277, -0.7654714584350586, -0.11664826422929764, -0.0365775041282177], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith('$')]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "('", "$", "')", "]", "\n"], "logprobs": [-0.7044394016265869, -0.6327607035636902, -0.23968376219272614, -0.002373380120843649, -0.0005994430393911898, -0.004096925724297762, -0.042246416211128235, -0.12772656977176666, -0.541154682636261, -0.18355722725391388, -9.667406266089529e-05, -7.152555099310121e-07, -0.5701297521591187, -0.009413735941052437, -0.00043072958942502737, -0.304679811000824, -0.022675279527902603], "finish_reason": "stop"}, {"text": " nltk.FreqDist(dollar_words)", "tokens": [" n", "lt", "k", ".", "F", "req", "Dist", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-3.060950756072998, -0.047349829226732254, -0.00048196621355600655, -0.0004881620698142797, -0.03488767147064209, -0.016463503241539, -0.000442407006630674, -0.006792786996811628, -0.024715617299079895, -1.597391747054644e-05, -6.818538531661034e-05, -0.001166973845101893, -0.028762416914105415], "finish_reason": "stop"}, {"text": " pd.DataFrame(list(zip(list(freq.keys()), list(freq.values()))))", "tokens": [" pd", ".", "DataFrame", "(", "list", "(", "zip", "(", "list", "(", "freq", ".", "keys", "())", ",", " list", "(", "freq", ".", "values", "()))", "))", "\n"], "logprobs": [-0.018741434440016747, -4.7205765440594405e-05, -0.015693729743361473, -0.5677781701087952, -2.0354247093200684, -0.003678580978885293, -1.805756688117981, -0.07090405374765396, -2.1632020473480225, -0.002159764990210533, -0.04742077365517616, -0.030748076736927032, -0.05457882583141327, -0.0008755664457567036, -0.006255096290260553, -0.21858462691307068, -0.001379372552037239, -0.004531234968453646, -0.0003240775258745998, -0.012129475362598896, -0.3691061735153198, -0.01709933578968048, -0.0716589018702507], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.03797714784741402, -0.824683427810669], "finish_reason": "stop"}], "metrics": {"energy": 144.3083124485016, "energy_per_token": 1.2229518004110305, "time": 6.1071787, "time_per_token": 0.05175575169491526}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError", "tokens": [" Value", "Error", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -2.9025511741638184], "finish_reason": "stop"}, {"text": " nltk.wordpunct_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "punct", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.16092899441719055, -0.0068033249117434025, -1.7165990357170813e-05, -0.00658804876729846, -0.5803220272064209, -1.1846644878387451, -0.00026770823751576245, -0.0008922410197556019, -0.003592824097722769, -0.00308728008531034, -0.005883044097572565, -0.043950676918029785, -0.04311969131231308], "finish_reason": "stop"}, {"text": " [word.lower() for word in tk.tokenize(text) if word[0] == \"$\"]", "tokens": [" [", "word", ".", "lower", "()", " for", " word", " in", " t", "k", ".", "token", "ize", "(", "text", ")", " if", " word", "[", "0", "]", " ==", " \"", "$", "\"", "]", "\n"], "logprobs": [-1.9341824054718018, -1.11711585521698, -0.49952682852745056, -0.22295431792736053, -0.0010311054065823555, -0.1969948709011078, -0.00939224287867546, -0.0037746625021100044, -0.05554988980293274, -9.464769391342998e-05, -0.0002644904307089746, -0.0038296468555927277, -0.002203657291829586, -0.0009753713966347277, -0.0059071010909974575, -0.34410029649734497, -0.16295023262500763, -0.6909728646278381, -0.81166011095047, -0.004424544516950846, -0.0048933508805930614, -0.13784973323345184, -0.4426601529121399, -0.0006852186052128673, -2.8371408916427754e-05, -0.3266570568084717, -0.015706872567534447], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith(\"$\")]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-0.738824725151062, -0.5723260045051575, -0.26579952239990234, -0.002368147252127528, -0.0007476395112462342, -0.0030036119278520346, -0.04976976662874222, -0.12861040234565735, -0.46934354305267334, -0.1652052104473114, -0.00011455356434453279, -7.152555099310121e-07, -0.9055002331733704, -0.007232436444610357, -0.00046266341814771295, -0.000631848000921309, -0.18239788711071014, -0.018746113404631615], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts()", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-0.8085365891456604, -9.583967766957358e-05, -0.5319815874099731, -0.06236182898283005, -0.2755795419216156, -0.0001284993631998077, -0.00035696811391972005, -0.018119992688298225, -0.008934401907026768, -0.05061338096857071, -7.724463648628443e-05, -0.0007151191821321845, -0.03987026959657669, -0.15673275291919708], "finish_reason": "stop"}, {"text": " pd.DataFrame.from_dict(freq.most_common(), orient='index', columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", ".", "from", "_", "dict", "(", "freq", ".", "most", "_", "common", "()", ",", " orient", "='", "index", "'", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.020490514114499092, -4.8040190449682996e-05, -0.013649110682308674, -2.127962112426758, -0.00040141629870049655, -0.0006104036583565176, -0.054537396878004074, -0.017359107732772827, -0.023853469640016556, -1.071077585220337, -0.5731942653656006, -1.0490362910786644e-05, -0.002085417043417692, -0.17861796915531158, -0.0015281677478924394, -0.052020009607076645, -0.3190900683403015, -0.013736831955611706, -0.2348496913909912, -0.01013265922665596, -0.011730619706213474, -0.12454687803983688, -0.026236582547426224, -0.0022075825836509466, -0.0003524397616274655, -0.13221023976802826, -0.0012415089877322316, -0.001335086883045733, -0.002883683191612363, -0.031075863167643547], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.03566283360123634, -0.7492828369140625], "finish_reason": "stop"}], "metrics": {"energy": 127.30796300792693, "energy_per_token": 1.1070257652863211, "time": 6.1083593, "time_per_token": 0.053116167826086955}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -0.4014422297477722, -0.00205163168720901, -0.021972665563225746, -0.014374163001775742, -0.11593862622976303, -0.00022182388056535274, -0.0029114973731338978], "finish_reason": "stop"}, {"text": " nltk.wordpunct_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "punct", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -1.0985040664672852, -0.00021801002731081098, -0.0008772339206188917, -0.003242237726226449, -0.0032510305754840374, -0.006029393058270216, -0.04513290897011757, -0.04307379201054573], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " [word for word in words if word[0] == '$']", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", "[", "0", "]", " ==", " '$", "']", "\n"], "logprobs": [-0.7446612119674683, -0.555412232875824, -0.2809702754020691, -0.0025064502842724323, -0.0006363751017488539, -0.0034303173888474703, -0.048688046634197235, -0.12508267164230347, -1.0504968166351318, -0.0037319078110158443, -0.008164010010659695, -0.09712844341993332, -1.212782859802246, -0.00248873233795166, -0.03955337405204773], "finish_reason": "stop"}, {"text": " pd.DataFrame(list(set(dollar_words)))", "tokens": [" pd", ".", "DataFrame", "(", "list", "(", "set", "(", "dollar", "_", "words", ")))", "\n"], "logprobs": [-0.7606593370437622, -8.797258487902582e-05, -1.0208408832550049, -0.43758293986320496, -1.9093281030654907, -0.025170359760522842, -1.9448245763778687, -0.02537679113447666, -0.03282993659377098, -1.0847986231965479e-05, -8.761498611420393e-05, -2.230462074279785, -0.7294470071792603], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.016701988875865936, -5.1020273531321436e-05, -0.014749882742762566, -1.624598503112793, -0.04557493329048157, -0.009498050436377525, -0.03979867324233055, -0.7717432975769043, -3.576278118089249e-07, -0.005589688662439585, -0.04914212226867676, -0.0003947432560380548, -0.0025455711875110865, -0.08769425004720688, -0.0036520943976938725, -0.0025075203739106655, -0.0013017522869631648, -0.056217700242996216, -0.0006423317245207727, -0.0016699191182851791, -0.025212205946445465, -0.0845542922616005, -0.008185411803424358, -0.04008594527840614], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.03161250427365303, -0.7581168413162231], "finish_reason": "stop"}], "metrics": {"energy": 105.04192906570432, "energy_per_token": 1.0941867611010867, "time": 5.2660666, "time_per_token": 0.05485486041666667}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text is not a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " is", " not", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -1.7306087017059326, -0.007608719170093536, -0.02426113374531269, -0.022834455594420433, -0.09486642479896545, -0.0002681849291548133, -0.0037699120584875345], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.0596445798873901, -0.025069916620850563], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-2.9798853397369385, -0.022045550867915154], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.items(), columns=['Word','Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "items", "()", ",", " columns", "=['", "Word", "'", ",", "'", "F", "requency", "'])", "\n"], "logprobs": [-0.019807400181889534, -5.113947918289341e-05, -0.015020717866718769, -0.551566481590271, -0.38968905806541443, -0.08469504117965698, -0.6827731728553772, -0.06297004222869873, -0.0007695574313402176, -0.02842019870877266, -0.42232319712638855, -0.008088333532214165, -0.0004351384413894266, -0.0003620922507252544, -2.3862853050231934, -0.004211960360407829, -0.004253507591784, -0.009569605812430382, -0.0378115251660347], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.045987728983163834, -0.8022155165672302], "finish_reason": "stop"}], "metrics": {"energy": 81.2166770477295, "energy_per_token": 1.2305557128443865, "time": 4.2078695, "time_per_token": 0.06375559848484849}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError", "tokens": [" Value", "Error", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -2.9025511741638184], "finish_reason": "stop"}, {"text": " nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text)", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "Re", "gex", "p", "Token", "izer", "(", "r", "'\\", "w", "+", "')", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.16092899441719055, -0.0068033249117434025, -1.7165990357170813e-05, -0.00658804876729846, -1.3535325527191162, -0.000436925794929266, -0.001301514101214707, -2.521977424621582, -6.115249561844394e-05, -0.011306026950478554, -0.003494230331853032, -0.0010324153117835522, -0.09281518310308456, -0.19701826572418213, -0.8823961019515991, -0.16746419668197632, -0.12528261542320251, -0.11000745743513107, -1.4685264825820923, -0.006149892695248127, -0.0003455280384514481, -0.004201632924377918, -0.007286757696419954, -0.13388320803642273, -0.030450941994786263], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4608280062675476, -0.00022075122979003936, -0.0009211826254613698, -0.007431126665323973, -0.00022790218645241112, -0.0026290398091077805, -0.0041859629563987255, -0.03510914370417595, -0.012430338189005852], "finish_reason": "stop"}, {"text": " [word[1:] for word in words if word.startswith(\"$\")]", "tokens": [" [", "word", "[", "1", ":]", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-0.7689715027809143, -0.5372699499130249, -2.5713860988616943, -0.044162850826978683, -0.009395549073815346, -0.1426868736743927, -0.0030957176350057125, -0.0005550036439672112, -0.010364163666963577, -0.05960456654429436, -0.12830102443695068, -0.8265666365623474, -0.06243720278143883, -6.55629628454335e-05, -4.768370445162873e-07, -0.8142072558403015, -0.0043892948888242245, -0.0003685271949507296, -0.0005147324409335852, -0.2350693792104721, -0.033958550542593], "finish_reason": "stop"}, {"text": " [text.count(word) for word in dollar_words]", "tokens": [" [", "text", ".", "count", "(", "word", ")", " for", " word", " in", " dollar", "_", "words", "]", "\n"], "logprobs": [-2.2209067344665527, -0.9863765835762024, -0.012537237256765366, -0.09091103821992874, -0.011981071904301643, -0.28781065344810486, -0.02252645045518875, -0.00851772166788578, -0.004171954933553934, -0.00028236693469807506, -0.033469170331954956, -9.894321920000948e-06, -3.015949550899677e-05, -0.014825996942818165, -0.01060398854315281], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word':freq.keys(),'Frequency':freq.values()},columns = ['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", "freq", ".", "keys", "()", ",", "'", "F", "requency", "':", "freq", ".", "values", "()}", ",", "columns", " =", " ['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.018682699650526047, -5.721882189391181e-05, -0.014197885990142822, -1.736104965209961, -0.049787458032369614, -0.00866770464926958, -0.038258057087659836, -2.7240374088287354, -0.00709087960422039, -0.05031834915280342, -0.00045980370487086475, -0.008529660291969776, -1.4566311836242676, -0.005573684815317392, -0.004651084542274475, -0.000895933189895004, -0.06270202994346619, -0.0023712394759058952, -0.034927848726511, -2.6090221405029297, -0.01585976965725422, -1.0700899362564087, -2.8798632621765137, -0.06257864087820053, -0.0024035871028900146, -0.0005094899097457528, -0.0008725888328626752, -2.1401405334472656, -0.0007404923089779913, -0.0007241725688800216, -0.007063181139528751, -0.032466258853673935], "finish_reason": "stop"}, {"text": " df<|endoftext|>from typing import List, Tuple, Dict", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "from", " typing", " import", " List", ",", " T", "uple", ",", " D", "ict", "\n"], "logprobs": [-0.03393435478210449, -1.5607861280441284, -8.106198947643861e-06, -1.7881377516459906e-06, -3.659658250398934e-05, -5.125868119648658e-05, -0.0691007599234581, -1.3077726364135742, -2.3866384029388428, -0.0009808497270569205, -0.5481081008911133, -1.4501361846923828, -1.212211012840271, -0.0001382732152706012, -0.9375587105751038, -1.4329252243041992, -0.005755400285124779, -0.44642725586891174], "finish_reason": "stop"}], "metrics": {"energy": 172.44265111732483, "energy_per_token": 1.316356115399426, "time": 7.1599226, "time_per_token": 0.05465589770992366}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"Text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "Text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -2.4910898208618164, -0.35208860039711, -0.0012838939437642694, -0.03902338817715645, -0.01176325511187315, -0.6456002593040466, -0.0002754547167569399, -0.002804515417665243], "finish_reason": "stop"}, {"text": " nltk.tokenize.WordPunctTokenizer()", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "Word", "Punct", "Token", "izer", "()", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -1.2952587604522705, -0.00038413776201196015, -0.001291870721615851, -2.3314571380615234, -0.004043618217110634, -0.0009490515221841633, -0.00033802041434682906, -0.03756435960531235, -0.03284793347120285], "finish_reason": "stop"}, {"text": " [x for x in tk.tokenize(text) if x[0] == \"$\" and x[1:].isalnum()]", "tokens": [" [", "x", " for", " x", " in", " t", "k", ".", "token", "ize", "(", "text", ")", " if", " x", "[", "0", "]", " ==", " \"", "$", "\"", " and", " x", "[", "1", ":]", ".", "isal", "num", "()", "]", "\n"], "logprobs": [-1.9032509326934814, -2.384746789932251, -0.6634833216667175, -0.0038262030575424433, -0.0013635394861921668, -0.045637574046850204, -0.00010907054820563644, -0.0002057340752799064, -0.006606641225516796, -0.0005209281225688756, -0.0008509114268235862, -0.0039747534319758415, -0.13847331702709198, -0.03343804180622101, -0.34299683570861816, -0.5466611981391907, -0.002303924411535263, -0.0056595089845359325, -0.12117616832256317, -0.8276610970497131, -0.0004068977141287178, -2.2172682292875834e-05, -2.1291663646698, -0.6686555743217468, -0.6177778840065002, -0.19614841043949127, -0.3753150999546051, -0.0611673966050148, -1.4080636501312256, -0.0002766464895103127, -0.000551310193259269, -0.0873832032084465, -0.020511535927653313], "finish_reason": "stop"}, {"text": " [w for w in words if w[0] == '$' and w.isalpha()]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", "[", "0", "]", " ==", " '", "$'", " and", " w", ".", "is", "alpha", "()", "]", "\n"], "logprobs": [-0.7044394016265869, -1.1392438411712646, -0.22027918696403503, -0.0013150862650945783, -0.0011875727213919163, -0.004500736016780138, -0.038600482046604156, -0.09520312398672104, -0.8886383175849915, -0.0020080895628780127, -0.008573155850172043, -0.12241362035274506, -2.0614471435546875, -0.00021681819634977728, -0.056247226893901825, -0.5883505940437317, -0.8117223978042603, -0.7420632839202881, -0.10353638976812363, -0.0021466801408678293, -0.04726307466626167, -0.029053453356027603], "finish_reason": "stop"}, {"text": " [dollar_words.count(word) for word in set(dollar_words)]", "tokens": [" [", "dollar", "_", "words", ".", "count", "(", "word", ")", " for", " word", " in", " set", "(", "dollar", "_", "words", ")]", "\n"], "logprobs": [-2.2142446041107178, -2.3356974124908447, -0.00048458753735758364, -0.003262556390836835, -0.0009505997295491397, -0.0005548844928853214, -0.007701230235397816, -0.3127830922603607, -0.010933845303952694, -0.008763189427554607, -0.016676664352416992, -0.0003097769513260573, -0.5269978642463684, -0.00035124807618558407, -0.004815765656530857, -0.00011932138295378536, -3.45700973412022e-05, -0.023253366351127625, -0.014092456549406052], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.items(), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "items", "()", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.017871541902422905, -5.495397272170521e-05, -0.014059897512197495, -0.4943825304508209, -0.3702628016471863, -0.10220237821340561, -0.6583489179611206, -0.04909127578139305, -0.0007977878558449447, -0.03247179836034775, -0.4106559753417969, -0.009193823672831059, -0.00043418517452664673, -0.00034683887497521937, -0.12310181558132172, -0.0023467401042580605, -0.002474105916917324, -0.007840808480978012, -0.03371504321694374], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.040007032454013824, -0.6897097229957581], "finish_reason": "stop"}], "metrics": {"energy": 144.55623215866092, "energy_per_token": 1.1293455637395384, "time": 6.9546647, "time_per_token": 0.05433331796875}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -0.4014422297477722, -0.00205163168720901, -0.021972665563225746, -0.014374163001775742, -0.11593862622976303, -0.00022182388056535274, -0.0029114973731338978], "finish_reason": "stop"}, {"text": " nltk.wordpunct_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "punct", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -1.0985040664672852, -0.00021801002731081098, -0.0008772339206188917, -0.003242237726226449, -0.0032510305754840374, -0.006029393058270216, -0.04513290897011757, -0.04307379201054573], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " [w.lower() for w in words if w.lower().startswith('$') and w not in punctuation]", "tokens": [" [", "w", ".", "lower", "()", " for", " w", " in", " words", " if", " w", ".", "lower", "()", ".", "start", "sw", "ith", "('", "$", "')", " and", " w", " not", " in", " punct", "uation", "]", "\n"], "logprobs": [-0.7446612119674683, -1.2852039337158203, -1.960846185684204, -0.22129686176776886, -0.003996956627815962, -0.09120140224695206, -0.003617767943069339, -0.00083078199531883, -0.012004158459603786, -0.04653520509600639, -0.11612173169851303, -0.5392609238624573, -1.5505142211914062, -0.3313174545764923, -0.027120115235447884, -0.05339573323726654, -3.421248038648628e-05, -1.1920928244535389e-07, -0.5641870498657227, -0.005521759856492281, -0.00022218143567442894, -1.0009539127349854, -0.8792557120323181, -2.449422836303711, -0.0006413786904886365, -0.35024842619895935, -0.00010847456724150106, -0.16191436350345612, -0.03182869404554367], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts().to_frame(\"Frequency\")", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", ".", "to", "_", "frame", "(", "\"", "F", "requency", "\"", ")", "\n"], "logprobs": [-0.8544464111328125, -8.427741704508662e-05, -0.5379802584648132, -0.0738082155585289, -0.3747662603855133, -8.713819261174649e-05, -0.0003178806509822607, -0.014761981554329395, -0.010214907117187977, -0.047312989830970764, -6.353653589030728e-05, -0.0006563892820850015, -0.042046524584293365, -2.2150542736053467, -1.591633677482605, -0.00162588432431221, -0.04778280481696129, -0.8051702976226807, -0.6246589422225952, -0.05398254096508026, -0.0033521433360874653, -0.00022289653134066612, -0.02316903881728649, -0.3510659337043762], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.02136489935219288, -5.936446541454643e-05, -0.013019083067774773, -1.6215355396270752, -0.052144259214401245, -0.009583655744791031, -0.046492647379636765, -0.6798481345176697, -4.768370445162873e-07, -0.004780411254614592, -0.04572811350226402, -0.0003885467885993421, -0.0027479289565235376, -0.08235836774110794, -0.00368131254799664, -0.0026253541000187397, -0.001257463125512004, -0.05056464672088623, -0.000562271336093545, -0.0014384171226993203, -0.02760809101164341, -0.09923552721738815, -0.007942876778542995, -0.04678585007786751], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.04301680997014046], "finish_reason": "stop"}], "metrics": {"energy": 133.89449006462098, "energy_per_token": 1.115787417205175, "time": 6.5237074, "time_per_token": 0.054364228333333334}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"Input should be a string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " should", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -1.2210054397583008, -2.3244824409484863, -0.007913191802799702, -0.19333742558956146, -0.05986228957772255, -0.5764700174331665, -0.00043335105874575675, -0.0038881907239556313], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith(\"$\")]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-0.7446612119674683, -0.555412232875824, -0.2809702754020691, -0.0025064502842724323, -0.0006363751017488539, -0.0034303173888474703, -0.048688046634197235, -0.12508267164230347, -0.4705350995063782, -0.1923176348209381, -7.83174327807501e-05, -4.768370445162873e-07, -0.9922456741333008, -0.007462246343493462, -0.0005470209871418774, -0.0006199345225468278, -0.21149347722530365, -0.018774542957544327], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts()", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-0.8856619000434875, -8.570780482841656e-05, -0.5658907890319824, -0.07873176038265228, -0.3904268741607666, -0.00012158608296886086, -0.000324673397699371, -0.01585531234741211, -0.010095486417412758, -0.06114709749817848, -8.391981828026474e-05, -0.0007619338575750589, -0.034231606870889664, -0.16799601912498474], "finish_reason": "stop"}, {"text": " pd.DataFrame(list(freq.items()), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "list", "(", "freq", ".", "items", "())", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.01777106709778309, -5.280832192511298e-05, -0.01504326518625021, -0.5175586938858032, -1.9183871746063232, -0.003211937379091978, -0.21363691985607147, -0.0007371568935923278, -0.04309491440653801, -0.017855381593108177, -0.0016174342017620802, -0.04423493519425392, -0.3912649154663086, -0.010211131535470486, -0.00039176418795250356, -0.00038795097498223186, -0.13248813152313232, -0.002303329762071371, -0.0024695871397852898, -0.00765769649296999, -0.034170664846897125], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.03740314766764641], "finish_reason": "stop"}], "metrics": {"energy": 108.04942242240908, "energy_per_token": 1.1373623412885165, "time": 5.272708, "time_per_token": 0.05550218947368421}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text is not a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " is", " not", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -1.7306087017059326, -0.007608719170093536, -0.02426113374531269, -0.022834455594420433, -0.09486642479896545, -0.0002681849291548133, -0.0037699120584875345], "finish_reason": "stop"}, {"text": " nltk.wordpunct_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "punct", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -1.0985040664672852, -0.00021801002731081098, -0.0008772339206188917, -0.003242237726226449, -0.0032510305754840374, -0.006029393058270216, -0.04513290897011757, -0.04307379201054573], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " [w for w in words if w.startswith('$') and len(w) > 1]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", ".", "start", "sw", "ith", "('", "$", "')", " and", " len", "(", "w", ")", " >", " ", "1", "]", "\n"], "logprobs": [-0.7446612119674683, -1.2852039337158203, -0.20165471732616425, -0.0012255546171218157, -0.0015065044863149524, -0.004396416246891022, -0.03854634240269661, -0.0994182676076889, -0.6226596832275391, -0.17491888999938965, -8.463501580990851e-05, -4.768370445162873e-07, -0.4559953808784485, -0.00956275686621666, -0.00036864637513644993, -0.9911702275276184, -1.9774398803710938, -0.044442370533943176, -0.036392223089933395, -0.3956395983695984, -0.11108151823282242, -0.022072836756706238, -0.04162832349538803, -0.10168541222810745, -0.041239235550165176], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts()", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-0.8544464111328125, -8.427741704508662e-05, -0.5379802584648132, -0.0738082155585289, -0.3747662603855133, -8.713819261174649e-05, -0.0003178806509822607, -0.014761981554329395, -0.010214907117187977, -0.047312989830970764, -6.353653589030728e-05, -0.0006563892820850015, -0.042046524584293365, -0.13113856315612793], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.most_common(), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "most", "_", "common", "()", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.02136489935219288, -5.936446541454643e-05, -0.013019083067774773, -0.593713104724884, -0.39290010929107666, -0.07896322757005692, -0.9123812317848206, -1.156323378381785e-05, -0.0011264891363680363, -0.1115192249417305, -0.0017736908048391342, -0.04037817195057869, -0.3558623790740967, -0.008532260544598103, -0.0004668337933253497, -0.00033706706017255783, -0.12359033524990082, -0.0013443727511912584, -0.0019469369435682893, -0.006047640927135944, -0.023227743804454803], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.033013228327035904], "finish_reason": "stop"}], "metrics": {"energy": 149.3611844596863, "energy_per_token": 1.4501085869872459, "time": 5.6821055, "time_per_token": 0.05516607281553398}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"Input must be a string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -1.2210054397583008, -0.9398485422134399, -0.001859008800238371, -0.061045609414577484, -0.016411321237683296, -0.4903161823749542, -0.00023100091493688524, -0.0025699466932564974], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith('$') and word.isalnum()]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "('", "$", "')", " and", " word", ".", "isal", "num", "()", "]", "\n"], "logprobs": [-0.7446612119674683, -0.555412232875824, -0.2809702754020691, -0.0025064502842724323, -0.0006363751017488539, -0.0034303173888474703, -0.048688046634197235, -0.12508267164230347, -0.4705350995063782, -0.1923176348209381, -7.83174327807501e-05, -4.768370445162873e-07, -0.46538639068603516, -0.008608848787844181, -0.000377583724912256, -1.4043333530426025, -0.9232367873191833, -0.9268385171890259, -1.899994134902954, -0.0002087136817863211, -0.0025074013974517584, -0.07016818970441818, -0.031140344217419624], "finish_reason": "stop"}, {"text": " nltk.FreqDist(dollar_words)", "tokens": [" n", "lt", "k", ".", "F", "req", "Dist", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-3.054426431655884, -0.03791103884577751, -0.0005927712772972882, -0.000563224486541003, -0.03924337029457092, -0.01556733250617981, -0.0004674295778386295, -0.009707978926599026, -0.025092586874961853, -1.680836794548668e-05, -5.864924969500862e-05, -0.001135776867158711, -0.02710213139653206], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.most_common())", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "most", "_", "common", "())", "\n"], "logprobs": [-0.01964014396071434, -5.07818695041351e-05, -0.013002138584852219, -0.5904743671417236, -0.3928782343864441, -0.09423359483480453, -0.7282207608222961, -1.4066597032069694e-05, -0.0011631635716184974, -2.699730396270752, -0.17339251935482025], "finish_reason": "stop"}, {"text": " df<|endoftext|>from nltk.tokenize import sent_tokenize", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "from", " n", "lt", "k", ".", "token", "ize", " import", " sent", "_", "token", "ize", "\n"], "logprobs": [-0.038579490035772324, -1.6337254047393799, -7.271740287251305e-06, -1.6689286894688848e-06, -3.85038583772257e-05, -4.851700214203447e-05, -0.07981999963521957, -1.2187501192092896, -3.2625527381896973, -0.082913838326931, -1.7046782886609435e-05, -0.22807474434375763, -0.9812999963760376, -0.0003629264247138053, -0.03306489810347557, -0.700365424156189, -9.023735765367746e-05, -0.0010518262861296535, -0.0011872155591845512, -0.4655912220478058], "finish_reason": "stop"}], "metrics": {"energy": 117.19346728706357, "energy_per_token": 1.0851246971024404, "time": 5.6801233, "time_per_token": 0.052593734259259256}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"Text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "Text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -2.4910898208618164, -0.35208860039711, -0.0012838939437642694, -0.03902338817715645, -0.01176325511187315, -0.6456002593040466, -0.0002754547167569399, -0.002804515417665243], "finish_reason": "stop"}, {"text": " nltk.wordpunct_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "punct", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -1.0985040664672852, -0.00021801002731081098, -0.0008772339206188917, -0.003242237726226449, -0.0032510305754840374, -0.006029393058270216, -0.04513290897011757, -0.04307379201054573], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.0596445798873901, -0.025069916620850563], "finish_reason": "stop"}, {"text": " [words.count(w) for w in dollar_words]", "tokens": [" [", "words", ".", "count", "(", "w", ")", " for", " w", " in", " dollar", "_", "words", "]", "\n"], "logprobs": [-2.207031488418579, -0.8752935528755188, -0.000316927267704159, -0.0008441222598776221, -0.0036526883486658335, -2.638110637664795, -0.026836173608899117, -0.006343349348753691, -0.0013450870756059885, -0.0004576589271891862, -0.060001399368047714, -1.2993727978027891e-05, -1.7762025890988298e-05, -0.014370755292475224, -0.009315123781561852], "finish_reason": "stop"}, {"text": " pd.DataFrame.from_dict(freq, orient='index', columns=['Frequency'])", "tokens": [" pd", ".", "DataFrame", ".", "from", "_", "dict", "(", "freq", ",", " orient", "='", "index", "'", ",", " columns", "=['", "F", "requency", "'])", "\n"], "logprobs": [-0.019807400181889534, -5.113947918289341e-05, -0.015020717866718769, -1.938865065574646, -0.0003634030872490257, -0.00033504117163829505, -0.038530170917510986, -0.013881330378353596, -0.030322810634970665, -0.38570818305015564, -0.02500120736658573, -0.29647478461265564, -0.0030652941204607487, -0.9156906604766846, -0.007094075437635183, -0.012805494479835033, -0.10272606462240219, -0.033696141093969345, -0.0017936823423951864, -0.0033968149218708277, -0.0859379693865776], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.034479137510061264], "finish_reason": "stop"}], "metrics": {"energy": 94.48540051651001, "energy_per_token": 1.1664864261297532, "time": 4.841752, "time_per_token": 0.05977471604938271}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError('Parameter \"text\" must be of type \"str\".')", "tokens": [" Value", "Error", "('", "Parameter", " \"", "text", "\"", " must", " be", " of", " type", " \"", "str", "\"", ".", "')", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -2.5395426750183105, -4.13466739654541, -1.839828610420227, -0.011432964354753494, -0.003377924906089902, -0.32784807682037354, -0.008415593765676022, -1.9734885692596436, -0.054728150367736816, -0.9479677081108093, -0.0481862835586071, -0.006700665690004826, -1.1808277368545532, -0.10047130286693573, -0.0020512747578322887], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.16092899441719055, -0.0068033249117434025, -1.7165990357170813e-05, -0.00658804876729846, -0.5803220272064209, -0.37004923820495605, -0.0003687655262183398, -0.0002361257211305201, -0.0027393694035708904, -0.003709698561578989, -0.04438398778438568, -0.026280242949724197], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4608280062675476, -0.00022075122979003936, -0.0009211826254613698, -0.007431126665323973, -0.00022790218645241112, -0.0026290398091077805, -0.0041859629563987255, -0.03510914370417595, -0.012430338189005852], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith(\"$\")]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-0.7689715027809143, -0.5372699499130249, -0.25305527448654175, -0.0028795229736715555, -0.0005937243695370853, -0.0030452092178165913, -0.05147530511021614, -0.10661162436008453, -0.5623965263366699, -0.17997300624847412, -0.00010609064338495955, -7.152555099310121e-07, -1.0171066522598267, -0.008381666615605354, -0.0005113962688483298, -0.0007611000328324735, -0.19363725185394287, -0.017470283433794975], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts()", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-0.7426450252532959, -8.248942322097719e-05, -0.4678784906864166, -0.08024068921804428, -0.2908048629760742, -9.7508447652217e-05, -0.0002908283786382526, -0.018725287169218063, -0.008981424383819103, -0.05246216803789139, -7.86750388215296e-05, -0.0006700892699882388, -0.04441374912858009, -0.14336249232292175], "finish_reason": "stop"}, {"text": " pd.DataFrame([[key, val] for key, val in freq.items()], columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "([", "[", "key", ",", " val", "]", " for", " key", ",", " val", " in", " fre", "q", ".", "items", "()", "]", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.01667361706495285, -5.411955135059543e-05, -0.013855465687811375, -2.646298408508301, -0.9923334717750549, -2.1341934204101562, -0.021257655695080757, -1.4676525592803955, -0.005844290368258953, -0.01606062613427639, -0.03513204678893089, -0.0012711548479273915, -0.03794007748365402, -0.002625829540193081, -0.04451272264122963, -1.1920928244535389e-07, -0.00011526874004630372, -0.05628441274166107, -0.0009004590683616698, -0.35388052463531494, -0.0016219568205997348, -0.14695046842098236, -0.34732872247695923, -0.01112319529056549, -0.000262106885202229, -0.0003579214389901608, -0.05626187473535538, -0.0014110381016507745, -0.002310703741386533, -0.008124753832817078, -0.029002850875258446], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.038811955600976944, -0.7658337354660034], "finish_reason": "stop"}], "metrics": {"energy": 159.9617750720978, "energy_per_token": 1.4410970727216017, "time": 5.8954396, "time_per_token": 0.053112068468468464}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -0.15053309500217438, -0.1289343535900116, -0.9143285155296326, -0.4014422297477722, -0.00205163168720901, -0.021972665563225746, -0.014374163001775742, -0.11593862622976303, -0.00022182388056535274, -0.0029114973731338978], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.17375779151916504, -0.007680764887481928, -1.645074735279195e-05, -0.006534636951982975, -0.6012584567070007, -0.41048914194107056, -0.00035565727739594877, -0.0002531684876885265, -0.002619646955281496, -0.0034768865443766117, -0.05111166089773178, -0.02716733328998089], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.4501659870147705, -0.00020001317898277193, -0.001158400671556592, -0.006175484042614698, -0.00024339574156329036, -0.0027589849196374416, -0.0048353406600654125, -0.03875989466905594, -0.011938665993511677], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.0596445798873901, -0.025069916620850563], "finish_reason": "stop"}, {"text": " pd.Series(list(map(lambda x: x.count('$'), dollar_words)), index=dollar_words).to_frame(\"Frequency\").reset_index(drop=False).set_columns(['Word'])", "tokens": [" pd", ".", "Series", "(", "list", "(", "map", "(", "lambda", " x", ":", " x", ".", "count", "('", "$", "')", ",", " dollar", "_", "words", "))", ",", " index", "=", "dollar", "_", "words", ")", ".", "to", "_", "frame", "(", "\"", "F", "requency", "\"", ")", ".", "reset", "_", "index", "(", "drop", "=", "False", ")", ".", "set", "_", "columns", "(['", "Word", "'])", "\n"], "logprobs": [-0.8544065356254578, -8.630380034446716e-05, -0.4608425199985504, -0.08176787942647934, -3.0129876136779785, -0.007698627654463053, -0.7941722869873047, -0.008165665902197361, -0.11028070002794266, -0.23700256645679474, -0.03516669198870659, -1.905126929283142, -0.26569050550460815, -0.662533164024353, -0.3184448778629303, -0.1267622411251068, -0.0006611545104533434, -0.1264527142047882, -0.42589136958122253, -7.986990567587782e-06, -7.83174327807501e-05, -1.7665938138961792, -0.018806949257850647, -0.7696020603179932, -0.10571111738681793, -0.03560082986950874, -1.9192511899746023e-05, -8.583032467868179e-06, -0.07962935417890549, -1.6479110717773438, -2.6887288093566895, -0.000179036331246607, -0.02126104012131691, -0.7723180055618286, -1.4200994968414307, -0.019731314852833748, -0.0034035868011415005, -0.00020132421923335642, -0.004114971030503511, -1.1793240308761597, -0.5010840892791748, -2.7418097943154862e-06, -0.00022384998737834394, -0.5949929356575012, -1.2259435653686523, -0.02873704954981804, -0.4115699827671051, -0.21003492176532745, -2.9987030029296875, -1.7228121757507324, -0.00014911970356479287, -1.9375643730163574, -1.6132285594940186, -0.01256160531193018, -0.5357675552368164, -0.0653233677148819], "finish_reason": "stop"}, {"text": " pd.DataFrame([(word, freq[word]) for word in freq], columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "([", "(", "word", ",", " fre", "q", "[", "word", "])", " for", " word", " in", " fre", "q", "]", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.019807400181889534, -5.113947918289341e-05, -0.015020717866718769, -2.690305233001709, -1.2277723550796509, -0.6807767748832703, -0.08524954319000244, -0.11990345269441605, -5.876845170860179e-05, -0.3410834074020386, -0.0027518521528691053, -0.012869163416326046, -0.017380546778440475, -0.004536100197583437, -0.01265848521143198, -0.3050716817378998, -0.0013952055014669895, -1.0492686033248901, -0.0015168600948527455, -0.0982389971613884, -0.3473582863807678, -0.011907449923455715, -0.0002675890573300421, -0.0003349220205564052, -0.06366077065467834, -0.0015224544331431389, -0.002662092447280884, -0.01077181939035654, -0.06332369148731232], "finish_reason": "stop"}, {"text": " df.set_index(\"Word\")", "tokens": [" df", ".", "set", "_", "index", "(", "\"", "Word", "\"", ")", "\n"], "logprobs": [-0.034479137510061264, -3.6096739768981934, -1.3370312452316284, -0.0001784403866622597, -6.735097849741578e-05, -0.4051353931427002, -0.021712107583880424, -0.0033267175313085318, -4.470248313737102e-05, -0.20903143286705017, -0.44235092401504517], "finish_reason": "stop"}], "metrics": {"energy": 156.13463859272, "energy_per_token": 1.12327078124259, "time": 7.3629045, "time_per_token": 0.052970535971223025}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.26442864537239075, -0.02714262157678604, -0.0005301024066284299, -0.0005719218170270324, -0.00048756631440483034, -0.025785112753510475, -0.006315985694527626, -0.013876392506062984], "finish_reason": "stop"}, {"text": " ValueError('Input must be a string.')", "tokens": [" Value", "Error", "('", "Input", " must", " be", " a", " string", ".", "')", "\n"], "logprobs": [-0.023659877479076385, -4.327203714638017e-05, -2.5395426750183105, -1.2084379196166992, -0.9441555738449097, -0.002994103590026498, -0.11755254119634628, -0.035894256085157394, -1.5755841732025146, -0.03447844460606575, -0.002018558792769909], "finish_reason": "stop"}, {"text": " nltk.tokenize.wordpunct_tokenize(text)", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "punct", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.16092899441719055, -0.0068033249117434025, -1.7165990357170813e-05, -0.00658804876729846, -1.3535325527191162, -0.000436925794929266, -0.001301514101214707, -0.4412412941455841, -0.215386301279068, -2.52720492426306e-05, -0.006119087804108858, -0.010507263243198395, -0.0023972841445356607, -0.006504199001938105, -0.04968276992440224, -0.04239416867494583], "finish_reason": "stop"}, {"text": " nltk.corpus.words.words(text)", "tokens": [" n", "lt", "k", ".", "cor", "pus", ".", "words", ".", "words", "(", "text", ")", "\n"], "logprobs": [-2.585994005203247, -1.0925590991973877, -0.00015746307326480746, -0.000259723310591653, -2.994832992553711, -7.390703103737906e-05, -0.00032431588624604046, -0.7504048347473145, -0.18734999001026154, -0.19152773916721344, -2.314608335494995, -0.9921921491622925, -0.319968044757843, -0.04240136593580246], "finish_reason": "stop"}, {"text": " list(filter(lambda word: word[0]==\"$\", words))", "tokens": [" list", "(", "filter", "(", "lambda", " word", ":", " word", "[", "0", "]", "==", "\"", "$", "\"", ",", " words", "))", "\n"], "logprobs": [-2.3304920196533203, -0.18544159829616547, -0.10713396966457367, -0.012783955782651901, -0.028695696964859962, -2.059622287750244, -0.015583998523652554, -0.19741719961166382, -0.7491140365600586, -0.0030469917692244053, -0.008299031294882298, -2.6858444213867188, -1.5702931880950928, -0.0004932855372317135, -4.207999518257566e-05, -0.1495818942785263, -0.07931456714868546, -0.008996074087917805, -0.06290376931428909], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts().reset_index(name='Frequency').set_index(\"Word\")", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", ".", "reset", "_", "index", "(", "name", "='", "F", "requency", "')", ".", "set", "_", "index", "(", "\"", "Word", "\"", ")", "\n"], "logprobs": [-0.8566550016403198, -8.546940807718784e-05, -0.5338372588157654, -0.07058662176132202, -0.3295555114746094, -7.60526381782256e-05, -0.00024828212917782366, -0.019031541422009468, -0.009626987390220165, -0.049360569566488266, -0.00010215714428341016, -0.0009253510506823659, -0.043314576148986816, -2.0000967979431152, -1.2057675123214722, -3.2782016205601394e-05, -9.464769391342998e-05, -0.5047838687896729, -0.028769945725798607, -0.4267287850379944, -0.05850611999630928, -0.006610904820263386, -0.0029122107662260532, -1.752880573272705, -1.5257952213287354, -4.935142715112306e-05, -0.016341542825102806, -1.798573613166809, -0.060526780784130096, -0.0020281951874494553, -0.00019333878299221396, -0.023970210924744606, -0.038237519562244415], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.items(), columns = ['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "items", "()", ",", " columns", " =", " ['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.019393591210246086, -5.590759246842936e-05, -0.013296151533722878, -0.5730448961257935, -0.4102439880371094, -0.09207494556903839, -0.5663658380508423, -0.0572371631860733, -0.0008391196606680751, -0.03186633810400963, -2.1430158615112305, -0.37397176027297974, -0.011050572618842125, -0.000782183778937906, -0.0020967184100300074, -0.16210658848285675, -0.0027096483390778303, -0.002907693851739168, -0.009096961468458176, -0.02941235713660717], "finish_reason": "stop"}, {"text": " df.sort_values(by='Frequency', ascending=False)", "tokens": [" df", ".", "sort", "_", "values", "(", "by", "='", "F", "requency", "'", ",", " ascending", "=", "False", ")", "\n"], "logprobs": [-0.03719056770205498, -3.660602331161499, -0.5048244595527649, -0.0038158714305609465, -0.11228272318840027, -0.20195896923542023, -0.4971645176410675, -0.9269707202911377, -0.07814507931470871, -9.965400386136025e-05, -0.018663745373487473, -0.00028320116689428687, -0.02978813834488392, -0.03391072899103165, -0.008907699957489967, -0.42931482195854187, -0.19759438931941986], "finish_reason": "stop"}], "metrics": {"energy": 164.18403507423395, "energy_per_token": 1.1897393845958981, "time": 7.5749464, "time_per_token": 0.05489091594202899}}
