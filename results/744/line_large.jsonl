{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Input must be a string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -1.250410795211792, -0.0017625049222260714, -0.10377573221921921, -0.01786966808140278, -0.25808170437812805, -0.00039426659350283444, -0.009650364518165588], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -0.05714777112007141, -0.015522966161370277, -0.010418668389320374, -0.03339134901762009, -0.02971547096967697], "finish_reason": "stop"}, {"text": " [word for word in tk.tokenize(text) if word.startswith(\"$\")]", "tokens": [" [", "word", " for", " word", " in", " t", "k", ".", "token", "ize", "(", "text", ")", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-1.8076647520065308, -0.7185527682304382, -0.11693189293146133, -0.0010091932490468025, -0.00040797016117721796, -0.01012109499424696, -9.131014667218551e-05, -3.862306402879767e-05, -0.0001399419124936685, -1.1324817933200393e-05, -0.0005316513124853373, -0.004666391294449568, -0.06704441457986832, -0.07774291932582855, -0.20077133178710938, -0.3791157305240631, -0.044429827481508255, -2.3841574147809297e-05, -2.3841855067985307e-07, -0.27737119793891907, -0.006321671884506941, -0.000205018965061754, -3.8742269680369645e-05, -0.1506204605102539, -0.020348601043224335], "finish_reason": "stop"}, {"text": " [word for word in words if word[0]==\"$\"]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", "[", "0", "]", "==", "\"", "$", "\"", "]", "\n"], "logprobs": [-0.5442787408828735, -0.5437167286872864, -0.11734000593423843, -0.0013097288319841027, -0.00030214988510124385, -0.005751252174377441, -0.017451424151659012, -0.10041782259941101, -2.045433759689331, -0.0005322470096871257, -0.023603644222021103, -2.309274911880493, -0.7344046831130981, -0.00024673278676345944, -3.45700973412022e-05, -0.3805994987487793, -0.08751033991575241], "finish_reason": "stop"}, {"text": " len(dollar_words)", "tokens": [" len", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-4.047060012817383, -0.07204332202672958, -0.009619667194783688, -6.198863957251888e-06, -8.511180931236595e-05, -0.09788300096988678, -0.2314087450504303], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq, index=['']).T.reset_index()", "tokens": [" pd", ".", "DataFrame", "(", "freq", ",", " index", "=[", "''", "])", ".", "T", ".", "reset", "_", "index", "()", "\n"], "logprobs": [-0.006788169499486685, -2.3603161025675945e-05, -0.0023737368173897266, -0.9120269417762756, -0.7487871646881104, -1.4071221351623535, -0.030816741287708282, -0.6040518283843994, -0.36563393473625183, -0.009579169563949108, -0.07416635751724243, -0.12245465070009232, -0.048691339790821075, -0.13337913155555725, -2.861018856492592e-06, -8.344646857949556e-07, -0.010596675798296928, -0.5663682222366333], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.004499905277043581, -0.8033377528190613], "finish_reason": "stop"}], "metrics": {"energy": 470.04796254348787, "energy_per_token": 4.65394022320285, "time": 12.205388, "time_per_token": 0.12084542574257424}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0905691385269165, -0.3392646312713623, -0.002661854727193713, -0.11764993518590927, -0.019427379593253136, -0.1159302368760109, -0.0005416594794951379, -0.0068497369065880775], "finish_reason": "stop"}, {"text": " nltk.TweetTokenizer()", "tokens": [" n", "lt", "k", ".", "T", "weet", "Token", "izer", "()", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -3.35811185836792, -0.020682966336607933, -6.389413465512916e-05, -4.6491513785440475e-06, -0.6273066401481628, -0.08369871228933334], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " [word for word in words if len(word) > 1 and word[0] == '$']", "tokens": [" [", "word", " for", " word", " in", " words", " if", " len", "(", "word", ")", " >", " ", "1", " and", " word", "[", "0", "]", " ==", " '$", "']", "\n"], "logprobs": [-0.5513269305229187, -0.462419331073761, -0.11415238678455353, -0.001311990898102522, -0.00030214988510124385, -0.008055459707975388, -0.017151247709989548, -2.889415740966797, -0.0007643162389285862, -0.013533392921090126, -0.1908414512872696, -0.20466464757919312, -0.01242292020469904, -0.8535575866699219, -0.02851937711238861, -0.023899678140878677, -0.3937704265117645, -0.0006444760947488248, -0.004066057503223419, -0.04033821076154709, -1.2181450128555298, -0.0010704031446948647, -0.08837193995714188], "finish_reason": "stop"}, {"text": " pd.DataFrame(pd.Series(dollar_words).value_counts()).reset_index()", "tokens": [" pd", ".", "DataFrame", "(", "pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "())", ".", "reset", "_", "index", "()", "\n"], "logprobs": [-1.548614501953125, -6.151010165922344e-05, -2.0043437480926514, -0.4270267188549042, -1.6107618808746338, -1.9550132492440753e-05, -0.35757070779800415, -0.0018648391123861074, -0.06628774106502533, -3.814689989667386e-06, -4.529942543740617e-06, -0.01027815230190754, -0.0014966250164434314, -0.0006962973857298493, -5.960462772236497e-07, -0.00012373158824630082, -0.2832340598106384, -0.11604870855808258, -0.004586889408528805, -3.814689989667386e-06, -4.768370445162873e-07, -0.02399604767560959, -0.22972972691059113], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.007943941280245781, -2.7179348762729205e-05, -0.0027128581423312426, -0.7783971428871155, -0.10964590311050415, -0.004556036554276943, -0.017713213339447975, -0.2392435520887375, -3.576278118089249e-07, -0.0013859200989827514, -0.01897071860730648, -0.0045927041210234165, -0.001046348363161087, -0.06518901884555817, -0.002232917817309499, -0.0007570500019937754, -0.0010338444262742996, -0.04502875730395317, -1.9073468138230965e-06, -5.8887653722194955e-05, -0.0008150592911988497, -0.010225645266473293, -0.005596919916570187, -0.024322913959622383], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.004490885883569717, -0.9328168034553528], "finish_reason": "stop"}], "metrics": {"energy": 496.9677611331936, "energy_per_token": 4.4771870372359786, "time": 12.832515, "time_per_token": 0.11560824324324324}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Input text must be a string.\")", "tokens": [" Value", "Error", "(", "\"", "Input", " text", " must", " be", " a", " string", ".", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -0.9037979245185852, -0.18261530995368958, -0.0008575814426876605, -0.04289039596915245, -0.016670802608132362, -1.0645644664764404, -0.009959882125258446, -0.0009647720726206899, -0.0027544675394892693], "finish_reason": "stop"}, {"text": " nltk.tokenize.WordPunctTokenizer()", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "Word", "Punct", "Token", "izer", "()", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -1.0804400444030762, -0.0008142255246639252, -0.012112634256482124, -1.491560935974121, -0.04228184372186661, -0.0005368936690501869, -1.8954096958623268e-05, -0.008480843156576157, -0.1399821788072586], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " [w for w in words if w.startswith(\"$\")]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-0.5513269305229187, -1.3396928310394287, -0.09144708514213562, -0.0020459212828427553, -0.00035958975786343217, -0.004531471990048885, -0.012178703211247921, -0.10250820964574814, -0.15072935819625854, -0.02767917327582836, -4.23184028477408e-05, -2.3841855067985307e-07, -0.5475481152534485, -0.0029740172903984785, -0.0001658063702052459, -3.4689302992774174e-05, -0.757683277130127, -0.09793553501367569], "finish_reason": "stop"}, {"text": " nltk.FreqDist(dollar_words)", "tokens": [" n", "lt", "k", ".", "F", "req", "Dist", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-1.7362940311431885, -6.41325386823155e-05, -2.396077979938127e-05, -9.035655966727063e-05, -0.05110815167427063, -2.8132995794294402e-05, -3.2782016205601394e-05, -0.0015445933677256107, -0.001910886145196855, -1.0490362910786644e-05, -3.886147169396281e-05, -0.00019786784832831472, -0.0621245801448822], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.007293148431926966, -2.1934269170742482e-05, -0.0025154873728752136, -0.7516798377037048, -0.11366508156061172, -0.004291017074137926, -0.016296977177262306, -0.3050481975078583, -3.576278118089249e-07, -0.0015574480639770627, -0.019461285322904587, -0.00463874451816082, -0.0012486526975408196, -0.07124791294336319, -0.0023505459539592266, -0.0006428082706406713, -0.00105408881790936, -0.043113067746162415, -1.6689286894688848e-06, -6.55629628454335e-05, -0.00119066855404526, -0.008768743835389614, -0.004977928940206766, -0.02547580748796463], "finish_reason": "stop"}, {"text": " df<|endoftext|>#!/usr/bin/env python3", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "#", "!", "/", "usr", "/", "bin", "/", "env", " python", "3", "\n"], "logprobs": [-0.006447466555982828, -1.152741551399231, -7.510157047363464e-06, -7.390948667307384e-06, -2.264974000354414e-06, -2.264974000354414e-06, -0.10459283739328384, -1.1757431030273438, -1.2780466079711914, -0.0711335614323616, -0.06406896561384201, -7.462222856702283e-05, -0.010071411728858948, -0.0002889215829782188, -0.07871996611356735, -0.004701512400060892, -0.4732038378715515, -0.014802035875618458], "finish_reason": "stop"}], "metrics": {"energy": 514.9494212226869, "energy_per_token": 4.401277104467409, "time": 14.301123, "time_per_token": 0.12223182051282051}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError('text is not a string')", "tokens": [" Value", "Error", "('", "text", " is", " not", " a", " string", "')", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -1.936584234237671, -0.6128314733505249, -1.3485839366912842, -0.016269532963633537, -0.05713820084929466, -0.013147920370101929, -0.0636698305606842, -0.0058016241528093815], "finish_reason": "stop"}, {"text": " nltk.tokenize.word_tokenize(text, language='english')", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "_", "token", "ize", "(", "text", ",", " language", "='", "eng", "lish", "')", "\n"], "logprobs": [-0.08111252635717392, -2.7417760065873154e-05, -4.172316494077677e-06, -0.0017880895175039768, -0.9599440693855286, -0.0005996812833473086, -0.013653579168021679, -0.8363466858863831, -0.023535648360848427, -0.0007727735792286694, -0.3824978172779083, -0.02781633846461773, -0.012508982792496681, -2.0440673828125, -0.14952564239501953, -1.0723158121109009, -0.051039282232522964, -1.2040065485052764e-05, -0.017407024279236794, -0.020919397473335266], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.20826326310634613, -7.295342220459133e-05, -2.4437606043647975e-05, -0.00011085849109804258, -5.8412379075889476e-06, -0.0004700509598478675, -0.0018980359891429543, -0.010654824785888195, -0.02220284566283226], "finish_reason": "stop"}, {"text": " [w for w in words if w.startswith('$') and not set(w).issubset(punctuation)]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", ".", "start", "sw", "ith", "('", "$", "')", " and", " not", " set", "(", "w", ")", ".", "iss", "ub", "set", "(", "punct", "uation", ")]", "\n"], "logprobs": [-0.5221788287162781, -1.326674222946167, -0.08757368475198746, -0.0018310461891815066, -0.0004481264913920313, -0.004632930271327496, -0.011212906800210476, -0.10684583336114883, -0.16000784933567047, -0.02905275858938694, -4.434487345861271e-05, -2.3841855067985307e-07, -0.8885524868965149, -0.002103260951116681, -0.00021264675888232887, -0.7025689482688904, -0.4760223925113678, -1.7581324577331543, -0.008637921884655952, -0.21345262229442596, -0.4347281754016876, -0.4062236249446869, -0.020905153825879097, -1.1920928244535389e-07, -1.1920928244535389e-07, -0.010542533360421658, -1.216355562210083, -3.6954811548639555e-06, -0.021112818270921707, -0.09576623886823654], "finish_reason": "stop"}, {"text": " [sum([word == word_dc for word in words]) for word_dc in dollar_words]", "tokens": [" [", "sum", "([", "word", " ==", " word", "_", "dc", " for", " word", " in", " words", "])", " for", " word", "_", "dc", " in", " dollar", "_", "words", "]", "\n"], "logprobs": [-3.114649534225464, -3.5704832077026367, -1.8029932975769043, -1.7086573839187622, -0.13315449655056, -2.979829788208008, -0.5420802235603333, -3.9475624561309814, -0.035157252103090286, -0.0004138090298511088, -0.08514203876256943, -0.40113383531570435, -0.006229151971638203, -0.01307273842394352, -0.0016814630944281816, -0.00043847484630532563, -1.0251946150674485e-05, -0.00013326710904948413, -0.13740575313568115, -3.576272320060525e-06, -4.0531076592742465e-06, -0.00931689515709877, -0.05188974738121033], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.007679700385779142, -2.098061486321967e-05, -0.0023178397677838802, -0.9001435041427612, -0.11158256232738495, -0.004858473781496286, -0.017469581216573715, -0.25296592712402344, -3.576278118089249e-07, -0.0015783959534019232, -0.016930218786001205, -0.0038484097458422184, -0.0010171722387894988, -0.07309388369321823, -0.002801305614411831, -0.0007600279641337693, -0.0015499495202675462, -0.043911319226026535, -2.0265558760002023e-06, -6.09140915912576e-05, -0.0010439666220918298, -0.008542543277144432, -0.005415886640548706, -0.02756819874048233], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.004489936400204897], "finish_reason": "stop"}], "metrics": {"energy": 558.6111371021273, "energy_per_token": 4.468889096817018, "time": 15.55599, "time_per_token": 0.12444791999999999}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Input text is not a string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " text", " is", " not", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -0.9037979245185852, -2.6991138458251953, -0.012588092125952244, -0.04786178469657898, -0.013428599573671818, -0.39661192893981934, -0.0009205871028825641, -0.006299519911408424], "finish_reason": "stop"}, {"text": " nltk.TweetTokenizer()", "tokens": [" n", "lt", "k", ".", "T", "weet", "Token", "izer", "()", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -3.35811185836792, -0.020682966336607933, -6.389413465512916e-05, -4.6491513785440475e-06, -0.6273066401481628, -0.08369871228933334], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.5703012943267822, -0.03230549395084381], "finish_reason": "stop"}, {"text": " [dollar_words.count(word) for word in set(dollar_words)]", "tokens": [" [", "dollar", "_", "words", ".", "count", "(", "word", ")", " for", " word", " in", " set", "(", "dollar", "_", "words", ")]", "\n"], "logprobs": [-3.1366379261016846, -0.6696415543556213, -0.0002964295563288033, -0.000402250443585217, -0.0002232540718978271, -3.0040289857424796e-05, -0.0008738989708945155, -0.14599698781967163, -0.0022689569741487503, -0.0033462028950452805, -0.0009098681039176881, -0.00019035911827813834, -1.5610405206680298, -0.0005335576133802533, -4.124556289752945e-05, -1.0728830375228426e-06, -6.318072337307967e-06, -0.011633649468421936, -0.034142669290304184], "finish_reason": "stop"}, {"text": " pd.DataFrame({", "tokens": [" pd", ".", "DataFrame", "({", "\n"], "logprobs": [-0.009083377197384834, -2.407998726994265e-05, -0.0030852595809847116, -0.5836488604545593, -2.2248342037200928], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.0052426476031541824, -0.9330222606658936], "finish_reason": "stop"}], "metrics": {"energy": 392.72084741973913, "energy_per_token": 5.775306579702046, "time": 10.51261, "time_per_token": 0.15459720588235296}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0905691385269165, -0.3392646312713623, -0.002661854727193713, -0.11764993518590927, -0.019427379593253136, -0.1159302368760109, -0.0005416594794951379, -0.0068497369065880775], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -0.05714777112007141, -0.015522966161370277, -0.010418668389320374, -0.03339134901762009, -0.02971547096967697], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith('$') and not all([c in punctuation for c in word[1:]])]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "('", "$", "')", " and", " not", " all", "([", "c", " in", " punct", "uation", " for", " c", " in", " word", "[", "1", ":", "]]", ")]", "\n"], "logprobs": [-0.5513269305229187, -0.462419331073761, -0.11415238678455353, -0.001311990898102522, -0.00030214988510124385, -0.008055459707975388, -0.017151247709989548, -0.1245269849896431, -0.16697020828723907, -0.02148263156414032, -3.111314072157256e-05, -1.1920928244535389e-07, -1.0156010389328003, -0.0015447123441845179, -0.0001515035255579278, -0.8606828451156616, -0.4755958914756775, -0.6197057962417603, -1.8389264345169067, -1.9923186302185059, -0.0025106121320277452, -0.008561808615922928, -1.0371154530730564e-05, -0.003988289274275303, -0.00018785618885885924, -0.00019167017308063805, -0.00381112121976912, -1.7413880825042725, -0.0019002966582775116, -0.04077457636594772, -0.0007549058645963669, -0.027273597195744514, -0.0870816707611084], "finish_reason": "stop"}, {"text": " Counter(dollar_words)", "tokens": [" Counter", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-2.262359619140625, -0.022181391716003418, -0.0037376086693257093, -7.033323527139146e-06, -3.802703940891661e-05, -0.00015770144818816334, -0.08983632922172546], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.006113874725997448, -2.455681169521995e-05, -0.0018250966677442193, -0.8084314465522766, -0.12529639899730682, -0.004864405374974012, -0.018456507474184036, -0.2754996418952942, -3.576278118089249e-07, -0.0016106502152979374, -0.01478547416627407, -0.004764394834637642, -0.0013269914779812098, -0.07197000086307526, -0.0024126251228153706, -0.0008163695456460118, -0.0014334175502881408, -0.04056786373257637, -1.6689286894688848e-06, -5.245071224635467e-05, -0.0007120219524949789, -0.007979066111147404, -0.0048220534808933735, -0.022894926369190216], "finish_reason": "stop"}, {"text": " df<|endoftext|>import nltk", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "import", " n", "lt", "k", "\n"], "logprobs": [-0.0046996138989925385, -1.1917823553085327, -8.106198947643861e-06, -6.198863957251888e-06, -2.0265558760002023e-06, -2.861018856492592e-06, -0.13960424065589905, -1.0293738842010498, -1.0778108835220337, -0.002500504720956087, -3.504691630951129e-05, -0.028152158483862877], "finish_reason": "stop"}], "metrics": {"energy": 491.635106142044, "energy_per_token": 4.202009454205504, "time": 13.042953, "time_per_token": 0.11147823076923077}}
{"outputs": [{"text": " type(text) is not str:", "tokens": [" type", "(", "text", ")", " is", " not", " str", ":", "\n"], "logprobs": [-2.0513792037963867, -0.0005845506675541401, -0.0005514293443411589, -0.00015162272029556334, -1.7009313106536865, -0.03422907367348671, -0.002559007378295064, -0.008963702246546745, -0.04648934677243233], "finish_reason": "stop"}, {"text": " ValueError(\"Input text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -0.9037979245185852, -0.18261530995368958, -0.0008575814426876605, -0.04289039596915245, -0.016670802608132362, -0.4888504147529602, -0.0003524397616274655, -0.0039001840632408857], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -0.05714777112007141, -0.015522966161370277, -0.010418668389320374, -0.03339134901762009, -0.02971547096967697], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-2.5120813846588135, -0.058837298303842545], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-1.6890995502471924, -0.013232861645519733], "finish_reason": "stop"}, {"text": " pd.DataFrame(columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.009083377197384834, -2.407998726994265e-05, -0.0030852595809847116, -1.201690912246704, -2.268049955368042, -0.44721028208732605, -0.003726682160049677, -0.00032824851223267615, -0.0005539313424378633, -0.060630567371845245, -0.0017473918851464987, -0.0010519453790038824, -0.018507882952690125, -0.01608455553650856], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.004608367569744587, -0.8034664988517761], "finish_reason": "stop"}], "metrics": {"energy": 340.0340623989106, "energy_per_token": 5.397366069823978, "time": 10.101153, "time_per_token": 0.1603357619047619}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Input must be a string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -1.250410795211792, -0.0017625049222260714, -0.10377573221921921, -0.01786966808140278, -0.25808170437812805, -0.00039426659350283444, -0.009650364518165588], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -0.05714777112007141, -0.015522966161370277, -0.010418668389320374, -0.03339134901762009, -0.02971547096967697], "finish_reason": "stop"}, {"text": " [word for word in tk.tokenize(text) if word.startswith('$')]", "tokens": [" [", "word", " for", " word", " in", " t", "k", ".", "token", "ize", "(", "text", ")", " if", " word", ".", "start", "sw", "ith", "('", "$", "')", "]", "\n"], "logprobs": [-1.8076647520065308, -0.7185527682304382, -0.11693189293146133, -0.0010091932490468025, -0.00040797016117721796, -0.01012109499424696, -9.131014667218551e-05, -3.862306402879767e-05, -0.0001399419124936685, -1.1324817933200393e-05, -0.0005316513124853373, -0.004666391294449568, -0.06704441457986832, -0.07774291932582855, -0.20077133178710938, -0.3791157305240631, -0.044429827481508255, -2.3841574147809297e-05, -2.3841855067985307e-07, -1.419468641281128, -0.001177571015432477, -0.00011216964776394889, -0.1758829802274704, -0.01942363753914833], "finish_reason": "stop"}, {"text": " [w for w in words if w.startswith('$') and not all(c in punctuation for c in w)]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", ".", "start", "sw", "ith", "('", "$", "')", " and", " not", " all", "(", "c", " in", " punct", "uation", " for", " c", " in", " w", ")]", "\n"], "logprobs": [-0.5442787408828735, -1.2696917057037354, -0.07241097092628479, -0.0016927688848227262, -0.0003355178632773459, -0.004772225394845009, -0.010970043949782848, -0.09498492628335953, -0.15826480090618134, -0.02425101026892662, -5.018585216021165e-05, -3.576278118089249e-07, -0.8380946516990662, -0.0017985610757023096, -0.00019870213873218745, -0.6510297060012817, -0.45520421862602234, -0.5729415416717529, -0.2511558532714844, -0.23398979008197784, -0.0009084388730116189, -0.00896701030433178, -1.168244216387393e-05, -0.003184131346642971, -0.0001147919538198039, -0.00014137222024146467, -0.0028118856716901064, -0.36510753631591797, -0.06867591291666031], "finish_reason": "stop"}, {"text": " nltk.FreqDist(dollar_words)", "tokens": [" n", "lt", "k", ".", "F", "req", "Dist", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-1.7307679653167725, -6.09140915912576e-05, -1.7165990357170813e-05, -7.581423415103927e-05, -0.04374142736196518, -2.3841574147809297e-05, -3.1470757676288486e-05, -0.002309038769453764, -0.002157504903152585, -9.059865078597795e-06, -4.875540980719961e-05, -0.00018404220463708043, -0.06612752377986908], "finish_reason": "stop"}, {"text": " pd.DataFrame({\"Word\": freq.keys(), \"Frequency\": freq.values()})", "tokens": [" pd", ".", "DataFrame", "(", "{\"", "Word", "\":", " fre", "q", ".", "keys", "()", ",", " \"", "F", "requency", "\":", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.005957346875220537, -2.074220174108632e-05, -0.002542241709306836, -0.9830965399742126, -1.5117828845977783, -0.001844849088229239, -0.03629853576421738, -0.22765198349952698, -3.576278118089249e-07, -0.0012822272256016731, -0.018766822293400764, -0.005005328916013241, -0.001166854752227664, -0.0870102047920227, -0.0029867347329854965, -0.000649956171400845, -0.0010070496937260032, -0.05003014951944351, -1.311301275563892e-06, -5.304672595229931e-05, -0.0019376566633582115, -0.013462002389132977, -0.008533797226846218, -0.02800043299794197], "finish_reason": "stop"}, {"text": " df<|endoftext|>from nltk.tokenize import word_tokenize", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "from", " n", "lt", "k", ".", "token", "ize", " import", " word", "_", "token", "ize", "\n"], "logprobs": [-0.005338697694242001, -1.1565847396850586, -8.940656698541716e-06, -6.556489552167477e-06, -1.7881377516459906e-06, -2.861018856492592e-06, -0.1151595339179039, -1.6204802989959717, -2.323192596435547, -0.013942933641374111, -0.0002535260282456875, -0.19240300357341766, -1.0324580669403076, -4.851700214203447e-05, -0.03327927738428116, -1.3820849657058716, -0.023558586835861206, -0.00018130090029444546, -0.0018500846344977617, -0.14954546093940735], "finish_reason": "stop"}], "metrics": {"energy": 622.5661831359862, "energy_per_token": 4.384268895323847, "time": 16.829073, "time_per_token": 0.11851459859154931}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError('text must be a string')", "tokens": [" Value", "Error", "('", "text", " must", " be", " a", " string", "')", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -1.936584234237671, -0.6128314733505249, -0.5977275371551514, -0.003903271397575736, -0.17063313722610474, -0.026956049725413322, -0.06626208126544952, -0.006395113188773394], "finish_reason": "stop"}, {"text": " nltk.tokenize.word_tokenize(text, language=\"english\")", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "_", "token", "ize", "(", "text", ",", " language", "=", "\"", "eng", "lish", "\"", ")", "\n"], "logprobs": [-0.08111252635717392, -2.7417760065873154e-05, -4.172316494077677e-06, -0.0017880895175039768, -0.9599440693855286, -0.0005996812833473086, -0.013653579168021679, -0.8363466858863831, -0.023535648360848427, -0.0007727735792286694, -0.3824978172779083, -0.02781633846461773, -0.012508982792496681, -2.0440673828125, -0.14952564239501953, -0.4679495096206665, -0.003548636334016919, -0.27202481031417847, -1.0132738680113107e-05, -0.0001679517881711945, -0.012739702127873898, -0.018342982977628708], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.20826326310634613, -7.295342220459133e-05, -2.4437606043647975e-05, -0.00011085849109804258, -5.8412379075889476e-06, -0.0004700509598478675, -0.0018980359891429543, -0.010654824785888195, -0.02220284566283226], "finish_reason": "stop"}, {"text": " [w for w in words if w.startswith(\"$\") and not all(c in punctuation for c in w)]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")", " and", " not", " all", "(", "c", " in", " punct", "uation", " for", " c", " in", " w", ")]", "\n"], "logprobs": [-0.5221788287162781, -1.326674222946167, -0.08757368475198746, -0.0018310461891815066, -0.0004481264913920313, -0.004632930271327496, -0.011212906800210476, -0.10684583336114883, -0.16000784933567047, -0.02905275858938694, -4.434487345861271e-05, -2.3841855067985307e-07, -0.5302684903144836, -0.0028293600771576166, -0.00011848701251437888, -3.635817120084539e-05, -0.6520695686340332, -0.05253071337938309, -0.5430910587310791, -0.8126282095909119, -0.23323120176792145, -0.2450246959924698, -0.0009240409708581865, -0.009648711420595646, -1.2278481335670222e-05, -0.002481121802702546, -0.00014041867689229548, -0.00017176583060063422, -0.002683611586689949, -0.31883060932159424, -0.07346878200769424], "finish_reason": "stop"}, {"text": " nltk.FreqDist(dollar_words)", "tokens": [" n", "lt", "k", ".", "F", "req", "Dist", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-1.6976935863494873, -5.519237674889155e-05, -2.407998726994265e-05, -0.0001012035645544529, -0.03842394798994064, -2.5987286790041253e-05, -2.753696753643453e-05, -0.0021923573222011328, -0.0016646826406940818, -1.1205610462639015e-05, -3.099393507000059e-05, -0.0002112165529979393, -0.06360170990228653], "finish_reason": "stop"}, {"text": " pd.DataFrame.from_dict(freq, orient='index', columns=['Frequency']).reset_index()", "tokens": [" pd", ".", "DataFrame", ".", "from", "_", "dict", "(", "freq", ",", " orient", "='", "index", "'", ",", " columns", "=['", "F", "requency", "'])", ".", "reset", "_", "index", "()", "\n"], "logprobs": [-0.007184503600001335, -2.6225699912174605e-05, -0.00236969324760139, -2.267700672149658, -1.585470999998506e-05, -1.2874520507466514e-05, -0.019622961059212685, -0.001341991825029254, -0.1097165197134018, -0.0050642783753573895, -0.030246363952755928, -0.2896982431411743, -0.0003734129713848233, -0.11864414811134338, -0.0006292270263656974, -0.016959285363554955, -0.1021789163351059, -0.01713109202682972, -0.0007429938414134085, -0.00935598649084568, -0.8901780247688293, -0.07031220197677612, -3.576278118089249e-07, 0.0, -0.004351314157247543, -1.2709989547729492], "finish_reason": "stop"}, {"text": " df<|endoftext|>import numpy as np", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "import", " numpy", " as", " np", "\n"], "logprobs": [-0.004156878218054771, -1.1437327861785889, -6.6756979322235566e-06, -7.152531907195225e-06, -1.9073468138230965e-06, -2.622600959512056e-06, -0.11545389145612717, -0.9984676837921143, -2.679377555847168, -0.017566220834851265, -0.0027580340392887592, -0.007790420204401016], "finish_reason": "stop"}], "metrics": {"energy": 561.8304229536059, "energy_per_token": 4.288781854607678, "time": 15.573408, "time_per_token": 0.11888097709923665}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"The text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "The", " text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -3.026604175567627, -1.597090721130371, -1.56290864944458, -0.006511660758405924, -0.13489776849746704, -0.019317589700222015, -0.8012087345123291, -0.0006606780225411057, -0.005099029745906591], "finish_reason": "stop"}, {"text": " nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "T", "weet", "Token", "izer", "(", "strip", "_", "hand", "les", "=", "True", ",", " reduce", "_", "len", "=", "True", ")", "\n"], "logprobs": [-0.0847516655921936, -2.253030106658116e-05, -4.0531076592742465e-06, -0.001347110839560628, -0.8215465545654297, -0.0008317348547279835, -0.016171947121620178, -1.9676439762115479, -0.0049257357604801655, -8.713819261174649e-05, -2.9802276912960224e-06, -0.6723558902740479, -2.4523727893829346, -1.1920928244535389e-07, -0.047536615282297134, -0.00046206763363443315, -0.04410821571946144, -0.5494900941848755, -0.7993549108505249, -1.6010774374008179, -1.9550132492440753e-05, -0.0010634964564815164, -0.002944184234365821, -0.3065304160118103, -0.1683027297258377, -0.03680377081036568], "finish_reason": "stop"}, {"text": " [word for word in tk.tokenize(text) if word.startswith('$')]", "tokens": [" [", "word", " for", " word", " in", " t", "k", ".", "token", "ize", "(", "text", ")", " if", " word", ".", "start", "sw", "ith", "('", "$", "')", "]", "\n"], "logprobs": [-1.6863423585891724, -0.6298859119415283, -0.1156029999256134, -0.0009926398051902652, -0.00046623803791590035, -0.00983393844217062, -6.174850568640977e-05, -4.7205765440594405e-05, -0.00015078838623594493, -6.318072337307967e-06, -0.0007234578370116651, -0.005391224753111601, -0.05544252693653107, -0.059829503297805786, -0.19006852805614471, -0.3147452175617218, -0.03943740576505661, -2.6464111215318553e-05, -3.576278118089249e-07, -1.4392306804656982, -0.0010636154329404235, -0.00010430268594063818, -0.18418459594249725, -0.017104139551520348], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-2.619614601135254, -0.059438109397888184], "finish_reason": "stop"}, {"text": " dict()", "tokens": [" dict", "()", "\n"], "logprobs": [-2.39617919921875, -1.1790728569030762, -0.02028493955731392], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.008600102737545967, -2.062299427052494e-05, -0.0023526865988969803, -0.7945613861083984, -0.1024225577712059, -0.004887182265520096, -0.014415407553315163, -0.2484009563922882, -3.576278118089249e-07, -0.0012815127847716212, -0.018024107441306114, -0.004146550316363573, -0.001121845212765038, -0.06678474694490433, -0.0028255560901015997, -0.0007739647408016026, -0.0012724645202979445, -0.04169395938515663, -2.0265558760002023e-06, -4.815939246327616e-05, -0.0008648469229228795, -0.008454129099845886, -0.0055577997118234634, -0.025266606360673904], "finish_reason": "stop"}, {"text": " df<|endoftext|># Task 4: Check for palindrome", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "#", " Task", " ", "4", ":", " Check", " for", " pal", "ind", "rome", "\n"], "logprobs": [-0.005128205753862858, -1.1767022609710693, -6.437280717364047e-06, -9.179073458653875e-06, -2.9802276912960224e-06, -2.3841830625315197e-06, -0.1062663123011589, -1.3473190069198608, -4.000907897949219, -0.6155901551246643, -2.6403985023498535, -1.8461840152740479, -4.931289196014404, -2.8034958839416504, -2.4967257976531982, -0.007767236325889826, -0.20300635695457458, -0.42404019832611084], "finish_reason": "stop"}], "metrics": {"energy": 541.3375621223449, "energy_per_token": 4.58760645866394, "time": 15.1428585, "time_per_token": 0.1283293093220339}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError('Text must be a string')", "tokens": [" Value", "Error", "('", "Text", " must", " be", " a", " string", "')", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -1.936584234237671, -2.655731678009033, -0.9094654321670532, -0.00520920567214489, -0.24041910469532013, -0.019523246213793755, -0.287515252828598, -0.005970263388007879], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.08111252635717392, -2.7417760065873154e-05, -4.172316494077677e-06, -0.0017880895175039768, -0.8120558857917786, -0.005209680180996656, -0.0003551805857568979, -0.053832314908504486, -0.015612166374921799, -0.007603158708661795, -0.029011188074946404, -0.02667182870209217], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.20826326310634613, -7.295342220459133e-05, -2.4437606043647975e-05, -0.00011085849109804258, -5.8412379075889476e-06, -0.0004700509598478675, -0.0018980359891429543, -0.010654824785888195, -0.02220284566283226], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.6308958530426025, -0.033669400960206985], "finish_reason": "stop"}, {"text": " Counter(dollar_words)", "tokens": [" Counter", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-2.248095989227295, -0.021917037665843964, -0.002881662454456091, -7.629365427419543e-06, -3.373566141817719e-05, -0.00019929806876461953, -0.08330021053552628], "finish_reason": "stop"}, {"text": " pd.DataFrame({\"Word\": freq.keys(), \"Frequency\": freq.values()})", "tokens": [" pd", ".", "DataFrame", "(", "{\"", "Word", "\":", " fre", "q", ".", "keys", "()", ",", " \"", "F", "requency", "\":", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.007491826545447111, -2.169585604860913e-05, -0.0032252457458525896, -0.9963356852531433, -1.4788689613342285, -0.0022809698712080717, -0.032312650233507156, -0.30536535382270813, -2.3841855067985307e-07, -0.0016452836571261287, -0.012890700250864029, -0.003761004889383912, -0.001177332829684019, -0.08731044828891754, -0.0027796700596809387, -0.0006258912617340684, -0.0011801904765889049, -0.052582740783691406, -1.4305104514278355e-06, -5.340433563105762e-05, -0.0017805927200242877, -0.010141155682504177, -0.0074018994346261024, -0.024444719776511192], "finish_reason": "stop"}, {"text": " df<|endoftext|>from typing import List", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "from", " typing", " import", " List", "\n"], "logprobs": [-0.005390513222664595, -1.1487207412719727, -7.033323527139146e-06, -5.006777428206988e-06, -1.7881377516459906e-06, -1.311301275563892e-06, -0.11144702881574631, -1.596664547920227, -1.5603535175323486, -0.001369491801597178, -0.6493155360221863, -0.1612395942211151], "finish_reason": "stop"}], "metrics": {"energy": 438.0679222793581, "energy_per_token": 5.215094312849501, "time": 12.202316, "time_per_token": 0.14526566666666665}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"input text is not a string\")", "tokens": [" Value", "Error", "(", "\"", "input", " text", " is", " not", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -2.8167901039123535, -1.1931401491165161, -1.3883085250854492, -0.018580200150609016, -0.08784013241529465, -0.02535947412252426, -0.094597727060318, -0.0009683449170552194, -0.007583638187497854], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -0.05714777112007141, -0.015522966161370277, -0.010418668389320374, -0.03339134901762009, -0.02971547096967697], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " [w for w in words if w.startswith('$') and not w.strip(punctuation)]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", ".", "start", "sw", "ith", "('", "$", "')", " and", " not", " w", ".", "strip", "(", "punct", "uation", ")]", "\n"], "logprobs": [-0.5513269305229187, -1.3396928310394287, -0.09144708514213562, -0.0020459212828427553, -0.00035958975786343217, -0.004531471990048885, -0.012178703211247921, -0.10250820964574814, -0.15072935819625854, -0.02767917327582836, -4.23184028477408e-05, -2.3841855067985307e-07, -0.8643434643745422, -0.002281088614836335, -0.00017653337272349745, -0.5943892598152161, -0.463942289352417, -1.2676081657409668, -0.8187738060951233, -0.4221622943878174, -0.01885058544576168, -0.016947448253631592, -1.4066597032069694e-05, -0.153476744890213, -0.051685433834791183], "finish_reason": "stop"}, {"text": " dict.fromkeys(set(dollar_words), 0)", "tokens": [" dict", ".", "from", "keys", "(", "set", "(", "dollar", "_", "words", ")", ",", " ", "0", ")", "\n"], "logprobs": [-2.4027349948883057, -1.9037973880767822, -3.981510963058099e-05, -1.07287787614041e-05, -0.0023199806455522776, -2.9949681758880615, -0.004868913441896439, -0.0016338583081960678, -6.198863957251888e-06, -1.7881377516459906e-06, -0.15457908809185028, -0.0017416798509657383, -0.08319984376430511, -0.005710715427994728, -0.0019219514215365052, -0.0602298341691494], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq, index=[\"Frequency\"]).T.reset_index().rename(columns={'index':'Word'})", "tokens": [" pd", ".", "DataFrame", "(", "freq", ",", " index", "=[", "\"", "F", "requency", "\"", "])", ".", "T", ".", "reset", "_", "index", "()", ".", "rename", "(", "columns", "={", "'", "index", "':", "'", "Word", "'})", "\n"], "logprobs": [-0.006779288873076439, -2.455681169521995e-05, -0.0035272545646876097, -0.8532527685165405, -0.8770509958267212, -1.3496203422546387, -0.03560255467891693, -0.5570476055145264, -1.4146257638931274, -0.2694576382637024, -0.004031626507639885, -0.0006541258189827204, -0.008935228921473026, -0.6019343137741089, -0.3243033289909363, -0.4026212692260742, -0.14301848411560059, -1.311301275563892e-06, -4.172316494077677e-06, -0.011835001409053802, -0.6858818531036377, -0.032709263265132904, -0.008594902232289314, -0.017126288264989853, -1.7676875591278076, -0.045982833951711655, -0.0011805477552115917, -0.004437955562025309, -1.125248908996582, -0.001518169417977333, -0.0361224003136158, -0.07968065142631531], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.005331108812242746, -0.9634293913841248], "finish_reason": "stop"}], "metrics": {"energy": 545.2707034358976, "energy_per_token": 4.66043336269998, "time": 14.9415865, "time_per_token": 0.1277058675213675}}
{"outputs": [{"text": " type(text) is not str:", "tokens": [" type", "(", "text", ")", " is", " not", " str", ":", "\n"], "logprobs": [-2.0513792037963867, -0.0005845506675541401, -0.0005514293443411589, -0.00015162272029556334, -1.7009313106536865, -0.03422907367348671, -0.002559007378295064, -0.008963702246546745, -0.04648934677243233], "finish_reason": "stop"}, {"text": " ValueError(\"Expected a string\")", "tokens": [" Value", "Error", "(", "\"", "Expected", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -4.050446033477783, -0.7627609968185425, -0.03398136794567108, -1.1975547075271606, -0.0006790239713154733, -0.006001428235322237], "finish_reason": "stop"}, {"text": " nltk.tokenize.TweetTokenizer()", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "T", "weet", "Token", "izer", "()", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -1.0804400444030762, -0.0008142255246639252, -0.012112634256482124, -2.08487606048584, -0.0019675197545439005, -8.153582894010469e-05, -5.483612312673358e-06, -0.6174272894859314, -0.08011052757501602], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " [word[1:] for word in words if word[0] == '$']", "tokens": [" [", "word", "[", "1", ":]", " for", " word", " in", " words", " if", " word", "[", "0", "]", " ==", " '$", "']", "\n"], "logprobs": [-0.5513269305229187, -0.462419331073761, -2.7683157920837402, -0.03134866803884506, -0.012384772300720215, -0.10324571281671524, -0.000553335587028414, -0.00013207517622504383, -0.014208228327333927, -0.019698821008205414, -0.15655305981636047, -1.2398080825805664, -0.011032769456505775, -0.03400406613945961, -0.11161177605390549, -2.6070377826690674, -0.016190361231565475, -0.13322222232818604], "finish_reason": "stop"}, {"text": " Counter(dollar_words)", "tokens": [" Counter", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-2.2493624687194824, -0.018288088962435722, -0.003972616046667099, -9.536697689327411e-06, -4.088794958079234e-05, -0.00015817821258679032, -0.09531540423631668], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.most_common(), columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "most", "_", "common", "()", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.006731689441949129, -2.169585604860913e-05, -0.0016648017335683107, -1.0698230266571045, -0.7650474309921265, -0.39459845423698425, -0.14922697842121124, -4.768370445162873e-07, -2.038458114839159e-05, -0.08036202937364578, -0.0008571050129830837, -0.027088092640042305, -0.8869587779045105, -0.009396139532327652, -0.0033320642542093992, -8.201262971851975e-05, -0.00045563330058939755, -0.04906664788722992, -0.0016502822982147336, -0.0009859706042334437, -0.0003156163729727268, -0.00623021787032485, -0.020620843395590782], "finish_reason": "stop"}, {"text": " df<|endoftext|>def task_func(string):", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "def", " task", "_", "func", "(", "string", "):", "\n"], "logprobs": [-0.0037768001202493906, -1.084592580795288, -7.867782187531702e-06, -8.702239938429557e-06, -1.5497195136049413e-06, -3.814689989667386e-06, -0.11006640642881393, -2.111477851867676, -0.615204930305481, -0.08159288018941879, -0.25737565755844116, -0.06748711317777634, -3.329556465148926, -0.4503228962421417, -0.006512963213026524], "finish_reason": "stop"}], "metrics": {"energy": 487.92410733985855, "energy_per_token": 4.691577955190947, "time": 13.683918, "time_per_token": 0.13157613461538462}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Input text should be a string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " text", " should", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -0.9037979245185852, -2.5637950897216797, -0.0008593680104240775, -0.06729443371295929, -0.017626777291297913, -0.44419339299201965, -0.00033468366018496454, -0.0030768217984586954], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -0.05714777112007141, -0.015522966161370277, -0.010418668389320374, -0.03339134901762009, -0.02971547096967697], "finish_reason": "stop"}, {"text": " [w for w in tk.tokenize(text) if w.startswith(\"$\")]", "tokens": [" [", "w", " for", " w", " in", " t", "k", ".", "token", "ize", "(", "text", ")", " if", " w", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-1.8076647520065308, -1.1133511066436768, -0.07861308008432388, -0.0008928364841267467, -0.000516757951118052, -0.009670081548392773, -0.00018773700867313892, -2.753696753643453e-05, -0.00010978573118336499, -1.0490362910786644e-05, -0.0005710878176614642, -0.002711788285523653, -0.05365709587931633, -0.038912173360586166, -0.1772029846906662, -0.26938843727111816, -0.0470123216509819, -2.2649508537142538e-05, -2.3841855067985307e-07, -0.29851290583610535, -0.004397128242999315, -0.00018904806347563863, -6.294052582234144e-05, -0.3228815197944641, -0.024491017684340477], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith(\"$\")]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-0.5442787408828735, -0.5437167286872864, -0.11734000593423843, -0.0013097288319841027, -0.00030214988510124385, -0.005751252174377441, -0.017451424151659012, -0.10041782259941101, -0.15094833076000214, -0.019022652879357338, -3.683499380713329e-05, -3.576278118089249e-07, -0.4616698920726776, -0.0026328444946557283, -0.00013267113536130637, -3.886147169396281e-05, -0.5185056924819946, -0.09588202834129333], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-1.5339840650558472, -0.01722448505461216], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.008133149705827236, -2.3841574147809297e-05, -0.002484927186742425, -0.731419026851654, -0.11834109574556351, -0.0043173679150640965, -0.017921658232808113, -0.2837222218513489, -3.576278118089249e-07, -0.0014157997211441398, -0.015349121764302254, -0.004687748849391937, -0.0009333306807093322, -0.07456619292497635, -0.002684087259694934, -0.0006637753685936332, -0.0019039851613342762, -0.03932841867208481, -1.5497195136049413e-06, -5.972207145532593e-05, -0.0007884969236329198, -0.009403107687830925, -0.004811613354831934, -0.02533099800348282], "finish_reason": "stop"}, {"text": " df<|endoftext|>from typing import Any, List", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "from", " typing", " import", " Any", ",", " List", "\n"], "logprobs": [-0.005604862235486507, -1.1114275455474854, -7.390948667307384e-06, -8.4638240878121e-06, -2.264974000354414e-06, -2.861018856492592e-06, -0.1100323423743248, -1.6304985284805298, -1.5865148305892944, -0.001291751628741622, -2.0523951053619385, -0.6230438947677612, -1.5274691581726074, -0.6825759410858154], "finish_reason": "stop"}], "metrics": {"energy": 526.553780277252, "energy_per_token": 4.539256726528035, "time": 13.4581, "time_per_token": 0.11601810344827586}}
{"outputs": [{"text": " type(text) != str:", "tokens": [" type", "(", "text", ")", " ", "!", "=", " str", ":", "\n"], "logprobs": [-2.0513792037963867, -0.0005845506675541401, -0.0005514293443411589, -0.00015162272029556334, -0.2486417591571808, -1.1920922133867862e-06, -0.0001436368766007945, -0.03366236761212349, -0.009530758485198021, -0.04765096306800842], "finish_reason": "stop"}, {"text": " ValueError('text must be a string')", "tokens": [" Value", "Error", "('", "text", " must", " be", " a", " string", "')", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -1.936584234237671, -0.6128314733505249, -0.5977275371551514, -0.003903271397575736, -0.17063313722610474, -0.026956049725413322, -0.06626208126544952, -0.006395113188773394], "finish_reason": "stop"}, {"text": " nltk.tokenize.casual.TweetTokenizer()", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "cas", "ual", ".", "T", "weet", "Token", "izer", "()", "\n"], "logprobs": [-0.08111252635717392, -2.7417760065873154e-05, -4.172316494077677e-06, -0.0017880895175039768, -0.9599440693855286, -0.0005996812833473086, -0.013653579168021679, -3.2292776107788086, -0.048927370458841324, -0.004695461131632328, -0.014007940888404846, -0.0006650857976637781, -0.00011300401820335537, -6.556489552167477e-06, -0.7264838814735413, -0.09164441376924515], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.20826326310634613, -7.295342220459133e-05, -2.4437606043647975e-05, -0.00011085849109804258, -5.8412379075889476e-06, -0.0004700509598478675, -0.0018980359891429543, -0.010654824785888195, -0.02220284566283226], "finish_reason": "stop"}, {"text": " [x for x in words if x.startswith('$') and not x in punctuation]", "tokens": [" [", "x", " for", " x", " in", " words", " if", " x", ".", "start", "sw", "ith", "('", "$", "')", " and", " not", " x", " in", " punct", "uation", "]", "\n"], "logprobs": [-0.5221788287162781, -3.3620498180389404, -0.04610476270318031, -0.00201261043548584, -0.0003831844369415194, -0.013600778765976429, -0.005974411033093929, -0.11949954926967621, -0.25696268677711487, -0.03167695179581642, -5.590759246842936e-05, -1.1920928244535389e-07, -0.9773951172828674, -0.0023486430291086435, -0.000125281119835563, -0.7838335037231445, -0.5886110663414001, -1.6521337032318115, -2.991476058959961, -0.06958446651697159, -1.728519782773219e-05, -0.06717093288898468, -0.08867079019546509], "finish_reason": "stop"}, {"text": " dict((x,dollar_words.count(x)) for x in set(dollar_words))", "tokens": [" dict", "((", "x", ",", "dollar", "_", "words", ".", "count", "(", "x", "))", " for", " x", " in", " set", "(", "dollar", "_", "words", "))", "\n"], "logprobs": [-2.4041545391082764, -2.096933364868164, -0.8647068738937378, -0.0005924138822592795, -0.6866761445999146, -4.0531076592742465e-06, -1.3947389561508317e-05, -0.00014280252798926085, -1.4305012882687151e-05, -0.00011288482346571982, -0.0001225396408699453, -0.0010750473011285067, -0.002291911980137229, -0.0004049911512993276, -2.5033637939486653e-05, -0.04816049709916115, -0.0010093123419210315, -0.00022706791060045362, -4.768370445162873e-07, -1.0728830375228426e-06, -0.01659858226776123, -0.03419935330748558], "finish_reason": "stop"}, {"text": " pd.DataFrame([(word, freq[word]) for word in freq], columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "([", "(", "word", ",", " fre", "q", "[", "word", "])", " for", " word", " in", " fre", "q", "]", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.007679700385779142, -2.098061486321967e-05, -0.0023178397677838802, -2.73348331451416, -0.8322334885597229, -0.8100242614746094, -0.03085974045097828, -0.14328740537166595, -0.00020954797219019383, -0.010118498466908932, -0.00021526881027966738, -0.0017523899441584945, -0.00756399892270565, -0.0006401873542927206, -0.0024131007958203554, -0.2936584949493408, -1.5497195136049413e-06, -0.5004401803016663, -0.003114613238722086, -0.1404992938041687, -0.9919800162315369, -0.0038359409663826227, -0.004720970522612333, -4.875540980719961e-05, -0.0005708495154976845, -0.07350865751504898, -0.001954313600435853, -0.0005162813467904925, -0.00015269544383045286, -0.010416663251817226, -0.03084101527929306], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.004489936400204897], "finish_reason": "stop"}], "metrics": {"energy": 591.609526166916, "energy_per_token": 4.84925841120423, "time": 15.364449, "time_per_token": 0.12593810655737706}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Text must be a string!\")", "tokens": [" Value", "Error", "(", "\"", "Text", " must", " be", " a", " string", "!", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -3.0139660835266113, -0.5480970144271851, -0.0029996896628290415, -0.09284853935241699, -0.020815256983041763, -3.108952283859253, -0.01508859172463417, -0.0009896624833345413, -0.00562252476811409], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -0.05714777112007141, -0.015522966161370277, -0.010418668389320374, -0.03339134901762009, -0.02971547096967697], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " [word for word in words if word[0] == \"$\"]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", "[", "0", "]", " ==", " \"", "$", "\"", "]", "\n"], "logprobs": [-0.5513269305229187, -0.462419331073761, -0.11415238678455353, -0.001311990898102522, -0.00030214988510124385, -0.008055459707975388, -0.017151247709989548, -0.1245269849896431, -1.9554033279418945, -0.00051771110156551, -0.02037651836872101, -0.10927973687648773, -0.31083545088768005, -0.00019965562387369573, -2.2172682292875834e-05, -0.30837777256965637, -0.0815005823969841], "finish_reason": "stop"}, {"text": " {word: dollar_words.count(word) for word in set(dollar_words)}", "tokens": [" {", "word", ":", " dollar", "_", "words", ".", "count", "(", "word", ")", " for", " word", " in", " set", "(", "dollar", "_", "words", ")}", "\n"], "logprobs": [-2.280243158340454, -0.3578636348247528, -0.04734618961811066, -0.5766230821609497, -3.0278701160568744e-05, -0.00014482879487331957, -0.00010096516780322418, -2.706014311115723e-05, -0.0001784403866622597, -0.0001627074379939586, -0.00044860312482342124, -0.006047285161912441, -0.0002795067266561091, -0.00021634146105498075, -0.929892361164093, -0.0010946955299004912, -7.748303323751315e-05, -2.3841855067985307e-07, -1.311301275563892e-06, -0.019186154007911682, -0.03886527940630913], "finish_reason": "stop"}, {"text": " pd.DataFrame([(word, freq[word]) for word in set(dollar_words)], columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "([", "(", "word", ",", " fre", "q", "[", "word", "])", " for", " word", " in", " set", "(", "dollar", "_", "words", ")]", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0074592879973351955, -2.3483953555114567e-05, -0.0026702960021793842, -2.8239076137542725, -0.8024260401725769, -0.9172891974449158, -0.028251362964510918, -0.1029091328382492, -0.00016962042718660086, -0.01314639113843441, -0.00019596086349338293, -0.0015163839561864734, -0.006704691797494888, -0.0009371418273076415, -0.0034304363653063774, -1.86696195602417, -0.00041392818093299866, -0.07341982424259186, -7.152555099310121e-07, -3.576272320060525e-06, -0.025007717311382294, -0.01564936898648739, -0.2772984802722931, -1.042762041091919, -0.006209959741681814, -0.005233160685747862, -5.030505417380482e-05, -0.0006893880781717598, -0.06343255937099457, -0.0020113016944378614, -0.0012737740762531757, -0.00013958434283267707, -0.011537495069205761, -0.02414640411734581], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.005505399778485298], "finish_reason": "stop"}], "metrics": {"energy": 513.1598659315109, "energy_per_token": 4.462259703752269, "time": 13.255352, "time_per_token": 0.11526393043478261}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Input must be of type string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " must", " be", " of", " type", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -1.250410795211792, -0.0017625049222260714, -2.876721143722534, -0.10228072851896286, -0.3311133086681366, -0.38792654871940613, -0.0006524579366669059, -0.005981876514852047], "finish_reason": "stop"}, {"text": " nltk.tokenize.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -1.0804400444030762, -0.0008142255246639252, -0.012112634256482124, -0.8611764311790466, -0.017632165923714638, -0.00070296844933182, -0.3328709006309509, -0.03233850374817848, -0.011209606193006039, -0.11398742347955704, -0.025164779275655746], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-2.5120813846588135, -0.058837298303842545], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts()", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-1.8900330066680908, -7.211902266135439e-05, -0.8280318379402161, -0.002183792879804969, -0.13216082751750946, -8.344646857949556e-07, -5.006777428206988e-06, -0.008126763626933098, -0.017164958640933037, -0.0023695745039731264, -1.311301275563892e-06, -6.09140915912576e-05, -0.018262922763824463, -0.29810553789138794], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.most_common(), columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "most", "_", "common", "()", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.009083377197384834, -2.407998726994265e-05, -0.0030852595809847116, -1.201690912246704, -0.7441585659980774, -0.24687673151493073, -0.09929219633340836, -4.768370445162873e-07, -3.671578815556131e-05, -0.08805771172046661, -0.0009581027552485466, -0.020058291032910347, -0.8666847348213196, -0.007770075462758541, -0.00394079415127635, -6.890059739816934e-05, -0.0004850641416851431, -0.04509051889181137, -0.0016840812750160694, -0.000800408364739269, -0.00028618055512197316, -0.006145983003079891, -0.020433517172932625], "finish_reason": "stop"}, {"text": " df<|endoftext|>\"\"\"", "tokens": [" df", "<", "|", "end", "of", "text", "|", ">\"", "\"", "\"", "\n"], "logprobs": [-0.004608367569744587, -1.186877965927124, -7.033323527139146e-06, -8.34461570775602e-06, -1.9073468138230965e-06, -2.145764938177308e-06, -2.2560863494873047, -0.001764765940606594, -0.003422595327720046, -0.00030894274823367596, -0.3141699433326721], "finish_reason": "stop"}], "metrics": {"energy": 465.52602036666883, "energy_per_token": 4.900273898596514, "time": 12.829779, "time_per_token": 0.1350503052631579}}
{"outputs": [{"text": " type(text) != str:", "tokens": [" type", "(", "text", ")", " ", "!", "=", " str", ":", "\n"], "logprobs": [-2.0513792037963867, -0.0005845506675541401, -0.0005514293443411589, -0.00015162272029556334, -0.2486417591571808, -1.1920922133867862e-06, -0.0001436368766007945, -0.03366236761212349, -0.009530758485198021, -0.04765096306800842], "finish_reason": "stop"}, {"text": " ValueError('input must be a string')", "tokens": [" Value", "Error", "('", "input", " must", " be", " a", " string", "')", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -1.936584234237671, -3.2590088844299316, -1.1660445928573608, -0.004869387950748205, -0.39760053157806396, -0.034614335745573044, -0.04938008636236191, -0.004510111175477505], "finish_reason": "stop"}, {"text": " nltk.tokenize.word_tokenizer.WordPunctTokenizer()", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "_", "token", "izer", ".", "Word", "Punct", "Token", "izer", "()", "\n"], "logprobs": [-0.08111252635717392, -2.7417760065873154e-05, -4.172316494077677e-06, -0.0017880895175039768, -0.9599440693855286, -0.0005996812833473086, -0.013653579168021679, -0.8363466858863831, -0.023535648360848427, -0.0007727735792286694, -1.14703369140625, -0.27337244153022766, -0.04284517094492912, -0.4722234606742859, -8.702239938429557e-06, -2.2172682292875834e-05, -0.031267907470464706, -0.2959156632423401], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.20826326310634613, -7.295342220459133e-05, -2.4437606043647975e-05, -0.00011085849109804258, -5.8412379075889476e-06, -0.0004700509598478675, -0.0018980359891429543, -0.010654824785888195, -0.02220284566283226], "finish_reason": "stop"}, {"text": " [w for w in words if w.startswith('$')]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", ".", "start", "sw", "ith", "('", "$", "')", "]", "\n"], "logprobs": [-0.5221788287162781, -1.326674222946167, -0.08757368475198746, -0.0018310461891815066, -0.0004481264913920313, -0.004632930271327496, -0.011212906800210476, -0.10684583336114883, -0.16000784933567047, -0.02905275858938694, -4.434487345861271e-05, -2.3841855067985307e-07, -0.8885524868965149, -0.002103260951116681, -0.00021264675888232887, -0.7421693205833435, -0.07483889162540436], "finish_reason": "stop"}, {"text": " Counter(dollar_words)", "tokens": [" Counter", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-2.2468020915985107, -0.027067208662629128, -0.004444008227437735, -7.867782187531702e-06, -2.9205850296420977e-05, -0.0001829695247579366, -0.07066059857606888], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.007679700385779142, -2.098061486321967e-05, -0.0023178397677838802, -0.9001435041427612, -0.11158256232738495, -0.004858473781496286, -0.017469581216573715, -0.25296592712402344, -3.576278118089249e-07, -0.0015783959534019232, -0.016930218786001205, -0.0038484097458422184, -0.0010171722387894988, -0.07309388369321823, -0.002801305614411831, -0.0007600279641337693, -0.0015499495202675462, -0.043911319226026535, -2.0265558760002023e-06, -6.09140915912576e-05, -0.0010439666220918298, -0.008542543277144432, -0.005415886640548706, -0.02756819874048233], "finish_reason": "stop"}, {"text": " df<|endoftext|>#!/usr/bin/env python3", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "#", "!", "/", "usr", "/", "bin", "/", "env", " python", "3", "\n"], "logprobs": [-0.004489936400204897, -1.1334341764450073, -7.152531907195225e-06, -6.198863957251888e-06, -1.7881377516459906e-06, -3.4570634852570947e-06, -0.10232431441545486, -1.1533091068267822, -1.3525313138961792, -0.06833328306674957, -0.05811263248324394, -6.949660019017756e-05, -0.009722735732793808, -0.0002619877050165087, -0.0790567472577095, -0.004380868282169104, -0.5012827515602112, -0.01692752167582512], "finish_reason": "stop"}], "metrics": {"energy": 513.5140463905334, "energy_per_token": 4.544372091951623, "time": 14.524256, "time_per_token": 0.1285332389380531}}
{"outputs": [{"text": " type(text) != str:", "tokens": [" type", "(", "text", ")", " ", "!", "=", " str", ":", "\n"], "logprobs": [-2.0513792037963867, -0.0005845506675541401, -0.0005514293443411589, -0.00015162272029556334, -0.2486417591571808, -1.1920922133867862e-06, -0.0001436368766007945, -0.03366236761212349, -0.009530758485198021, -0.04765096306800842], "finish_reason": "stop"}, {"text": " ValueError('text must be a string')", "tokens": [" Value", "Error", "('", "text", " must", " be", " a", " string", "')", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -1.936584234237671, -0.6128314733505249, -0.5977275371551514, -0.003903271397575736, -0.17063313722610474, -0.026956049725413322, -0.06626208126544952, -0.006395113188773394], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.08111252635717392, -2.7417760065873154e-05, -4.172316494077677e-06, -0.0017880895175039768, -0.8120558857917786, -0.005209680180996656, -0.0003551805857568979, -0.053832314908504486, -0.015612166374921799, -0.007603158708661795, -0.029011188074946404, -0.02667182870209217], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.20826326310634613, -7.295342220459133e-05, -2.4437606043647975e-05, -0.00011085849109804258, -5.8412379075889476e-06, -0.0004700509598478675, -0.0018980359891429543, -0.010654824785888195, -0.02220284566283226], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith(\"$\") and not all(c in punctuation for c in word[1:])]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")", " and", " not", " all", "(", "c", " in", " punct", "uation", " for", " c", " in", " word", "[", "1", ":]", ")]", "\n"], "logprobs": [-0.5221788287162781, -0.463972806930542, -0.09291089326143265, -0.0018899451242759824, -0.0003150205302517861, -0.00682664941996336, -0.017010385170578957, -0.12289794534444809, -0.1903664767742157, -0.023620760068297386, -2.7894584491150454e-05, -1.1920928244535389e-07, -0.4595509469509125, -0.003045446705073118, -0.00015352977789007127, -3.611976353568025e-05, -0.7196840643882751, -0.07156047224998474, -0.4312104880809784, -0.7036941051483154, -0.19826355576515198, -0.8123956918716431, -0.0012379371328279376, -0.007780602667480707, -1.156323378381785e-05, -0.0023229538928717375, -0.00011145447206217796, -0.00010072677832795307, -0.0019000587053596973, -1.6738466024398804, -0.001510432455688715, -0.02603427693247795, -0.0030307096894830465, -0.08100281655788422], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-1.6000007390975952, -0.015528600662946701], "finish_reason": "stop"}, {"text": " pd.DataFrame.from_dict(dict(freq), orient='index', columns=[\"Frequency\"])", "tokens": [" pd", ".", "DataFrame", ".", "from", "_", "dict", "(", "dict", "(", "freq", ")", ",", " orient", "='", "index", "'", ",", " columns", "=[", "\"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.006531439255923033, -2.407998726994265e-05, -0.0026500842068344355, -2.02616024017334, -1.7762025890988298e-05, -1.0847986231965479e-05, -0.019418494775891304, -0.001856866991147399, -3.0768048763275146, -0.00825741607695818, -0.01715359091758728, -0.023037992417812347, -0.005397390574216843, -0.08127706497907639, -0.2682812809944153, -7.128461584215984e-05, -0.047996096312999725, -0.0003237200144212693, -0.01876997947692871, -1.677079200744629, -0.016995618119835854, -0.015343368984758854, -0.0005814530304633081, -5.8530047681415454e-05, -0.016138633713126183, -0.29449471831321716], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.006264218129217625], "finish_reason": "stop"}], "metrics": {"energy": 492.3148554306031, "energy_per_token": 4.733796686832722, "time": 12.408985, "time_per_token": 0.11931716346153845}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Input is not a string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " is", " not", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -2.5877254009246826, -0.022177893668413162, -0.08744437247514725, -0.02808377332985401, -0.3688620328903198, -0.0010153858456760645, -0.01171718817204237], "finish_reason": "stop"}, {"text": " nltk.tokenize.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -1.0804400444030762, -0.0008142255246639252, -0.012112634256482124, -0.8611764311790466, -0.017632165923714638, -0.00070296844933182, -0.3328709006309509, -0.03233850374817848, -0.011209606193006039, -0.11398742347955704, -0.025164779275655746], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.5703012943267822, -0.03230549395084381], "finish_reason": "stop"}, {"text": " dict()", "tokens": [" dict", "()", "\n"], "logprobs": [-2.4040939807891846, -1.3737881183624268, -0.022353585809469223], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.009083377197384834, -2.407998726994265e-05, -0.0030852595809847116, -0.5836488604545593, -0.1225963905453682, -0.004966067150235176, -0.019815698266029358, -0.3109079599380493, -3.576278118089249e-07, -0.0015804193681105971, -0.02051200345158577, -0.0044111330062150955, -0.0010980297811329365, -0.07556557655334473, -0.0020385454408824444, -0.0007393010891973972, -0.0012750837486237288, -0.040441252291202545, -2.0265558760002023e-06, -6.8662193370983e-05, -0.0008126770262606442, -0.00887201726436615, -0.005452760029584169, -0.029706791043281555], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.0052426476031541824, -0.9330222606658936], "finish_reason": "stop"}], "metrics": {"energy": 423.28412901496887, "energy_per_token": 5.643788386866252, "time": 11.159798, "time_per_token": 0.14879730666666668}}
{"outputs": [{"text": " type(text) != str:", "tokens": [" type", "(", "text", ")", " ", "!", "=", " str", ":", "\n"], "logprobs": [-2.0513792037963867, -0.0005845506675541401, -0.0005514293443411589, -0.00015162272029556334, -0.2486417591571808, -1.1920922133867862e-06, -0.0001436368766007945, -0.03366236761212349, -0.009530758485198021, -0.04765096306800842], "finish_reason": "stop"}, {"text": " ValueError(\"'text' is not a string\")", "tokens": [" Value", "Error", "(", "\"", "'", "text", "'", " is", " not", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -3.6431221961975098, -0.020520644262433052, -0.002062933286651969, -3.1016242504119873, -0.1150781512260437, -0.19098632037639618, -0.03973956033587456, -0.35486462712287903, -0.0011316093150526285, -0.011300605721771717], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -0.05714777112007141, -0.015522966161370277, -0.010418668389320374, -0.03339134901762009, -0.02971547096967697], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.5703012943267822, -0.03230549395084381], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-1.6890995502471924, -0.013232861645519733], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': list(freq.keys()), 'Frequency': list(freq.values())})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " list", "(", "freq", ".", "keys", "())", ",", " '", "F", "requency", "':", " list", "(", "freq", ".", "values", "())", "})", "\n"], "logprobs": [-0.009083377197384834, -2.407998726994265e-05, -0.0030852595809847116, -0.5836488604545593, -0.1225963905453682, -0.004966067150235176, -0.019815698266029358, -2.0620522499084473, -0.0005352256703190506, -0.07378728687763214, -0.037050433456897736, -0.00603282917290926, -0.0009382136631757021, -0.003932363353669643, -0.1005464419722557, -0.0028372055385261774, -0.0014184186002239585, -0.0010193157941102982, -0.05489291250705719, -0.00030429501202888787, -0.003305212128907442, -1.2397689715726301e-05, -0.0006087357178330421, -0.017245693132281303, -0.005943956319242716, -0.020206786692142487], "finish_reason": "stop"}, {"text": " df<|endoftext|>import pandas as pd", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "import", " pandas", " as", " pd", "\n"], "logprobs": [-0.0052426476031541824, -1.099480390548706, -7.271740287251305e-06, -5.364403477869928e-06, -2.145764938177308e-06, -2.145764938177308e-06, -0.10641089826822281, -1.0264707803726196, -3.2304868698120117, -0.018175246194005013, -0.003232494229450822, -0.013396724127233028], "finish_reason": "stop"}], "metrics": {"energy": 432.09381112289435, "energy_per_token": 4.9665955301482105, "time": 12.2027445, "time_per_token": 0.14026143103448274}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"The text parameter must be a string.\")", "tokens": [" Value", "Error", "(", "\"", "The", " text", " parameter", " must", " be", " a", " string", ".", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -3.026604175567627, -1.597090721130371, -1.296654224395752, -0.24906755983829498, -0.0049719978123903275, -0.09047053009271622, -0.012652600184082985, -0.5632611513137817, -0.010525311343371868, -0.0008981961291283369, -0.00265162973664701], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.0847516655921936, -2.253030106658116e-05, -4.0531076592742465e-06, -0.001347110839560628, -0.942962646484375, -0.004770089406520128, -0.0002494739310350269, -0.04056317359209061, -0.013402487151324749, -0.008528477512300014, -0.03373636677861214, -0.02737509272992611], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.24513067305088043, -6.949660019017756e-05, -2.6225699912174605e-05, -0.00015901254664640874, -4.6491513785440475e-06, -0.0004450284468475729, -0.0022218560334295034, -0.011837593279778957, -0.02034836821258068], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.7002277374267578, -0.03147527948021889], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-1.7065001726150513, -0.01690279133617878], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq)", "tokens": [" pd", ".", "DataFrame", "(", "freq", ")", "\n"], "logprobs": [-0.007575001567602158, -2.5033637939486653e-05, -0.0022845377679914236, -0.8501085042953491, -0.8516743779182434, -2.954416275024414, -0.2026035189628601], "finish_reason": "stop"}, {"text": " df<|endoftext|>import numpy as np", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "import", " numpy", " as", " np", "\n"], "logprobs": [-0.005013394635170698, -1.1497361660003662, -6.9141146923357155e-06, -7.629365427419543e-06, -1.6689286894688848e-06, -2.3841830625315197e-06, -0.10473036766052246, -1.023703694343567, -2.6258578300476074, -0.017388982698321342, -0.0025460466276854277, -0.006770882289856672], "finish_reason": "stop"}], "metrics": {"energy": 370.243826505661, "energy_per_token": 5.526027261278522, "time": 10.521336, "time_per_token": 0.1570348656716418}}
{"outputs": [{"text": " isinstance(text, str) != True:", "tokens": [" isinstance", "(", "text", ",", " str", ")", " ", "!", "=", " True", ":", "\n"], "logprobs": [-3.012392997741699, -0.0004894727608188987, -0.0009726322605274618, -0.0015968440566211939, -0.08898624032735825, -0.5931484699249268, -2.500901222229004, -5.960446742392378e-06, -6.198863957251888e-06, -0.04850204661488533, -0.018411099910736084, -0.03594842180609703], "finish_reason": "stop"}, {"text": " ValueError(\"text is not a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " is", " not", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0905691385269165, -2.212496519088745, -0.02016402781009674, -0.07650743424892426, -0.014532316476106644, -0.10852538794279099, -0.0008927173912525177, -0.00995881948620081], "finish_reason": "stop"}, {"text": " nltk.tokenize.TweetTokenizer()", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "T", "weet", "Token", "izer", "()", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -1.0804400444030762, -0.0008142255246639252, -0.012112634256482124, -2.08487606048584, -0.0019675197545439005, -8.153582894010469e-05, -5.483612312673358e-06, -0.6174272894859314, -0.08011052757501602], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " [w for w in words if w.startswith(\"$\")]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-0.5513269305229187, -1.3396928310394287, -0.09144708514213562, -0.0020459212828427553, -0.00035958975786343217, -0.004531471990048885, -0.012178703211247921, -0.10250820964574814, -0.15072935819625854, -0.02767917327582836, -4.23184028477408e-05, -2.3841855067985307e-07, -0.5475481152534485, -0.0029740172903984785, -0.0001658063702052459, -3.4689302992774174e-05, -0.757683277130127, -0.09793553501367569], "finish_reason": "stop"}, {"text": " nltk.FreqDist(dollar_words)", "tokens": [" n", "lt", "k", ".", "F", "req", "Dist", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-1.7362940311431885, -6.41325386823155e-05, -2.396077979938127e-05, -9.035655966727063e-05, -0.05110815167427063, -2.8132995794294402e-05, -3.2782016205601394e-05, -0.0015445933677256107, -0.001910886145196855, -1.0490362910786644e-05, -3.886147169396281e-05, -0.00019786784832831472, -0.0621245801448822], "finish_reason": "stop"}, {"text": " pd.DataFrame({\"Word\": freq.keys(), \"Frequency\": freq.values()})", "tokens": [" pd", ".", "DataFrame", "(", "{\"", "Word", "\":", " fre", "q", ".", "keys", "()", ",", " \"", "F", "requency", "\":", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.007293148431926966, -2.1934269170742482e-05, -0.0025154873728752136, -1.052884578704834, -1.502680778503418, -0.002103974809870124, -0.03611573204398155, -0.30784034729003906, -3.576278118089249e-07, -0.001658851164393127, -0.018602315336465836, -0.0045918733812868595, -0.0010977915953844786, -0.07375141233205795, -0.0036557766143232584, -0.000673424918204546, -0.0013405631761997938, -0.06378794461488724, -1.311301275563892e-06, -5.578839045483619e-05, -0.0017203782917931676, -0.007021395489573479, -0.00787404365837574, -0.025308100506663322], "finish_reason": "stop"}, {"text": " df<|endoftext|>def task_func(n):", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "def", " task", "_", "func", "(", "n", "):", "\n"], "logprobs": [-0.004488987382501364, -1.097942590713501, -8.4638240878121e-06, -9.298280929215252e-06, -1.6689286894688848e-06, -2.622600959512056e-06, -0.11695755273103714, -2.266664743423462, -0.6710272431373596, -0.07707575708627701, -0.23030483722686768, -0.06656460464000702, -2.0257787704467773, -0.43404820561408997, -0.00973796471953392], "finish_reason": "stop"}], "metrics": {"energy": 527.9877494316104, "energy_per_token": 4.551618529582848, "time": 14.30678, "time_per_token": 0.12333431034482759}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Input must be a string!\")", "tokens": [" Value", "Error", "(", "\"", "Input", " must", " be", " a", " string", "!", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -1.250410795211792, -0.0017625049222260714, -0.10377573221921921, -0.01786966808140278, -3.1786367893218994, -0.008413702249526978, -0.0008074360666796565, -0.008265810087323189], "finish_reason": "stop"}, {"text": " nltk.word_tokenizer.WordPunctTokenizer()", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "izer", ".", "Word", "Punct", "Token", "izer", "()", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -2.8928029537200928, -0.973399817943573, -0.14358681440353394, -0.07221002876758575, -1.7523612768854946e-05, -1.3470558769768104e-05, -0.039870958775281906, -0.17854079604148865], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " [w for w in words if w.startswith('$')]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", ".", "start", "sw", "ith", "('", "$", "')", "]", "\n"], "logprobs": [-0.5513269305229187, -1.3396928310394287, -0.09144708514213562, -0.0020459212828427553, -0.00035958975786343217, -0.004531471990048885, -0.012178703211247921, -0.10250820964574814, -0.15072935819625854, -0.02767917327582836, -4.23184028477408e-05, -2.3841855067985307e-07, -0.8643434643745422, -0.002281088614836335, -0.00017653337272349745, -0.8606474995613098, -0.07153217494487762], "finish_reason": "stop"}, {"text": " dict()", "tokens": [" dict", "()", "\n"], "logprobs": [-2.4027349948883057, -1.1827852725982666, -0.021721554920077324], "finish_reason": "stop"}, {"text": " pd.DataFrame(columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.006779288873076439, -2.455681169521995e-05, -0.0035272545646876097, -0.8532527685165405, -1.9959207773208618, -1.480399250984192, -0.003730839118361473, -0.0021830792538821697, -8.999896090244874e-05, -0.0006543640629388392, -0.09370391815900803, -0.0014841266674920917, -0.0007560970261693001, -9.107174992095679e-05, -0.015844399109482765, -0.018781563267111778], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.005331108812242746, -0.9634293913841248], "finish_reason": "stop"}], "metrics": {"energy": 412.9554665889742, "energy_per_token": 4.9753670673370385, "time": 11.770417, "time_per_token": 0.1418122530120482}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError('Text must be a string')", "tokens": [" Value", "Error", "('", "Text", " must", " be", " a", " string", "')", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -1.936584234237671, -2.655731678009033, -0.9094654321670532, -0.00520920567214489, -0.24041910469532013, -0.019523246213793755, -0.287515252828598, -0.005970263388007879], "finish_reason": "stop"}, {"text": " nltk.tokenize.casual.TweetTokenizer(reduce_len=False, strip_handles=False, preserve_case=True)", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "cas", "ual", ".", "T", "weet", "Token", "izer", "(", "reduce", "_", "len", "=", "False", ",", " strip", "_", "hand", "les", "=", "False", ",", " preserve", "_", "case", "=", "True", ")", "\n"], "logprobs": [-0.08111252635717392, -2.7417760065873154e-05, -4.172316494077677e-06, -0.0017880895175039768, -0.9599440693855286, -0.0005996812833473086, -0.013653579168021679, -3.2292776107788086, -0.048927370458841324, -0.004695461131632328, -0.014007940888404846, -0.0006650857976637781, -0.00011300401820335537, -6.556489552167477e-06, -0.6637282967567444, -2.2645413875579834, -1.0609570381348021e-05, -0.0031211492605507374, -0.018813500180840492, -0.44596508145332336, -0.14411063492298126, -1.2919387817382812, 0.0, -0.0005116345710121095, -6.544376083184034e-05, -0.00032658010604791343, -0.01763520948588848, -0.7916810512542725, -0.07614228874444962, -1.0728830375228426e-06, -0.0007129749283194542, -0.005294112488627434, -0.022498594596982002, -0.04004654660820961, -0.030859507620334625], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.20826326310634613, -7.295342220459133e-05, -2.4437606043647975e-05, -0.00011085849109804258, -5.8412379075889476e-06, -0.0004700509598478675, -0.0018980359891429543, -0.010654824785888195, -0.02220284566283226], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith(\"$\") and not set(word).issubset(set(punctuation))]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")", " and", " not", " set", "(", "word", ")", ".", "iss", "ub", "set", "(", "set", "(", "punct", "uation", "))", "]", "\n"], "logprobs": [-0.5221788287162781, -0.463972806930542, -0.09291089326143265, -0.0018899451242759824, -0.0003150205302517861, -0.00682664941996336, -0.017010385170578957, -0.12289794534444809, -0.1903664767742157, -0.023620760068297386, -2.7894584491150454e-05, -1.1920928244535389e-07, -0.4595509469509125, -0.003045446705073118, -0.00015352977789007127, -3.611976353568025e-05, -0.7196840643882751, -0.07156047224998474, -0.4312104880809784, -1.239353895187378, -0.004944715183228254, -0.07586927711963654, -0.46974873542785645, -0.22225461900234222, -0.026294177398085594, 0.0, -1.1920928244535389e-07, -0.004515095613896847, -0.2988259494304657, -0.00285705691203475, -0.003867053659632802, -1.6569954823353328e-05, -0.0020762570202350616, -0.038764480501413345, -0.08834312856197357], "finish_reason": "stop"}, {"text": " pd.Series(dollar_words).value_counts()", "tokens": [" pd", ".", "Series", "(", "dollar", "_", "words", ")", ".", "value", "_", "counts", "()", "\n"], "logprobs": [-1.583666205406189, -6.532455881824717e-05, -0.6277010440826416, -0.002734376350417733, -0.1350402683019638, -8.344646857949556e-07, -5.483612312673358e-06, -0.008164364844560623, -0.01951962150633335, -0.0024812407791614532, -1.0728830375228426e-06, -6.01988795096986e-05, -0.024649323895573616, -0.2508584260940552], "finish_reason": "stop"}, {"text": " pd.DataFrame(freq.most_common(), columns=['Word', 'Frequency'])", "tokens": [" pd", ".", "DataFrame", "(", "freq", ".", "most", "_", "common", "()", ",", " columns", "=['", "Word", "'", ",", " '", "F", "requency", "'])", "\n"], "logprobs": [-0.006531439255923033, -2.407998726994265e-05, -0.0026500842068344355, -0.861598014831543, -0.9130698442459106, -0.4115070104598999, -0.13187699019908905, -3.576278118089249e-07, -3.2066785934148356e-05, -0.08030889183282852, -0.0008679436868987978, -0.022801829501986504, -0.7435422539710999, -0.003011218272149563, -0.00024971229140646756, -0.0007790867821313441, -0.04153924435377121, -0.001622313866391778, -0.000967153930105269, -0.0026349846739321947, -0.02306373603641987], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.006403049454092979], "finish_reason": "stop"}], "metrics": {"energy": 548.6429074525836, "energy_per_token": 4.125134642500629, "time": 14.505829, "time_per_token": 0.10906638345864662}}
{"outputs": [{"text": " type(text) != str:", "tokens": [" type", "(", "text", ")", " ", "!", "=", " str", ":", "\n"], "logprobs": [-2.0513792037963867, -0.0005845506675541401, -0.0005514293443411589, -0.00015162272029556334, -0.2486417591571808, -1.1920922133867862e-06, -0.0001436368766007945, -0.03366236761212349, -0.009530758485198021, -0.04765096306800842], "finish_reason": "stop"}, {"text": " ValueError(\"Input text should be a string.\")", "tokens": [" Value", "Error", "(", "\"", "Input", " text", " should", " be", " a", " string", ".", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -0.9037979245185852, -2.5637950897216797, -0.0008593680104240775, -0.06729443371295929, -0.017626777291297913, -1.1977219581604004, -0.010518823750317097, -0.000865680689457804, -0.0027589849196374416], "finish_reason": "stop"}, {"text": " nltk.tokenize.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -1.0804400444030762, -0.0008142255246639252, -0.012112634256482124, -0.8611764311790466, -0.017632165923714638, -0.00070296844933182, -0.3328709006309509, -0.03233850374817848, -0.011209606193006039, -0.11398742347955704, -0.025164779275655746], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " {}", "tokens": [" {}", "\n"], "logprobs": [-2.5120813846588135, -0.058837298303842545], "finish_reason": "stop"}, {"text": " pd.DataFrame({\"Word\": dollar_words}).groupby(\"Word\").size().reset_index(name=\"Frequency\")", "tokens": [" pd", ".", "DataFrame", "(", "{\"", "Word", "\":", " dollar", "_", "words", "})", ".", "group", "by", "(", "\"", "Word", "\"", ")", ".", "size", "()", ".", "reset", "_", "index", "(", "name", "=", "\"", "F", "requency", "\"", ")", "\n"], "logprobs": [-1.8900330066680908, -7.211902266135439e-05, -1.9762747287750244, -0.3228040337562561, -1.478538155555725, -0.006463219411671162, -0.03282809257507324, -0.28746554255485535, -1.3828182090946939e-05, -2.992108420585282e-05, -0.3075738549232483, -1.7084219455718994, -0.0048474413342773914, -6.318072337307967e-06, -0.354213684797287, -0.020792724564671516, -1.8954096958623268e-05, -7.390948667307384e-06, -0.09125233441591263, -0.008413583971560001, -0.009609867818653584, -0.0032421189825981855, -0.1248113363981247, -0.28974491357803345, -1.1920922133867862e-06, -2.9802276912960224e-06, -0.5055915117263794, -0.007670827675610781, -0.21056081354618073, -0.0029883987735956907, -0.009211541153490543, -0.0007643162389285862, -4.5298504119273275e-05, -0.0022425521165132523, -0.09569203108549118], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.009083377197384834, -2.407998726994265e-05, -0.0030852595809847116, -0.5836488604545593, -0.1225963905453682, -0.004966067150235176, -0.019815698266029358, -0.3109079599380493, -3.576278118089249e-07, -0.0015804193681105971, -0.02051200345158577, -0.0044111330062150955, -0.0010980297811329365, -0.07556557655334473, -0.0020385454408824444, -0.0007393010891973972, -0.0012750837486237288, -0.040441252291202545, -2.0265558760002023e-06, -6.8662193370983e-05, -0.0008126770262606442, -0.00887201726436615, -0.005452760029584169, -0.029706791043281555], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.0052426476031541824, -0.9330222606658936], "finish_reason": "stop"}], "metrics": {"energy": 537.6236892681121, "energy_per_token": 4.843456660073082, "time": 14.312249, "time_per_token": 0.12893918018018016}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "Text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -3.0139660835266113, -0.5480970144271851, -0.0029996896628290415, -0.09284853935241699, -0.020815256983041763, -0.41089707612991333, -0.0005297449533827603, -0.009537134319543839], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -0.05714777112007141, -0.015522966161370277, -0.010418668389320374, -0.03339134901762009, -0.02971547096967697], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " []", "tokens": [" []", "\n"], "logprobs": [-1.5703012943267822, -0.03230549395084381], "finish_reason": "stop"}, {"text": " dict(Counter(dollar_words))", "tokens": [" dict", "(", "Counter", "(", "dollar", "_", "words", "))", "\n"], "logprobs": [-2.4040939807891846, -0.7179158329963684, -0.8411057591438293, -0.005874748341739178, -0.0052655343897640705, -1.1205610462639015e-05, -2.8371408916427754e-05, -0.029777957126498222, -0.03145090490579605], "finish_reason": "stop"}, {"text": " pd.DataFrame(list(freq.items()), columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "list", "(", "freq", ".", "items", "())", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.009083377197384834, -2.407998726994265e-05, -0.0030852595809847116, -1.201690912246704, -3.2788608074188232, -0.008085614070296288, -0.23387666046619415, -0.006362539250403643, -0.11369572579860687, -0.013477057218551636, -0.0018555581336840987, -0.03236700966954231, -0.740595817565918, -0.010527199134230614, -0.005144928116351366, -5.245071224635467e-05, -0.0005184260080568492, -0.06338287889957428, -0.0017037175130099058, -0.0011504229623824358, -0.00016115797916427255, -0.00627594580873847, -0.026275018230080605], "finish_reason": "stop"}, {"text": " df<|endoftext|>from functools import wraps", "tokens": [" df", "<", "|", "end", "of", "text", "|>", "from", " fun", "ct", "ools", " import", " wraps", "\n"], "logprobs": [-0.004863693844527006, -1.1258811950683594, -6.6756979322235566e-06, -9.059865078597795e-06, -2.622600959512056e-06, -2.50339189733495e-06, -0.1013251468539238, -1.6877121925354004, -3.9549479484558105, -0.012181764468550682, -0.0034050126560032368, -0.0007165487040765584, -2.8651223182678223, -0.014250537380576134], "finish_reason": "stop"}], "metrics": {"energy": 416.4421407375335, "energy_per_token": 4.679125176826219, "time": 11.782985, "time_per_token": 0.13239308988764045}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"text must be a string\")", "tokens": [" Value", "Error", "(", "\"", "text", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0905691385269165, -0.3392646312713623, -0.002661854727193713, -0.11764993518590927, -0.019427379593253136, -0.1159302368760109, -0.0005416594794951379, -0.0068497369065880775], "finish_reason": "stop"}, {"text": " nltk.word_tokenize(text)", "tokens": [" n", "lt", "k", ".", "word", "_", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -0.742002010345459, -0.00380850862711668, -0.0002885640424210578, -0.05714777112007141, -0.015522966161370277, -0.010418668389320374, -0.03339134901762009, -0.02971547096967697], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " [w for w in words if w[0] == \"$\" and w not in punctuation]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", "[", "0", "]", " ==", " \"", "$", "\"", " and", " w", " not", " in", " punct", "uation", "]", "\n"], "logprobs": [-0.5513269305229187, -1.3396928310394287, -0.09144708514213562, -0.0020459212828427553, -0.00035958975786343217, -0.004531471990048885, -0.012178703211247921, -0.10250820964574814, -2.0529625415802, -0.0005077026435174048, -0.013868398033082485, -0.12229295074939728, -0.33419451117515564, -0.0002236116270069033, -2.0861407392658293e-05, -1.1858186721801758, -0.9356740117073059, -1.9663416147232056, -0.0006856950931251049, -0.046626459807157516, -0.00014900050882715732, -0.049474939703941345, -0.08656787127256393], "finish_reason": "stop"}, {"text": " pd.value_counts(dollar_words)", "tokens": [" pd", ".", "value", "_", "counts", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-1.8935072422027588, -7.188061863416806e-05, -1.061452865600586, -1.1920928244535389e-07, -4.887569048150908e-06, -0.0010599239030852914, -0.11141866445541382, -3.576278118089249e-07, -3.099436753473128e-06, -0.06833139061927795, -0.12652036547660828], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.006779288873076439, -2.455681169521995e-05, -0.0035272545646876097, -0.9647563695907593, -0.1147318184375763, -0.004517350345849991, -0.01672203466296196, -0.2588038146495819, -3.576278118089249e-07, -0.0014317509485408664, -0.020306900143623352, -0.0037949702236801386, -0.0012051946250721812, -0.07967217266559601, -0.0021168221719563007, -0.0008282806375063956, -0.0012134101707488298, -0.034211449325084686, -1.5497195136049413e-06, -6.651657167822123e-05, -0.0007141662063077092, -0.010757667943835258, -0.0052036321721971035, -0.024113820865750313], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.005178965162485838, -0.9377671480178833], "finish_reason": "stop"}], "metrics": {"energy": 494.68980741119384, "energy_per_token": 4.897918885259345, "time": 13.671653, "time_per_token": 0.135362900990099}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"Input must be a string\")", "tokens": [" Value", "Error", "(", "\"", "Input", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -1.0888429880142212, -1.250410795211792, -0.0017625049222260714, -0.10377573221921921, -0.01786966808140278, -0.25808170437812805, -0.00039426659350283444, -0.009650364518165588], "finish_reason": "stop"}, {"text": " nltk.tokenize.word_tokenizer.WordPunctTokenizer()", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "_", "token", "izer", ".", "Word", "Punct", "Token", "izer", "()", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -1.0804400444030762, -0.0008142255246639252, -0.012112634256482124, -0.8611764311790466, -0.017632165923714638, -0.00070296844933182, -1.2629189491271973, -0.28843873739242554, -0.03623748943209648, -0.4525619447231293, -7.510157047363464e-06, -2.3841574147809297e-05, -0.027423229068517685, -0.3077893555164337], "finish_reason": "stop"}, {"text": " tk.tokenize(text)", "tokens": [" t", "k", ".", "token", "ize", "(", "text", ")", "\n"], "logprobs": [-0.21967877447605133, -8.654219709569588e-05, -2.5152843591058627e-05, -0.0001662831346038729, -6.079655122448457e-06, -0.00043990471749566495, -0.0017049076268449426, -0.011057410389184952, -0.022798100486397743], "finish_reason": "stop"}, {"text": " [w for w in words if w.startswith('$') and w[1:] not in punctuation]", "tokens": [" [", "w", " for", " w", " in", " words", " if", " w", ".", "start", "sw", "ith", "('", "$", "')", " and", " w", "[", "1", ":]", " not", " in", " punct", "uation", "]", "\n"], "logprobs": [-0.5513269305229187, -1.3396928310394287, -0.09144708514213562, -0.0020459212828427553, -0.00035958975786343217, -0.004531471990048885, -0.012178703211247921, -0.10250820964574814, -0.15072935819625854, -0.02767917327582836, -4.23184028477408e-05, -2.3841855067985307e-07, -0.8643434643745422, -0.002281088614836335, -0.00017653337272349745, -0.5943892598152161, -2.0909221172332764, -0.870027482509613, -0.003968697972595692, -0.11569060385227203, -0.07741948962211609, -0.00032824851223267615, -0.054236747324466705, -2.706014311115723e-05, -0.043531790375709534, -0.06264258176088333], "finish_reason": "stop"}, {"text": " nltk.FreqDist(dollar_words)", "tokens": [" n", "lt", "k", ".", "F", "req", "Dist", "(", "dollar", "_", "words", ")", "\n"], "logprobs": [-1.7362940311431885, -6.41325386823155e-05, -2.396077979938127e-05, -9.035655966727063e-05, -0.05110815167427063, -2.8132995794294402e-05, -3.2782016205601394e-05, -0.0015445933677256107, -0.001910886145196855, -1.0490362910786644e-05, -3.886147169396281e-05, -0.00019786784832831472, -0.0621245801448822], "finish_reason": "stop"}, {"text": " pd.DataFrame([(word, freq[word]) for word in freq.keys()],", "tokens": [" pd", ".", "DataFrame", "([", "(", "word", ",", " fre", "q", "[", "word", "])", " for", " word", " in", " fre", "q", ".", "keys", "()", "]", ",", "\n"], "logprobs": [-0.007293148431926966, -2.1934269170742482e-05, -0.0025154873728752136, -2.745011806488037, -0.7990391254425049, -0.7999910712242126, -0.02787616103887558, -0.15402373671531677, -0.00022849810193292797, -0.012993666343390942, -0.00021586472576018423, -0.001593511551618576, -0.0076711829751729965, -0.0004974558250978589, -0.0029170839115977287, -0.1132378950715065, -1.9073468138230965e-06, -1.4567124843597412, -0.017097108066082, -0.00018594920402392745, -0.049055181443691254, -0.0025955105666071177, -2.366687297821045], "finish_reason": "stop"}, {"text": " df", "tokens": [" df"], "logprobs": [-0.006447466555982828], "finish_reason": "stop"}], "metrics": {"energy": 490.82124499893195, "energy_per_token": 4.462011318172109, "time": 13.682044, "time_per_token": 0.12438221818181817}}
{"outputs": [{"text": " not isinstance(text, str):", "tokens": [" not", " isinstance", "(", "text", ",", " str", "):", "\n"], "logprobs": [-0.24291512370109558, -0.013184510171413422, -0.00037269797758199275, -0.0003332536434754729, -0.0004619484825525433, -0.04378421604633331, -0.0028414849657565355, -0.028225287795066833], "finish_reason": "stop"}, {"text": " ValueError(\"'text' must be a string\")", "tokens": [" Value", "Error", "(", "\"", "'", "text", "'", " must", " be", " a", " string", "\"", ")", "\n"], "logprobs": [-0.02748005837202072, -1.2636104656849056e-05, -0.18047691881656647, -0.0989268571138382, -3.6431221961975098, -0.020520644262433052, -0.002062933286651969, -0.38690438866615295, -0.00894432608038187, -0.304937481880188, -0.038415804505348206, -0.3636355698108673, -0.0005955114611424506, -0.008620667271316051], "finish_reason": "stop"}, {"text": " nltk.tokenize.word_tokenizer.WordPunctTokenizer()", "tokens": [" n", "lt", "k", ".", "token", "ize", ".", "word", "_", "token", "izer", ".", "Word", "Punct", "Token", "izer", "()", "\n"], "logprobs": [-0.07291258871555328, -2.8132995794294402e-05, -6.318072337307967e-06, -0.0016718233237043023, -1.0804400444030762, -0.0008142255246639252, -0.012112634256482124, -0.8611764311790466, -0.017632165923714638, -0.00070296844933182, -1.2629189491271973, -0.28843873739242554, -0.03623748943209648, -0.4525619447231293, -7.510157047363464e-06, -2.3841574147809297e-05, -0.027423229068517685, -0.3077893555164337], "finish_reason": "stop"}, {"text": " [word for word in tk.tokenize(text) if word.startswith('$')]", "tokens": [" [", "word", " for", " word", " in", " t", "k", ".", "token", "ize", "(", "text", ")", " if", " word", ".", "start", "sw", "ith", "('", "$", "')", "]", "\n"], "logprobs": [-1.8076647520065308, -0.7185527682304382, -0.11693189293146133, -0.0010091932490468025, -0.00040797016117721796, -0.01012109499424696, -9.131014667218551e-05, -3.862306402879767e-05, -0.0001399419124936685, -1.1324817933200393e-05, -0.0005316513124853373, -0.004666391294449568, -0.06704441457986832, -0.07774291932582855, -0.20077133178710938, -0.3791157305240631, -0.044429827481508255, -2.3841574147809297e-05, -2.3841855067985307e-07, -1.419468641281128, -0.001177571015432477, -0.00011216964776394889, -0.1758829802274704, -0.01942363753914833], "finish_reason": "stop"}, {"text": " [word for word in words if word.startswith(\"$\")]", "tokens": [" [", "word", " for", " word", " in", " words", " if", " word", ".", "start", "sw", "ith", "(", "\"", "$", "\"", ")]", "\n"], "logprobs": [-0.5442787408828735, -0.5437167286872864, -0.11734000593423843, -0.0013097288319841027, -0.00030214988510124385, -0.005751252174377441, -0.017451424151659012, -0.10041782259941101, -0.15094833076000214, -0.019022652879357338, -3.683499380713329e-05, -3.576278118089249e-07, -0.4616698920726776, -0.0026328444946557283, -0.00013267113536130637, -3.886147169396281e-05, -0.5185056924819946, -0.09588202834129333], "finish_reason": "stop"}, {"text": " dict(Counter(dollar_words))", "tokens": [" dict", "(", "Counter", "(", "dollar", "_", "words", "))", "\n"], "logprobs": [-2.320542335510254, -0.7426069378852844, -0.7726309895515442, -0.005773534532636404, -0.003972378559410572, -1.2636104656849056e-05, -3.0636318115284666e-05, -0.02921578660607338, -0.03647693619132042], "finish_reason": "stop"}, {"text": " pd.DataFrame({'Word': freq.keys(), 'Frequency': freq.values()})", "tokens": [" pd", ".", "DataFrame", "({", "'", "Word", "':", " fre", "q", ".", "keys", "()", ",", " '", "F", "requency", "':", " fre", "q", ".", "values", "()", "})", "\n"], "logprobs": [-0.008133149705827236, -2.3841574147809297e-05, -0.002484927186742425, -0.731419026851654, -0.11834109574556351, -0.0043173679150640965, -0.017921658232808113, -0.2837222218513489, -3.576278118089249e-07, -0.0014157997211441398, -0.015349121764302254, -0.004687748849391937, -0.0009333306807093322, -0.07456619292497635, -0.002684087259694934, -0.0006637753685936332, -0.0019039851613342762, -0.03932841867208481, -1.5497195136049413e-06, -5.972207145532593e-05, -0.0007884969236329198, -0.009403107687830925, -0.004811613354831934, -0.02533099800348282], "finish_reason": "stop"}, {"text": " df", "tokens": [" df", "\n"], "logprobs": [-0.005604862235486507, -0.9794963002204895], "finish_reason": "stop"}], "metrics": {"energy": 529.7164139289857, "energy_per_token": 4.527490717341759, "time": 13.45624, "time_per_token": 0.11501059829059829}}
