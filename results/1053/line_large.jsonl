{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0014412740711122751, -4.649054244509898e-05, -0.0002308817347511649, -3.814689989667386e-06, -9.691245941212401e-05, -0.00023672162205912173, -0.007037376053631306, -0.0003081085451412946, -2.3841855067985307e-07, -0.357320636510849, -0.005881266202777624], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS, max_features=10)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " max", "_", "features", "=", "1", "0", ")", "\n"], "logprobs": [-0.0003002431185450405, -9.536738616588991e-07, 0.0, -0.020855654031038284, -0.3361020088195801, -5.006777428206988e-06, -1.6689286894688848e-06, -0.001213171985000372, -0.00204806262627244, 0.0, -8.821448318485636e-06, -2.3841855067985307e-07, 0.0, -1.1920928244535389e-07, -2.6326029300689697, -0.5127800703048706, -8.940656698541716e-06, -0.013996773399412632, -0.0008722314960323274, -0.11558462679386139, -0.013059324584901333, -0.16600514948368073, -0.008834797888994217], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.Text)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "Text", ")", "\n"], "logprobs": [-0.04084484279155731, -5.8412379075889476e-06, -0.0004306104383431375, -0.013308032415807247, -0.0009567927336320281, -0.0002044230350293219, -0.005833268631249666, -0.011785522103309631, -2.2900145053863525, -0.14701427519321442, -0.0829932764172554, -0.13106182217597961], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.003862184938043356, -2.9802276912960224e-06, -8.22540732769994e-06, -0.0001397035230183974, -0.0033964586909860373, -7.235741941258311e-05, -5.352353764465079e-05, -0.00047159992391243577, -0.0005373702733777463, -0.0010121704544872046, -0.02878766693174839], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx])]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "]", ")]", "\n"], "logprobs": [-0.0410291887819767, -0.009372638538479805, -0.01687583141028881, -0.0618232898414135, -0.00048458753735758364, -0.0007005859515629709, -0.00012957210128661245, -0.015096576884388924, -0.00019131260341964662, -0.05584642291069031, -2.850957155227661, -1.9947965145111084, -0.002903533633798361], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.020735979080200195, -0.0069135501980781555, -0.03418760001659393, -0.00013541258522309363, -0.001416275859810412, -0.00044550508027896285, -0.02608305588364601, -0.00023016665363684297, -0.02787778526544571, -0.031776152551174164, -0.8459773063659668, -0.7987568378448486, -2.682172998902388e-05, -0.0041285050101578236, -0.0004236992390360683, -0.002999570919200778, -6.97350042173639e-05, -0.0029510778840631247, -1.1920928244535389e-07, -8.22540732769994e-06, -0.0011150578502565622, -1.1920928244535389e-07, -7.366862701019272e-05, 0.0, -1.1920922133867862e-06, -0.5894612073898315, -0.0016176722710952163], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.03710373491048813, -0.0004574206250254065, -0.0010031197452917695, -1.2040065485052764e-05, -4.768370445162873e-07, -0.0002643712505232543, -0.006164939608424902, -9.226373367710039e-05, -0.0015053142560645938, -0.0037319078110158443, -9.345571743324399e-05, -0.01759304478764534, -0.00011538793478393927, -0.00021455370006151497, -0.00530822342261672, -1.2397689715726301e-05, -0.00010668662434909493, -1.537788011773955e-05, -0.0041809771209955215, -0.04388975724577904, -1.304198980331421, -0.003946493845432997, -0.0019520529313012958, -0.0006349454633891582, -0.002639264799654484], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.27447444200515747, -5.960462772236497e-07, -3.576278118089249e-07, -0.0496920682489872, -0.0014619863359257579, -0.001939917216077447, -0.0031628606375306845, -0.0037537605967372656], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.007406396325677633, -1.5020257706055418e-05, -0.003843659767881036, -0.07806459069252014, -0.4882614016532898, -8.344646857949556e-07, -6.9141146923357155e-06, -0.021538518369197845, -0.004106779582798481, -0.01309262216091156, -0.0006208876147866249, -0.5317096710205078, -0.0011714984430000186, -2.9682672902708873e-05, -0.00013505500101018697, -0.41712790727615356, -0.08186838775873184, -0.0012762743281200528, -0.006433253176510334, -0.04056008160114288], "finish_reason": "stop"}, {"text": " df_top.plot(x=\"Word\", y=\"Count\", kind=\"bar\", legend=False, rot=90)", "tokens": [" df", "_", "top", ".", "plot", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " kind", "=", "\"", "bar", "\"", ",", " legend", "=", "False", ",", " rot", "=", "9", "0", ")", "\n"], "logprobs": [-0.09206429123878479, -0.00013136000779923052, -1.2397689715726301e-05, -0.1192006766796112, -0.01021101325750351, -0.462575763463974, -0.9651474952697754, -0.0005442806868813932, -0.0004183370911050588, -0.0006370898918248713, -2.9802276912960224e-06, -4.649054244509898e-05, -0.007398467976599932, -0.003052458632737398, -2.1219027985353023e-05, -0.00013255194062367082, -1.597391747054644e-05, -0.0014892450999468565, -0.014063071459531784, -0.0001394651480950415, -6.937739817658439e-05, -0.0003334919747430831, -0.1620093584060669, -0.195931077003479, -0.7944400310516357, -0.00026079590315930545, -0.10673259198665619, -0.9234774112701416, -0.6207320094108582, -0.013627473264932632, -1.6844079494476318, -0.0005361788207665086, -0.02592439576983452, -0.0033858849201351404], "finish_reason": "stop"}, {"text": " save_path is None:", "tokens": [" save", "_", "path", " is", " None", ":", "\n"], "logprobs": [-0.03111284226179123, -1.0132738680113107e-05, -1.847726889536716e-05, -0.7583411335945129, -1.0102322101593018, -0.0004314445541240275, -0.012248535640537739], "finish_reason": "stop"}, {"text": "savefig(save_path, bbox_inches=\"tight\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ")", "\n"], "logprobs": [-0.006050484720617533, -0.0005990855861455202, -7.509902934543788e-05, -0.001487697591073811, -7.748573807475623e-06, -2.3841830625315197e-06, -1.0847350358963013, -0.7503964900970459, -8.4638240878121e-06, -0.0001805857609724626, -2.6464111215318553e-05, -3.373566141817719e-05, -0.004157590679824352, -0.0013400870375335217, -4.768370445162873e-07, -1.7165990357170813e-05, -0.13068771362304688, -0.002565547125414014], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.05209480971097946, -0.10899916291236877, -0.03928910568356514], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.014594114385545254, -0.045065563172101974], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.048811350017786026, -2.9682672902708873e-05, -3.2186455882765586e-06, -0.3520151674747467, -0.35436761379241943, -0.0011599486460909247, -0.0054110256023705006], "finish_reason": "stop"}, {"text": " FileNotFoundError(", "tokens": [" File", "NotFound", "Error", "(", "\n"], "logprobs": [-0.05491548031568527, -8.248942322097719e-05, -1.597391747054644e-05, -0.004315824713557959, -1.9559714794158936], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.004851118661463261, -0.02117421105504036, -0.01167807076126337, -0.0011617346899583936, -0.01589825563132763], "finish_reason": "stop"}, {"text": "\"Error: \", e)", "tokens": ["\"", "Error", ":", " \"", ",", " e", ")", "\n"], "logprobs": [-0.936545729637146, -1.5664552450180054, -2.124699115753174, -0.4715147316455841, -0.02952486276626587, -0.046789832413196564, -0.012513809837400913, -0.03647417575120926], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.005030475091189146, -0.3207119107246399], "finish_reason": "stop"}], "metrics": {"energy": 1291.7800594577807, "energy_per_token": 5.36008323426465, "time": 34.30266, "time_per_token": 0.14233468879668051}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0011242267210036516, -3.957670196541585e-05, -0.00015043080202303827, -4.529942543740617e-06, -8.296622399939224e-05, -0.00026854246971197426, -0.005512631498277187, -0.00028701478731818497, -1.1920928244535389e-07, -0.40768662095069885, -0.006112334318459034], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.00031585473334416747, -9.536738616588991e-07, 0.0, -0.016561295837163925, -0.34498319029808044, -5.006777428206988e-06, -2.264974000354414e-06, -0.001157805323600769, -0.0023209319915622473, 0.0, -1.4424220353248529e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.08178743720054626, -0.018655672669410706], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.052060749381780624, -4.172316494077677e-06, -0.00028772983932867646, -0.014879672788083553, -0.0007225048611871898, -0.00014959646796341985, -0.006220859009772539, -0.014315172098577023, -0.15289601683616638, -0.00014852374442853034, -0.00010179955279454589, -6.9141146923357155e-06, -0.060401663184165955, -0.10949302464723587], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0024845702573657036, -3.2186455882765586e-06, -5.125986263010418e-06, -9.7508447652217e-05, -0.003799245459958911, -5.638440416078083e-05, -5.829164365422912e-05, -0.0004782725009135902, -0.0004944770480506122, -0.00045003299601376057, -0.020491331815719604], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.03177961707115173, -0.009874071925878525, -0.010280984453856945, -0.07047786563634872, -0.00046135272714309394, -0.000985375139862299, -6.365573790390044e-05, -0.01593228057026863, -9.500529267825186e-05, -0.049405843019485474, -0.0653049424290657, -0.4804321229457855, -0.0065437559969723225, -0.00013290952483657748, -0.008468550629913807, -0.0019126709084957838, -0.009623563848435879, -1.1920928244535389e-07, -0.00014554394874721766, -1.8000440832111053e-05, -0.001110890181735158, -2.0265558760002023e-06, -7.784063927829266e-05, -4.768360213347478e-06, -2.753696753643453e-05, -0.004897265695035458, -0.0018352109473198652], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.020971812307834625, -0.006688469089567661, -0.03089325875043869, -0.00012146688823122531, -0.0011018402874469757, -0.0003033416287507862, -0.03336310014128685, -0.0002165798214264214, -0.026682276278734207, -0.04219659045338631, -0.48887741565704346], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.04220081865787506, -0.0005882440018467605, -0.000976919662207365, -1.2516897186287679e-05, -7.152555099310121e-07, -0.00024589852546341717, -0.00684204138815403, -6.282132380874828e-05, -0.0012849654303863645, -0.0037370147183537483, -0.00011634149996098131, -0.016473939642310143, -7.366862701019272e-05, -0.0002127659390680492, -0.003934856969863176, -1.156323378381785e-05, -0.00012361239350866526, -1.4066597032069694e-05, -0.0038902093656361103, -0.03712039068341255, -1.5336699485778809, -0.004211367107927799, -0.002241719514131546, -0.0006478118011727929, -0.0029570208862423897], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.26887246966362, -4.768370445162873e-07, -4.768370445162873e-07, -0.06345850974321365, -0.0015594713622704148, -0.0021615491714328527, -0.0025402202736586332, -0.0032036192715168], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.007256579585373402, -1.5139465176616795e-05, -0.004839729983359575, -0.0666063204407692, -1.8172422647476196, -0.027086352929472923, -0.0889621376991272, -8.344646857949556e-07, -3.099436753473128e-06, -0.040197599679231644, -0.0027959563303738832, -0.030490372329950333, -0.0008188708452507854, -0.648687481880188, -0.0010998159414157271, -3.671578815556131e-05, -0.00017963226127903908, -0.6273936629295349, -0.18154345452785492, -0.0027204668149352074, -0.016626369208097458, -0.024088218808174133], "finish_reason": "stop"}, {"text": " df_top.plot(", "tokens": [" df", "_", "top", ".", "plot", "(", "\n"], "logprobs": [-0.13104863464832306, -0.0001137191939051263, -2.038458114839159e-05, -0.10237392038106918, -0.013027790002524853, -0.34020525217056274, -0.7050727605819702], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.03065108321607113, -1.0967194612021558e-05, -2.0265373677830212e-05, -0.6380022168159485, -0.41259118914604187, -6.9141146923357155e-06, -0.001937180757522583, -0.0035612278152257204], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.009139959700405598, -0.0005270045949146152, -6.353653589030728e-05, -0.0015764916315674782, -8.34461570775602e-06, -3.2186455882765586e-06, -0.3774455487728119, -0.0022285168524831533], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.041778795421123505, -0.1097511276602745, -0.03620529919862747], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.015006860718131065, -0.04427861422300339], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.04718359187245369, -3.766942609217949e-05, -4.6491513785440475e-06, -0.384410560131073, -0.4351763427257538, -0.0010546842822805047, -0.004741615150123835], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"The file '{file_path}' was not found.\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "The", " file", " '", "{", "file", "_", "path", "}'", " was", " not", " found", ".", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.06457892060279846, -0.00011777184408856556, -1.7523612768854946e-05, -0.004794173873960972, -0.21078522503376007, -0.059588730335235596, -1.6366379261016846, -0.2643257677555084, -1.0995383262634277, -0.0006432848167605698, -0.29323360323905945, -0.00027295202016830444, -0.00012468514614738524, -0.006811139173805714, -0.6133272051811218, -0.00023445718397852033, -0.0007120219524949789, -0.1317741870880127, -0.007890719920396805, -0.0020343814976513386, -0.03691119700670242, -0.000663894519675523, -0.006638260092586279], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005874155554920435, -0.01762806624174118, -0.012131006456911564, -0.0013223483692854643, -0.014065774157643318], "finish_reason": "stop"}, {"text": "\"An error occurred during function execution:\")", "tokens": ["\"", "An", " error", " occurred", " during", " function", " execution", ":\"", ")", "\n"], "logprobs": [-0.7974997162818909, -0.34448081254959106, -0.09713438898324966, -0.016639500856399536, -1.8471307754516602, -0.8703207969665527, -0.0011376821203157306, -0.15929245948791504, -2.8404016494750977, -0.0007676515378989279], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.005020037293434143, -0.3319731056690216], "finish_reason": "stop"}], "metrics": {"energy": 1195.9571408662794, "energy_per_token": 5.436168822119452, "time": 31.910362, "time_per_token": 0.14504709999999998}}
{"outputs": [{"text": " pd.read_csv(file_path, usecols=['Text'])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " use", "cols", "=['", "Text", "'])", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -1.3113775253295898, -1.1394150257110596, -0.0001394651480950415, -0.7570303082466125, -0.0771314948797226, -0.25141704082489014, -0.025707503780722618], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0002475670480635017, -8.344646857949556e-07, 0.0, -0.021683527156710625, -0.3723747730255127, -4.172316494077677e-06, -2.50339189733495e-06, -0.0013260389678180218, -0.0018917298875749111, 0.0, -1.0490362910786644e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.08720720559358597, -0.02087736874818802], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.056222546845674515, -4.410734163684538e-06, -0.00046230596490204334, -0.014868397265672684, -0.001176023157313466, -0.0003513672563713044, -0.005676933564245701, -0.01328238844871521, -0.1510406881570816, -0.00022182388056535274, -0.00010275312524754554, -6.556489552167477e-06, -0.07200771570205688, -0.09292012453079224], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0027132148388773203, -2.622600959512056e-06, -5.602820692729438e-06, -9.929640509653836e-05, -0.0016023189527913928, -6.305972783593461e-05, -5.125868119648658e-05, -0.00038223114097490907, -0.00043883229955099523, -0.0008139872807078063, -0.027737844735383987], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.042856134474277496, -0.007969368249177933, -0.007348767947405577, -0.09377740323543549, -0.00030250742565840483, -0.0008887869771569967, -7.092700980138034e-05, -0.00934783834964037, -0.00020203932945150882, -0.044889941811561584, -0.07361863553524017, -0.42553117871284485, -0.004869269207119942, -0.0001461399078834802, -0.009596997871994972, -0.0035271355882287025, -0.016070831567049026, -2.3841855067985307e-07, -0.0001486429391661659, -1.7881233361549675e-05, -0.0007823029300197959, -2.0265558760002023e-06, -8.022463589441031e-05, -5.245195097813848e-06, -2.3841574147809297e-05, -0.004599349107593298, -0.0018204560037702322], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.011630939319729805, -0.007829926908016205, -0.03286442905664444, -0.00012230125139467418, -0.0009154658182524145, -0.00018225439998786896, -0.01957830600440502, -0.0003587556129787117, -0.02082996815443039, -0.022825367748737335, -0.540742814540863], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.054951250553131104, -0.0005654881824739277, -0.0006922471220605075, -1.1444026313256472e-05, -1.0728830375228426e-06, -0.00031442465842701495, -0.0070283799432218075, -7.974783511599526e-05, -0.0022501645144075155, -0.0032942809630185366, -0.00011503035057103261, -0.01594272255897522, -9.524368942948058e-05, -0.00021753329201601446, -0.005758363753557205, -1.0609570381348021e-05, -0.00015007323236204684, -1.847726889536716e-05, -0.003587122540920973, -0.03618655726313591, -0.31643128395080566], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.1998671591281891, -4.768370445162873e-07, -9.536738616588991e-07, -0.0460548996925354, -0.0016843193443492055, -0.0019263536669313908, -0.002330684568732977, -0.0037100547924637794], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.008167439140379429, -1.6927575416048057e-05, -0.0038734660483896732, -0.0889434888958931, -0.5010229349136353, -9.536738616588991e-07, -8.34461570775602e-06, -0.028186580166220665, -0.004254457075148821, -0.011911808513104916, -0.0008630603551864624, -0.5311574935913086, -0.0009438112610951066, -2.610649426060263e-05, -0.00012778419477399439, -0.40508538484573364, -0.0816369354724884, -0.001090765930712223, -0.007892257533967495, -0.04954981803894043], "finish_reason": "stop"}, {"text": " df_top.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", "_", "top", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.1251959502696991, -0.00014757021563127637, -1.0490362910786644e-05, -0.12072766572237015, -0.011464430950582027, -1.0690171718597412, -0.010575561784207821, -0.8442882895469666, -0.2614433169364929, -0.0004390706308186054, -0.00035744477645494044, -9.16677454370074e-05, -3.814689989667386e-06, -0.005461414810270071, -0.019307415932416916, -0.006647259928286076, -1.3947389561508317e-05, -9.119095193454996e-05, -7.629365427419543e-06, -0.2835281193256378, -0.03527325764298439, -0.0030467540491372347, -0.18500913679599762, -0.36424511671066284, -0.0042199138551950455], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.025521710515022278, -1.0490362910786644e-05, -2.3245540432981215e-05, -0.6895089745521545, -0.4484100043773651, -8.22540732769994e-06, -0.0026333199348300695, -0.004097281955182552], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=300)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "3", "0", "0", ")", "\n"], "logprobs": [-0.009650719352066517, -0.0004468158003874123, -7.056941103655845e-05, -0.0022820401936769485, -7.986990567587782e-06, -3.933898824470816e-06, -1.1225767135620117, -0.9283595085144043, -1.3708974620385561e-05, -0.0007496645557694137, -0.24148334562778473, -0.002932298230007291, -8.868777513271198e-05, -0.315420538187027, -0.008189549669623375], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.03299742564558983, -0.1203329786658287, -0.03816075623035431], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.01951810158789158, -0.05239350348711014], "finish_reason": "stop"}, {"text": " FileNotFoundError:", "tokens": [" File", "NotFound", "Error", ":", "\n"], "logprobs": [-0.044579993933439255, -2.682172998902388e-05, -3.933898824470816e-06, -1.2008063793182373, -0.0038294093683362007], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"The specified file path does not exist: {exc}\")", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "The", " specified", " file", " path", " does", " not", " exist", ":", " {", "exc", "}\"", ")", "\n"], "logprobs": [-0.058304280042648315, -0.00010048838157672435, -1.3232143828645349e-05, -0.004002180881798267, -0.2195182740688324, -0.053408391773700714, -1.5738893747329712, -1.9983034133911133, -0.03873971104621887, -1.0522785186767578, -0.9838799238204956, -0.0006992755807004869, -0.002199494047090411, -0.07655778527259827, -0.01456862036138773, -1.6645976305007935, -0.14310723543167114, -0.0029277815483510494, -0.15117545425891876], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.004652745556086302, -0.01576237939298153, -0.011490829288959503, -0.001279250718653202, -0.015395843423902988], "finish_reason": "stop"}, {"text": "\"An error occurred:\", e)", "tokens": ["\"", "An", " error", " occurred", ":\"", ",", " e", ")", "\n"], "logprobs": [-0.7835250496864319, -0.44483715295791626, -0.10259503126144409, -0.018695687875151634, -0.43611660599708557, -0.013794681057333946, -0.19090388715267181, -0.01577693037688732, -0.061113230884075165], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.0042474535293877125, -0.3500503599643707], "finish_reason": "stop"}], "metrics": {"energy": 1208.4886196603782, "energy_per_token": 5.0991080998328195, "time": 31.305613, "time_per_token": 0.13209119409282702}}
{"outputs": [{"text": " pd.read_csv(file_path, header=None)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " header", "=", "None", ")", "\n"], "logprobs": [-0.0015687552513554692, -3.85038583772257e-05, -0.00039402826223522425, -4.410734163684538e-06, -9.83428253675811e-05, -0.00025829317746683955, -0.007803904823958874, -0.00030894274823367596, -1.1920928244535389e-07, -1.2586721181869507, -1.4549111127853394, -0.1957157552242279, -0.9199478626251221, -0.6661715507507324, -0.01880776882171631], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.00026294111739844084, -1.0728830375228426e-06, 0.0, -0.022795187309384346, -0.33202776312828064, -5.245195097813848e-06, -2.622600959512056e-06, -0.0013111574808135629, -0.002043066080659628, 0.0, -1.0371154530730564e-05, -2.3841855067985307e-07, 0.0, -1.1920928244535389e-07, -0.08232312649488449, -0.02028821036219597], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.05154652148485184, -5.125986263010418e-06, -0.0005505952867679298, -0.011384054087102413, -0.0008622265886515379, -0.00017355366435367614, -0.006067902315407991, -0.01386240217834711, -0.14489173889160156, -0.00017987063620239496, -9.178694017464295e-05, -8.34461570775602e-06, -0.06637732684612274, -0.11040239781141281], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.002781096613034606, -2.622600959512056e-06, -4.291525328881107e-06, -0.00014506718434859067, -0.0028523020446300507, -5.006664650863968e-05, -3.7431014789035544e-05, -0.0006280356901697814, -0.0003939091111533344, -0.0006790239713154733, -0.03864394873380661], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx])", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.02900088205933571, -0.01151074469089508, -0.010694689117372036, -0.0932648628950119, -0.00031740395934320986, -0.0011712603736668825, -8.964136941358447e-05, -0.017527800053358078, -0.00014995403762441128, -0.05127600207924843, -0.14658184349536896, -1.0266448259353638], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.012274324893951416, -0.00497662415727973, -0.02760542370378971, -0.0001461399078834802, -0.0012968709925189614, -0.00035089056473225355, -0.027066512033343315, -0.0002668739762157202, -0.03289592266082764, -0.042813532054424286, -0.496168315410614], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.028560621663928032, -0.0006276782951317728, -0.0010114560136571527, -1.0609570381348021e-05, -4.768370445162873e-07, -0.00026520551182329655, -0.007490998134016991, -6.341733387671411e-05, -0.0017377528129145503, -0.0034517014864832163, -0.00014757021563127637, -0.012667431496083736, -8.391981828026474e-05, -0.00015829740732442588, -0.006217660382390022, -1.0013530300057027e-05, -0.00010096516780322418, -1.2159273865108844e-05, -0.002936696168035269, -0.02612718939781189, -0.34954386949539185], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.29072248935699463, -7.152555099310121e-07, -4.768370445162873e-07, -0.05495993793010712, -0.0016445695655420423, -0.001789517467841506, -0.0044075725600123405, -0.004390481859445572], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "frequency", "\"", "])", "\n"], "logprobs": [-0.005044352263212204, -1.5616295058862306e-05, -0.0038558910600841045, -0.07107561081647873, -0.6339036822319031, -9.536738616588991e-07, -1.0013530300057027e-05, -0.026694579049944878, -0.004753835499286652, -0.012694974429905415, -0.0007180972606875002, -1.4216482639312744, -0.0022743092849850655, -1.823885577323381e-05, -0.00012718822108581662, -0.8941031098365784, -0.001557686016894877, -0.007631197106093168, -0.04554964601993561], "finish_reason": "stop"}, {"text": " df_top.plot(", "tokens": [" df", "_", "top", ".", "plot", "(", "\n"], "logprobs": [-0.11939376592636108, -0.0001408954558428377, -1.811964830267243e-05, -0.12277807295322418, -0.012017467990517616, -0.4406024217605591, -0.5210803151130676], "finish_reason": "stop"}, {"text": " save_path is None:", "tokens": [" save", "_", "path", " is", " None", ":", "\n"], "logprobs": [-0.030779751017689705, -1.0371154530730564e-05, -2.288792165927589e-05, -0.6914817690849304, -0.9277865290641785, -0.0006275591440498829, -0.011174947954714298], "finish_reason": "stop"}, {"text": "savefig(save_path, bbox_inches=\"tight\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ")", "\n"], "logprobs": [-0.00852812360972166, -0.0005938435206189752, -7.688703772146255e-05, -0.0014423454413190484, -8.34461570775602e-06, -2.3841830625315197e-06, -1.1780883073806763, -0.6482837200164795, -1.1801649634435307e-05, -0.0001896439935080707, -3.0397906812140718e-05, -5.280832192511298e-05, -0.004199970979243517, -0.0014653194230049849, -4.768370445162873e-07, -1.7762025890988298e-05, -0.12570720911026, -0.0020509178284555674], "finish_reason": "stop"}, {"text": "close(ax.figure)", "tokens": ["close", "(", "ax", ".", "figure", ")", "\n"], "logprobs": [-0.048239562660455704, -2.269205093383789, -0.10200690478086472, -0.07787283509969711, -0.0628165677189827, -0.0018180761253461242, -0.048594482243061066], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.01562249381095171, -0.04399904981255531], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.05379017814993858, -2.6940935640595853e-05, -4.172316494077677e-06, -0.3956342339515686, -0.3492177426815033, -0.0010450384579598904, -0.005502317100763321], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"The file {file_path} was not found\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "The", " file", " {", "file", "_", "path", "}", " was", " not", " found", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.06081107631325722, -9.762764238985255e-05, -2.109982233378105e-05, -0.00418774364516139, -0.18288590013980865, -0.05909566581249237, -1.6252140998840332, -0.2199428677558899, -1.456860899925232, -0.24780993163585663, -0.00036042393185198307, -3.015949550899677e-05, -0.01538070011883974, -0.4717611074447632, -0.00036745471879839897, -0.001959905494004488, -1.5396828651428223, -0.0056039136834442616, -0.06773790717124939, -0.0016736084362491965, -0.008227030746638775], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.004588788375258446, -0.018148673698306084, -0.010499123483896255, -0.0010650444310158491, -0.014384621754288673], "finish_reason": "stop"}, {"text": "\"An error occurred during execution.\")", "tokens": ["\"", "An", " error", " occurred", " during", " execution", ".", "\"", ")", "\n"], "logprobs": [-0.7977215647697449, -0.38593220710754395, -0.10765218734741211, -0.0188106931746006, -1.8859524726867676, -1.501963496208191, -2.7961087226867676, -0.5209207534790039, -0.12667644023895264, -0.0021172980777919292], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004711241461336613, -0.3596262037754059], "finish_reason": "stop"}], "metrics": {"energy": 1157.9398079147347, "energy_per_token": 5.436337126360257, "time": 30.6729, "time_per_token": 0.14400422535211266}}
{"outputs": [{"text": " pd.read_csv(file_path, names=[\"Text\"])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " names", "=[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -1.3113775253295898, -3.4515316486358643, -0.5265809297561646, -0.02248041145503521, -0.14093612134456635, -0.00571178225800395, -0.20049530267715454, -0.04070373252034187], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003149013500660658, -1.311301275563892e-06, 0.0, -0.018393076956272125, -0.3590691089630127, -5.125986263010418e-06, -2.622600959512056e-06, -0.001383896335028112, -0.0021381154656410217, 0.0, -1.0013530300057027e-05, -2.3841855067985307e-07, 0.0, -1.1920928244535389e-07, -0.0795195922255516, -0.021146666258573532], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.04148377850651741, -5.006777428206988e-06, -0.0004051103023812175, -0.017184995114803314, -0.0011099375551566482, -0.00021586472576018423, -0.006561757531017065, -0.014402481727302074, -0.12801724672317505, -0.00019643761334009469, -0.00011062010162277147, -6.556489552167477e-06, -0.07731864601373672, -0.11529318243265152], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0022706221789121628, -2.861018856492592e-06, -6.079655122448457e-06, -0.00010096516780322418, -0.0031305374577641487, -4.9470632802695036e-05, -4.541770613286644e-05, -0.0003268184664193541, -0.00036769305006600916, -0.0006127863889560103, -0.022979160770773888], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.018223943188786507, -0.008544198237359524, -0.009772200137376785, -0.1430172473192215, -0.0004599228559527546, -0.0010974343167617917, -6.282132380874828e-05, -0.01534231286495924, -0.00014661667228210717, -0.05051523447036743, -0.05789428576827049, -0.48269182443618774, -0.005056450609117746, -0.00011169286881340668, -0.01607552357017994, -0.002862405963242054, -0.013873218558728695, -2.3841855067985307e-07, -0.00016664070426486433, -1.6331539882230572e-05, -0.00100740697234869, -2.0265558760002023e-06, -6.282132380874828e-05, -6.794906312279636e-06, -2.932505594799295e-05, -0.003503258805721998, -0.0015046000480651855], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.025412702932953835, -0.005207071080803871, -0.02073959819972515, -0.00015686711412854493, -0.0016852713888511062, -0.00020787939138244838, -0.04722043499350548, -0.00021407696476671845, -0.037355948239564896, -0.03520098701119423, -0.6094203591346741], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.030749695375561714, -0.0008148210472427309, -0.0008593680104240775, -1.4424220353248529e-05, -5.960462772236497e-07, -0.0002942844294011593, -0.006549677811563015, -6.925819616299123e-05, -0.0013605633284896612, -0.0027703975792974234, -0.00012957210128661245, -0.01913820579648018, -9.190614218823612e-05, -0.0002252801787108183, -0.005027391016483307, -1.0371154530730564e-05, -0.00016246906307060272, -1.3708974620385561e-05, -0.003525234991684556, -0.03619874268770218, -0.27796027064323425], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.2950305640697479, -5.960462772236497e-07, -7.152555099310121e-07, -0.0687088593840599, -0.0015739921946078539, -0.0017492959741503, -0.0033946766052395105, -0.0033494106028229], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.006941844243556261, -1.9073304429184645e-05, -0.005219167564064264, -0.08904676884412766, -0.4119298756122589, -1.0728830375228426e-06, -9.775113539944869e-06, -0.02551555074751377, -0.004977216944098473, -0.014176612719893456, -0.00098204065579921, -0.552722692489624, -0.0009519098093733191, -2.932505594799295e-05, -0.00016091958968900144, -0.45852845907211304, -0.08430416882038116, -0.0009914488764479756, -0.006494960747659206, -0.052162930369377136], "finish_reason": "stop"}, {"text": " df_top.plot(x=\"Word\", y=\"Count\", kind=\"bar\")", "tokens": [" df", "_", "top", ".", "plot", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " kind", "=", "\"", "bar", "\"", ")", "\n"], "logprobs": [-0.1210617944598198, -0.00010775939153973013, -5.245195097813848e-06, -0.10933081805706024, -0.009929194115102291, -0.36241990327835083, -1.103165626525879, -0.0005248599336482584, -0.00038521020906046033, -0.0006636562757194042, -3.4570634852570947e-06, -6.16293036728166e-05, -0.006829727906733751, -0.0025346316397190094, -2.5748875486897305e-05, -0.00011622230522334576, -1.5616295058862306e-05, -0.0011013639159500599, -0.01625569351017475, -0.00013326710904948413, -4.8040190449682996e-05, -0.0003194298769813031, -0.16586346924304962, -2.0068349838256836, -0.007965820841491222], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03167545050382614, -1.0609570381348021e-05, -2.372236667724792e-05, -0.6937580704689026, -0.003972022328525782], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=300)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "3", "0", "0", ")", "\n"], "logprobs": [-0.008106308057904243, -0.0005011494504287839, -6.842378934379667e-05, -0.0019254018552601337, -8.106198947643861e-06, -2.861018856492592e-06, -0.9905833005905151, -0.9775533080101013, -9.65590606938349e-06, -0.00067473528906703, -0.22240608930587769, -0.003407507436349988, -0.00010394509445177391, -0.3526545464992523, -0.00809838529676199], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.03629807382822037, -0.1255553960800171, -0.03338328003883362], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.012882580049335957, -0.04904383048415184], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.049867529422044754, -3.635817120084539e-05, -3.6954811548639555e-06, -0.346727579832077, -0.4189264476299286, -0.0010275328531861305, -0.0049308366142213345], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found: {file_path}\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", ":", " {", "file", "_", "path", "}\"", ")", " from", " exc", "\n"], "logprobs": [-0.054916270077228546, -0.00010024998482549563, -1.7165990357170813e-05, -0.00455746054649353, -0.21691270172595978, -0.056948471814394, -0.40243130922317505, -0.4768276810646057, -0.0029878043569624424, -0.5804802775382996, -0.008836924098432064, -0.734815239906311, -0.0007818264421075583, -6.472854875028133e-05, -0.01447016280144453, -0.003949343226850033, -0.06154780089855194, -0.0006115949945524335, -0.010182339698076248], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005053010769188404, -0.01763099431991577, -0.012720162980258465, -0.0012518672738224268, -0.01461854949593544], "finish_reason": "stop"}, {"text": "f\"Error occurred: {str(e)}\")", "tokens": ["f", "\"", "Error", " occurred", ":", " {", "str", "(", "e", ")}", "\"", ")", "\n"], "logprobs": [-0.7641476392745972, -0.0017365626990795135, -1.2970235347747803, -1.0351622104644775, -0.49647530913352966, -0.0023054706398397684, -0.920447826385498, -4.672895011026412e-05, -0.0003492222458589822, -0.0053927660919725895, -0.009355632588267326, -0.02954222448170185, -0.052881062030792236], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.0058352830819785595, -0.29088693857192993], "finish_reason": "stop"}], "metrics": {"energy": 1410.2907733745574, "energy_per_token": 5.851828935164138, "time": 36.902348, "time_per_token": 0.15312177593360998}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -0.3150692284107208, -0.0062665874138474464], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0002972637885250151, -8.344646857949556e-07, 0.0, -0.02093060314655304, -1.3946571350097656], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray().flatten()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", ".", "fl", "atten", "()", "\n"], "logprobs": [-0.051607660949230194, -3.6954811548639555e-06, -0.0004615910293068737, -0.013302503153681755, -0.0009865660686045885, -0.00017975145601667464, -0.006229981314390898, -0.010841751471161842, -0.15555819869041443, -0.00023505308490712196, -9.16677454370074e-05, -7.271740287251305e-06, -0.06999970227479935, -2.5563831329345703, -0.4579644203186035, -6.270212179515511e-05, -0.0009556017466820776, -0.39916321635246277, -2.8210999965667725, -3.3378546504536644e-06, -0.0001280225842492655, -0.0136146554723382], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0031767638865858316, -2.3841830625315197e-06, -4.172316494077677e-06, -0.0001232548092957586, -0.0024584089405834675, -5.2569914259947836e-05, -5.400034933700226e-05, -0.0005265279905870557, -0.0004945961991325021, -0.0008011230966076255, -0.022368857637047768], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.03681433945894241, -0.00915992259979248, -0.013891559094190598, -0.08663848787546158, -0.0005247407825663686, -0.0009398810216225684, -8.189342770492658e-05, -0.014210226014256477, -0.0001629458274692297, -0.041393425315618515, -0.09962568432092667, -0.4451645016670227, -0.005998347420245409, -0.0001267114421352744, -0.011720369569957256, -0.002791557926684618, -0.01064432691782713, -1.1920928244535389e-07, -0.0001399419124936685, -1.3470558769768104e-05, -0.0011104138102382421, -2.9802276912960224e-06, -9.023735765367746e-05, -7.510157047363464e-06, -3.135155202471651e-05, -0.00490450207144022, -0.0017056216020137072], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names_out())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "())", "\n"], "logprobs": [-0.017941564321517944, -0.006142310332506895, -0.02395601198077202, -0.00013553177996072918, -0.0012969900853931904, -0.0002735478919930756, -0.021072199568152428, -0.00019202772818971425, -0.02427346631884575, -0.032742492854595184, -0.8671226501464844, -0.8540430665016174, -2.753696753643453e-05, -0.005659153219312429, -0.0004911408759653568, -0.004610147327184677, -8.022463589441031e-05, -0.0021771318279206753, -1.1920928244535389e-07, -7.152531907195225e-06, -0.0010988633148372173, -1.1920928244535389e-07, -6.01988795096986e-05, 0.0, -2.0265558760002023e-06, -1.1262362003326416, -0.003975109662860632, -0.002778600202873349, -0.005441140849143267], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.045523565262556076, -0.0007433511782437563, -0.0012849654303863645, -1.3947389561508317e-05, -7.152555099310121e-07, -0.00030119650182314217, -0.010490748099982738, -8.201262971851975e-05, -0.0017193072708323598, -0.004259086679667234, -0.00011073929636040702, -0.012091787531971931, -9.691245941212401e-05, -0.00020132421923335642, -0.004054659977555275, -8.940656698541716e-06, -0.00013624693383462727, -1.823885577323381e-05, -0.003823234234005213, -0.028970541432499886, -0.21488294005393982], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.2567460834980011, -7.152555099310121e-07, -7.152555099310121e-07, -0.0625956654548645, -0.0015829188050702214, -0.0018516314448788762, -0.003628220409154892, -0.003914433531463146], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.006490104831755161, -1.597391747054644e-05, -0.0038278657011687756, -0.07332000881433487, -0.5188219547271729, -1.1920922133867862e-06, -7.033323527139146e-06, -0.029823778197169304, -0.005488801747560501, -0.00959829706698656, -0.0008006466086953878, -0.5872178673744202, -0.0009398810216225684, -2.3007127310847864e-05, -0.00014244495832826942, -0.3814795911312103, -0.07723765075206757, -0.001258296542800963, -0.0073417858220636845, -0.03424854204058647], "finish_reason": "stop"}, {"text": " df_top.plot.barh(x=\"Word\", y=\"Count\", legend=False)", "tokens": [" df", "_", "top", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.11160623282194138, -0.0001538873475510627, -7.629365427419543e-06, -0.13902804255485535, -0.007974689826369286, -1.119193434715271, -0.010612952522933483, -0.49889427423477173, -0.003736183512955904, -0.26969560980796814, -0.00048708971007727087, -0.00038258862332440913, -0.0023657686542719603, -4.291525328881107e-06, -0.002303091809153557, -0.06165899336338043, -0.0005915798828937113, -1.7762025890988298e-05, -0.00011336160969221964, -6.6756979322235566e-06, -0.31299006938934326, -0.958748459815979, -7.152301259338856e-05, -0.027087396010756493, -0.6445972323417664, -0.0034572849981486797], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.032169174402952194, -1.07287787614041e-05, -2.098061486321967e-05, -0.628885805606842, -0.46735748648643494, -9.298280929215252e-06, -0.0024078681599348783, -0.003224770538508892], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.009392006322741508, -0.00036161558819003403, -8.070142939686775e-05, -0.001693839905783534, -7.629365427419543e-06, -2.622600959512056e-06, -0.3799206614494324, -0.0018704315880313516], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.056834954768419266, -0.1110355406999588, -0.03724949061870575], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.01632489077746868, -0.044564828276634216], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.04486121982336044, -3.504691630951129e-05, -3.2186455882765586e-06, -0.3316752314567566, -0.373001366853714, -0.0013387774815782905, -0.005286641884595156], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File '{file_path}' not found.\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " '", "{", "file", "_", "path", "}'", " not", " found", ".", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.05113046616315842, -9.321732068201527e-05, -1.7762025890988298e-05, -0.005294705741107464, -0.19469358026981354, -0.047763485461473465, -0.41752803325653076, -1.7463178634643555, -0.0005246216314844787, -0.23489652574062347, -0.00027533553657121956, -7.188061863416806e-05, -0.0065117790363729, -0.19713935256004333, -0.0013400870375335217, -0.23667877912521362, -0.012612106278538704, -0.002012015553191304, -0.04491262137889862, -0.0004196478403173387, -0.0062825800850987434], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005446001887321472, -0.017243817448616028, -0.012018527835607529, -0.0012978235026821494, -0.01610860414803028], "finish_reason": "stop"}, {"text": "f\"Error: {e}\")", "tokens": ["f", "\"", "Error", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.748637318611145, -0.0017485818825662136, -1.3111989498138428, -0.9479632377624512, -0.07559089362621307, -0.2519746422767639, -0.020833821967244148, -0.06675151735544205, -0.06718196719884872], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004236295353621244, -0.27064114809036255], "finish_reason": "stop"}], "metrics": {"energy": 1281.8491116237628, "energy_per_token": 5.232037190301073, "time": 33.79299, "time_per_token": 0.13793057142857143}}
{"outputs": [{"text": " pd.read_csv(file_path, encoding=\"utf-8\")", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " encoding", "=", "\"", "utf", "-", "8", "\"", ")", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -1.3113775253295898, -2.1938507556915283, -1.4420212507247925, -0.017615532502532005, -0.5861880779266357, -0.05189914256334305, -0.006207589991390705, -0.06575722992420197, -0.3305414617061615, -0.0065287151373922825], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003149013500660658, -1.311301275563892e-06, 0.0, -0.018393076956272125, -0.3590691089630127, -5.125986263010418e-06, -2.622600959512056e-06, -0.001383896335028112, -0.0021381154656410217, 0.0, -1.0013530300057027e-05, -2.3841855067985307e-07, 0.0, -1.1920928244535389e-07, -0.0795195922255516, -0.021146666258573532], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.04148377850651741, -5.006777428206988e-06, -0.0004051103023812175, -0.017184995114803314, -0.0011099375551566482, -0.00021586472576018423, -0.006561757531017065, -0.014402481727302074, -0.12801724672317505, -0.00019643761334009469, -0.00011062010162277147, -6.556489552167477e-06, -0.07731864601373672, -0.11529318243265152], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0022706221789121628, -2.861018856492592e-06, -6.079655122448457e-06, -0.00010096516780322418, -0.0031305374577641487, -4.9470632802695036e-05, -4.541770613286644e-05, -0.0003268184664193541, -0.00036769305006600916, -0.0006127863889560103, -0.022979160770773888], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.018223943188786507, -0.008544198237359524, -0.009772200137376785, -0.1430172473192215, -0.0004599228559527546, -0.0010974343167617917, -6.282132380874828e-05, -0.01534231286495924, -0.00014661667228210717, -0.05051523447036743, -0.05789428576827049, -0.48269182443618774, -0.005056450609117746, -0.00011169286881340668, -0.01607552357017994, -0.002862405963242054, -0.013873218558728695, -2.3841855067985307e-07, -0.00016664070426486433, -1.6331539882230572e-05, -0.00100740697234869, -2.0265558760002023e-06, -6.282132380874828e-05, -6.794906312279636e-06, -2.932505594799295e-05, -0.003503258805721998, -0.0015046000480651855], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.025412702932953835, -0.005207071080803871, -0.02073959819972515, -0.00015686711412854493, -0.0016852713888511062, -0.00020787939138244838, -0.04722043499350548, -0.00021407696476671845, -0.037355948239564896, -0.03520098701119423, -0.7890086770057678, -0.5223052501678467, -2.4318398573086597e-05, -0.0035143063869327307, -0.00046754872892051935, -0.00797220692038536, -5.721882189391181e-05, -0.002255040919408202, -1.1920928244535389e-07, -5.722029527532868e-06, -0.0005134217790327966, -1.1920928244535389e-07, -6.48477507638745e-05, 0.0, -2.3841830625315197e-06, -0.35364437103271484, -0.0015211451100185513], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03258650004863739, -0.0006957017467357218, -0.0013386583887040615, -1.3112935448589269e-05, -8.344646857949556e-07, -0.000327652640407905, -0.007541874423623085, -7.045020902296528e-05, -0.0021158705931156874, -0.004227511119097471, -0.00011860620725201443, -0.015547379851341248, -9.345571743324399e-05, -0.00019095504831057042, -0.0038932969328016043, -1.0609570381348021e-05, -0.00014518637908622622, -1.7523612768854946e-05, -0.0035908047575503588, -0.04150756448507309, -0.1963040828704834], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.34033724665641785, -4.768370445162873e-07, -8.344646857949556e-07, -0.058638446033000946, -0.0014355602907016873, -0.0023971651680767536, -0.0025587696582078934, -0.003362836316227913], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-0.008658249862492085, -1.549708758830093e-05, -0.0042341588996350765, -0.10042698681354523, -1.567647933959961], "finish_reason": "stop"}, {"text": " df_top.plot.barh(x=\"Word\", y=\"Count\", legend=False)", "tokens": [" df", "_", "top", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.12410220503807068, -0.00012778419477399439, -2.253030106658116e-05, -0.10800600796937943, -0.013378022238612175, -1.1727962493896484, -0.012912706471979618, -0.5256270170211792, -0.004156522452831268, -0.36987927556037903, -0.00038187362952157855, -0.0003231241717003286, -0.0031307751778513193, -3.933898824470816e-06, -0.002520481590181589, -0.05999410152435303, -0.0006837890832684934, -1.3589766240329482e-05, -0.00011789103882620111, -5.8412379075889476e-06, -0.29563871026039124, -0.8581268191337585, -7.986703712958843e-05, -0.03697679564356804, -0.5372926592826843, -0.0032808545511215925], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.031711481511592865, -9.775113539944869e-06, -2.002696055569686e-05, -0.7358130216598511, -0.004111409652978182], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.006988960318267345, -0.0006042085005901754, -6.83045873302035e-05, -0.0015397133538499475, -7.867782187531702e-06, -3.6954811548639555e-06, -0.36306214332580566, -0.002080539707094431], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.052441805601119995, -0.11818434298038483, -0.037981048226356506], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.014708887785673141, -0.04771609231829643], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.054986342787742615, -3.123234637314454e-05, -3.099436753473128e-06, -0.36342117190361023, -0.35091209411621094, -0.0011323237558826804, -0.0055164252407848835], "finish_reason": "stop"}, {"text": " FileNotFoundError(", "tokens": [" File", "NotFound", "Error", "(", "\n"], "logprobs": [-0.05714934691786766, -9.643566590966657e-05, -1.7165990357170813e-05, -0.004612995311617851, -2.15065336227417], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.00452672503888607, -0.019532248377799988, -0.01352998148649931, -0.0010623055277392268, -0.01463076751679182], "finish_reason": "stop"}, {"text": "f\"Error occurred: {str(e)}\")", "tokens": ["f", "\"", "Error", " occurred", ":", " {", "str", "(", "e", ")}", "\"", ")", "\n"], "logprobs": [-0.7534531950950623, -0.001958120847120881, -1.2753050327301025, -0.9161970615386963, -0.5122733116149902, -0.002582074608653784, -1.0231049060821533, -4.577531944960356e-05, -0.000359351426595822, -0.005850571673363447, -0.008290045894682407, -0.028173599392175674, -0.04595562443137169], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004157709423452616, -0.2967146635055542], "finish_reason": "stop"}], "metrics": {"energy": 1217.5471002769482, "energy_per_token": 5.435478126236376, "time": 31.958126, "time_per_token": 0.14267020535714287}}
{"outputs": [{"text": " pd.read_csv(file_path, header=0)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " header", "=", "0", ")", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -1.3113775253295898, -1.4137775897979736, -0.22411991655826569, -0.48038220405578613, -0.25695955753326416, -0.029730744659900665], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003149013500660658, -1.311301275563892e-06, 0.0, -0.018393076956272125, -0.3590691089630127, -5.125986263010418e-06, -2.622600959512056e-06, -0.001383896335028112, -0.0021381154656410217, 0.0, -1.0013530300057027e-05, -2.3841855067985307e-07, 0.0, -1.1920928244535389e-07, -0.0795195922255516, -0.021146666258573532], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.loc[:, \"Text\"]).toarray()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "loc", "[:", ",", " \"", "Text", "\"", "])", ".", "to", "array", "()", "\n"], "logprobs": [-0.04148377850651741, -5.006777428206988e-06, -0.0004051103023812175, -0.017184995114803314, -0.0011099375551566482, -0.00021586472576018423, -0.006561757531017065, -0.014402481727302074, -2.638416051864624, -2.8224635124206543, -0.016112826764583588, -0.012079067528247833, -0.10924756526947021, -0.00028224775451235473, -4.255681051290594e-05, -0.12375203520059586, -2.4086086750030518, -0.4201665222644806, -4.541770613286644e-05, -0.002783355303108692, -0.7591872811317444], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0051484862342476845, -2.7418097943154862e-06, -5.245195097813848e-06, -0.00014053787162993103, -0.0031742684077471495, -6.4490144723095e-05, -6.246371776796877e-05, -0.0005364171229302883, -0.00046885941992513835, -0.0009626284008845687, -0.03375134989619255], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx].astype(int)) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "]", ".", "ast", "ype", "(", "int", "))", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.03379122540354729, -0.010920166969299316, -0.010696222074329853, -0.0748552605509758, -0.0004814896092284471, -0.0007566926069557667, -0.00010108436981681734, -0.021952373906970024, -0.0003108495147898793, -0.062228985130786896, -2.888310432434082, -2.3516063690185547, -0.7668579816818237, -3.4927710657939315e-05, -0.006801075302064419, -0.029307246208190918, -0.005691513419151306, -0.6378713250160217, -0.03686144948005676, -0.0006046851049177349, -0.011467259377241135, -0.011808023788034916, -0.04489917308092117, -3.576278118089249e-07, -0.00011002412065863609, -0.00014757021563127637, -0.0004033228906337172, -7.510157047363464e-06, -0.00018320789968129247, -3.123234637314454e-05, -0.00010275312524754554, -0.012508747167885303, -0.0021649987902492285], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.01600501872599125, -0.006073115859180689, -0.0263113621622324, -0.0001618731184862554, -0.0010311054065823555, -0.0004403813509270549, -0.023903170600533485, -0.0002040654799202457, -0.05351913347840309, -0.04129414260387421, -0.4236154854297638], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03815111890435219, -0.0006119524477981031, -0.0010584949050098658, -9.775113539944869e-06, -7.152555099310121e-07, -0.00029774048016406596, -0.0065623498521745205, -7.807903602952138e-05, -0.0017339446349069476, -0.003851616056635976, -0.00010299152199877426, -0.01369708776473999, -7.70062324590981e-05, -0.0002325502864550799, -0.0035640785936266184, -1.2159273865108844e-05, -9.417090768693015e-05, -1.5020257706055418e-05, -0.0041326601058244705, -0.036815375089645386, -0.23621614277362823], "finish_reason": "stop"}, {"text": " [words_freq[word_idx][0] for word_idx in range(10)]", "tokens": [" [", "words", "_", "freq", "[", "word", "_", "idx", "][", "0", "]", " for", " word", "_", "idx", " in", " range", "(", "1", "0", ")]", "\n"], "logprobs": [-1.9050118923187256, -3.042880058288574, -0.16956092417240143, -0.0015113847330212593, -0.008812228217720985, -2.9266293048858643, -1.281489372253418, -0.5953421592712402, -0.29835590720176697, -0.007909052073955536, -0.00010346830822527409, -0.0013727060286328197, -0.00012540031457319856, -4.0531076592742465e-06, -5.781483559985645e-05, -0.0002579356369096786, -0.00157196877989918, -0.00015937011630740017, -0.16994671523571014, -0.004707563202828169, -0.0023977598175406456, -0.0019747773185372353], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-0.007278237026184797, -1.680836794548668e-05, -0.005736673250794411, -0.07628831267356873, -1.5791261196136475], "finish_reason": "stop"}, {"text": " df_top.plot(x=\"Word\", y=\"Count\", kind=\"bar\", legend=False, rot=90)", "tokens": [" df", "_", "top", ".", "plot", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " kind", "=", "\"", "bar", "\"", ",", " legend", "=", "False", ",", " rot", "=", "9", "0", ")", "\n"], "logprobs": [-0.1081710010766983, -0.00012599628826137632, -1.0490362910786644e-05, -0.1031683012843132, -0.01327497698366642, -0.34732258319854736, -0.947551429271698, -0.0004558716027531773, -0.0005011494504287839, -0.0007445423980243504, -3.099436753473128e-06, -5.578839045483619e-05, -0.0064814588986337185, -0.002362676663324237, -2.2172682292875834e-05, -0.00014399446081370115, -1.680836794548668e-05, -0.0014805557439103723, -0.014264287427067757, -0.00014852374442853034, -6.568216485902667e-05, -0.00021240839851088822, -0.14247693121433258, -0.13461628556251526, -0.9225280284881592, -0.00031001531169749796, -0.13889162242412567, -0.933410108089447, -0.5702633261680603, -0.011794593185186386, -1.601821780204773, -0.000459565402707085, -0.025453610345721245, -0.0027998790610581636], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.031412553042173386, -9.417489309271332e-06, -1.490105023549404e-05, -0.6954048871994019, -0.0042633600533008575], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0070146480575203896, -0.0005502378917299211, -7.486063259420916e-05, -0.0014784131199121475, -7.867782187531702e-06, -2.50339189733495e-06, -0.4057537913322449, -0.0018980359891429543], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.05103350430727005, -0.11784873157739639, -0.04017332196235657], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.019137853756546974, -0.051232174038887024], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.05017074570059776, -3.862306402879767e-05, -2.861018856492592e-06, -0.34941738843917847, -0.39114439487457275, -0.0011879300000146031, -0.00504731759428978], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File '{file_path}' not found.\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " '", "{", "file", "_", "path", "}'", " not", " found", ".", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.060223549604415894, -0.00010716341057559475, -2.1457441107486375e-05, -0.005184776149690151, -0.17511671781539917, -0.05488625541329384, -0.40284016728401184, -1.9612371921539307, -0.00048780461656861007, -0.2854723036289215, -0.0003302744007669389, -7.807903602952138e-05, -0.00806231889873743, -0.19234547019004822, -0.0011553048389032483, -0.25685545802116394, -0.010804015211760998, -0.0024812407791614532, -0.06024083495140076, -0.0003424296446610242, -0.007088038604706526], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005072462372481823, -0.0220514964312315, -0.010256442241370678, -0.0011743560899049044, -0.014908798970282078], "finish_reason": "stop"}, {"text": "e)", "tokens": ["e", ")", "\n"], "logprobs": [-2.397080898284912, -0.023231005296111107, -0.048515331000089645], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004423832520842552, -0.2925952672958374], "finish_reason": "stop"}], "metrics": {"energy": 1309.7474086837765, "energy_per_token": 5.345907790546026, "time": 34.781315, "time_per_token": 0.14196455102040817}}
{"outputs": [{"text": " pd.read_csv(file_path, usecols=['Text'])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " use", "cols", "=['", "Text", "'])", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -1.3113775253295898, -1.1394150257110596, -0.0001394651480950415, -0.7570303082466125, -0.0771314948797226, -0.25141704082489014, -0.025707503780722618], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0002475670480635017, -8.344646857949556e-07, 0.0, -0.021683527156710625, -1.5381157398223877], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.05699452757835388, -4.887569048150908e-06, -0.0004144048143643886, -0.010498887859284878, -0.0010013333521783352, -0.00026663561584427953, -0.0064892759546637535, -0.009706089273095131, -0.13537713885307312, -0.00021300431399140507, -0.0001037067049765028, -9.179073458653875e-06, -0.07952994108200073, -0.10368535667657852], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.004407691303640604, -2.7418097943154862e-06, -5.602820692729438e-06, -0.00013374387344811112, -0.0033983595203608274, -5.936446541454643e-05, -5.447716102935374e-05, -0.0004611144249793142, -0.0004919749335385859, -0.0006292270263656974, -0.02194642648100853], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx])", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.024019557982683182, -0.005972514860332012, -0.010485793463885784, -0.09227269142866135, -0.0004017737810499966, -0.0006445952458307147, -5.340433563105762e-05, -0.013276741839945316, -0.00017009719158522785, -0.05533718690276146, -0.09199176728725433, -1.1704082489013672], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.009826146997511387, -0.007283680606633425, -0.0319814570248127, -0.00011562632425921038, -0.0009901388548314571, -0.00020621081057470292, -0.010764390230178833, -0.0001629458274692297, -0.031497228890657425, -0.03449549153447151, -0.5151476263999939], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.03281090408563614, -0.000625176471658051, -0.0015532821416854858, -1.0847986231965479e-05, -9.536738616588991e-07, -0.0003108495147898793, -0.007146510295569897, -7.199982064776123e-05, -0.0018035589018836617, -0.003100709058344364, -8.987976616481319e-05, -0.012579145841300488, -7.199982064776123e-05, -0.00022456508304458112, -0.0042932722717523575, -1.0251946150674485e-05, -0.000129691296024248, -1.2516897186287679e-05, -0.0033420443069189787, -0.033521391451358795, -1.7213581800460815, -0.003431743010878563, -0.0018711454467847943, -0.0006127863889560103, -0.0030961930751800537], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.3074410557746887, -4.768370445162873e-07, -8.344646857949556e-07, -0.04349447041749954, -0.0015312624163925648, -0.0020543676801025867, -0.0028383941389620304, -0.0037491286639124155], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0065632970072329044, -1.585470999998506e-05, -0.004041243344545364, -0.08156705647706985, -0.31479987502098083, -1.0728830375228426e-06, -8.940656698541716e-06, -0.030012939125299454, -0.005489987321197987, -0.00982508435845375, -0.0007578838267363608, -0.5023563504219055, -0.0009778724052011967, -2.8967437174287625e-05, -0.00013028726971242577, -0.43859922885894775, -0.08471990376710892, -0.0010040724882856011, -0.007683367468416691, -0.04622303321957588], "finish_reason": "stop"}, {"text": " df_top.plot(", "tokens": [" df", "_", "top", ".", "plot", "(", "\n"], "logprobs": [-0.13028584420681, -0.000120751719805412, -1.3947389561508317e-05, -0.09355411678552628, -0.016479216516017914, -0.3453207015991211, -0.5352088809013367], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03182753920555115, -1.0251946150674485e-05, -2.5033637939486653e-05, -0.6976693868637085, -0.004782072268426418], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.008328113704919815, -0.0004403813509270549, -8.189342770492658e-05, -0.0021031422074884176, -6.794906312279636e-06, -2.861018856492592e-06, -0.4309360980987549, -0.001734420657157898], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.04194273427128792, -0.09903115779161453, -0.03559990972280502], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.014606332406401634, -0.04835076257586479], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.04134172573685646, -3.8265450712060556e-05, -4.768360213347478e-06, -0.31730616092681885, -0.3755289316177368, -0.0010026433737948537, -0.005599883385002613], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found: {exc.filename}\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", ":", " {", "exc", ".", "filename", "}\"", ")", " from", " exc", "\n"], "logprobs": [-0.053484898060560226, -0.0001037067049765028, -1.966933996300213e-05, -0.004674934316426516, -0.21557916700839996, -0.05455681309103966, -0.4119909703731537, -0.3875161111354828, -0.0024307011626660824, -0.43773210048675537, -0.009027144871652126, -0.7517669200897217, -0.26444491744041443, -0.0028808305505663157, -0.013187451288104057, -0.0036618339363485575, -0.4710085988044739, -0.02618827298283577, -0.01357855275273323], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.006343112327158451, -0.017731482163071632, -0.011614796705543995, -0.001334967790171504, -0.012234287336468697], "finish_reason": "stop"}, {"text": "\"Error: An unexpected error occurred:\", str(e))", "tokens": ["\"", "Error", ":", " An", " unexpected", " error", " occurred", ":\"", ",", " str", "(", "e", "))", "\n"], "logprobs": [-0.8253440856933594, -1.4742475748062134, -2.2040724754333496, -1.3052830696105957, -0.5121858716011047, -0.07437299191951752, -0.011387825012207031, -1.997291922569275, -0.036075834184885025, -1.0108294486999512, -8.737658936297521e-05, -0.0005415403284132481, -0.010830076411366463, -0.06985285878181458], "finish_reason": "stop"}, {"text": " None", "tokens": [" None"], "logprobs": [-0.004719784017652273], "finish_reason": "stop"}], "metrics": {"energy": 1137.34050021553, "energy_per_token": 5.892955959665959, "time": 29.84859, "time_per_token": 0.1546559067357513}}
{"outputs": [{"text": " pd.read_csv(file_path, usecols=[\"Text\"])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " use", "cols", "=[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.0015687552513554692, -3.85038583772257e-05, -0.00039402826223522425, -4.410734163684538e-06, -9.83428253675811e-05, -0.00025829317746683955, -0.007803904823958874, -0.00030894274823367596, -1.1920928244535389e-07, -1.2586721181869507, -1.2460335493087769, -0.00011514954530866817, -0.7570868730545044, -0.5805656909942627, -0.10027214884757996, -0.0010119322687387466, -0.23290817439556122, -0.0252460315823555], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0002884448622353375, -9.536738616588991e-07, 0.0, -0.024772951379418373, -1.6208497285842896], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.04571126028895378, -3.3378546504536644e-06, -0.0004119024670217186, -0.01168796792626381, -0.0009541726321913302, -0.00016449528629891574, -0.005749829579144716, -0.013603131286799908, -0.1254560351371765, -0.0001958416833076626, -0.00010287232726113871, -8.940656698541716e-06, -0.07086695730686188, -0.0909477174282074], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.003821452846750617, -3.576272320060525e-06, -6.437280717364047e-06, -0.0001397035230183974, -0.0020718553569167852, -5.686121585313231e-05, -8.21318244561553e-05, -0.0004822045157197863, -0.000597179401665926, -0.0008480527903884649, -0.028491919860243797], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx])", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.024594774469733238, -0.008776187896728516, -0.0081019327044487, -0.07303304970264435, -0.0005308172549121082, -0.001176023157313466, -8.40390202938579e-05, -0.010844345204532146, -0.0001731960946926847, -0.06490351259708405, -0.10138654708862305, -1.1432229280471802], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.022070620208978653, -0.005923099350184202, -0.028675077483057976, -0.00020215852418914437, -0.001534119131974876, -0.00026782741770148277, -0.01942574232816696, -0.0002603192115202546, -0.025494517758488655, -0.01735652983188629, -0.5378718972206116], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.0305627528578043, -0.0005751386051997542, -0.0012531769461929798, -1.3708974620385561e-05, -8.344646857949556e-07, -0.0002494739310350269, -0.007399769499897957, -7.378782902378589e-05, -0.001686461502686143, -0.003483776468783617, -0.00010549465514486656, -0.016128310933709145, -6.5205356804654e-05, -0.0002324311062693596, -0.0031527597457170486, -1.2636104656849056e-05, -0.00010966652916977182, -1.5258672647178173e-05, -0.004050623159855604, -0.03665231913328171, -0.18782532215118408], "finish_reason": "stop"}, {"text": " [word_freq[0] for word_freq in words_freq[:10]]", "tokens": [" [", "word", "_", "freq", "[", "0", "]", " for", " word", "_", "freq", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-1.7913522720336914, -0.5953351259231567, -1.6337727308273315, -0.15480029582977295, -0.03347747027873993, -0.006611970253288746, -0.0009052231325767934, -0.004150467924773693, -0.000587767455726862, -1.5616295058862306e-05, -1.490105023549404e-05, -4.0531076592742465e-06, -0.0018378286622464657, -3.576278118089249e-07, -3.576278118089249e-07, -0.07162028551101685, -0.0020170120988041162, -0.0024779110681265593, -0.00492075365036726, -0.00436852453276515], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.007529924623668194, -1.4662635294371285e-05, -0.004462047480046749, -0.07227811962366104, -0.5527958869934082, -1.4305104514278355e-06, -8.821448318485636e-06, -0.02337658405303955, -0.005113024730235338, -0.012199075892567635, -0.0008081507403403521, -0.4660293757915497, -0.001077190856449306, -2.9205850296420977e-05, -0.0001113352773245424, -0.47895076870918274, -0.08247242867946625, -0.0010725465836003423, -0.007548026740550995, -0.04574405774474144], "finish_reason": "stop"}, {"text": " df_top.plot(", "tokens": [" df", "_", "top", ".", "plot", "(", "\n"], "logprobs": [-0.1421605348587036, -0.0001677133986959234, -2.52720492426306e-05, -0.08860773593187332, -0.013538920320570469, -0.43503352999687195, -0.7443398237228394], "finish_reason": "stop"}, {"text": " save_path is None:", "tokens": [" save", "_", "path", " is", " None", ":", "\n"], "logprobs": [-0.03087245486676693, -1.2040065485052764e-05, -2.074220174108632e-05, -0.6509777307510376, -1.0309370756149292, -0.0005360596696846187, -0.011064720340073109], "finish_reason": "stop"}, {"text": "savefig(save_path, bbox_inches=\"tight\", dpi=300)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ",", " d", "pi", "=", "3", "0", "0", ")", "\n"], "logprobs": [-0.006285067647695541, -0.00041392818093299866, -6.472854875028133e-05, -0.0019602624233812094, -8.34461570775602e-06, -2.7418097943154862e-06, -1.2755565643310547, -0.6797435283660889, -1.7165990357170813e-05, -0.0001262346631847322, -3.302042750874534e-05, -3.683499380713329e-05, -0.005091676488518715, -0.0014300844632089138, -5.960462772236497e-07, -1.8715683836489916e-05, -2.228790760040283, -0.19347280263900757, -3.290122185717337e-05, -0.0004926898400299251, -0.24418218433856964, -0.0056069958955049515, -0.00017271934484597296, -0.018174076452851295, -0.005686535034328699], "finish_reason": "stop"}, {"text": "clf()", "tokens": ["cl", "f", "()", "\n"], "logprobs": [-2.886362075805664, -0.029424162581562996, -0.00047743841423653066, -0.12459224462509155], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.01362488605082035, -0.043409090489149094], "finish_reason": "stop"}, {"text": " FileNotFoundError as file_err:", "tokens": [" File", "NotFound", "Error", " as", " file", "_", "err", ":", "\n"], "logprobs": [-0.04598317667841911, -3.981510963058099e-05, -4.0531076592742465e-06, -0.38404688239097595, -3.0245141983032227, -0.01944783888757229, -1.139915943145752, -0.0017805927200242877, -0.008412756025791168], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found: {exc.filename}\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", ":", " {", "exc", ".", "filename", "}\"", ")", " from", " exc", "\n"], "logprobs": [-0.05783915892243385, -0.00010418349120300263, -1.6569954823353328e-05, -0.0047134957276284695, -0.19610176980495453, -0.05329176038503647, -0.40355730056762695, -0.4994436502456665, -0.0023631523363292217, -0.48283836245536804, -0.008129957132041454, -0.8549574017524719, -0.19441702961921692, -0.0022827538195997477, -0.014311999082565308, -0.003759936196729541, -0.38887596130371094, -0.019625883549451828, -0.013045910745859146], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005502198822796345, -0.015817061066627502, -0.011120838113129139, -0.0011756658786907792, -0.017706772312521935], "finish_reason": "stop"}, {"text": "e)", "tokens": ["e", ")", "\n"], "logprobs": [-2.6794917583465576, -0.02377396449446678, -0.04817640036344528], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.005268261767923832, -0.3112431764602661], "finish_reason": "stop"}], "metrics": {"energy": 1220.9321348648075, "energy_per_token": 5.678754115650268, "time": 31.949078, "time_per_token": 0.1486003627906977}}
{"outputs": [{"text": " pd.read_csv(file_path, header='infer')", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " header", "='", "in", "fer", "')", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -1.3113775253295898, -1.4137775897979736, -1.765181303024292, -0.0032696856651455164, -2.9802276912960224e-06, -0.1720845252275467, -0.01520741730928421], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS, max_features=100)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " max", "_", "features", "=", "1", "0", "0", ")", "\n"], "logprobs": [-0.0003149013500660658, -1.311301275563892e-06, 0.0, -0.018393076956272125, -0.3590691089630127, -5.125986263010418e-06, -2.622600959512056e-06, -0.001383896335028112, -0.0021381154656410217, 0.0, -1.0013530300057027e-05, -2.3841855067985307e-07, 0.0, -1.1920928244535389e-07, -2.5906736850738525, -0.4905749559402466, -1.0013530300057027e-05, -0.010827246122062206, -0.0009709649602882564, -0.11188681423664093, -0.013127919286489487, -2.180551767349243, -0.9308393597602844, -0.015611931681632996], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.04148377850651741, -5.006777428206988e-06, -0.0004051103023812175, -0.017184995114803314, -0.0011099375551566482, -0.00021586472576018423, -0.006561757531017065, -0.014402481727302074, -0.12801724672317505, -0.00019643761334009469, -0.00011062010162277147, -6.556489552167477e-06, -0.07731864601373672, -0.11529318243265152], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0022706221789121628, -2.861018856492592e-06, -6.079655122448457e-06, -0.00010096516780322418, -0.0031305374577641487, -4.9470632802695036e-05, -4.541770613286644e-05, -0.0003268184664193541, -0.00036769305006600916, -0.0006127863889560103, -0.022979160770773888], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx])", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.018223943188786507, -0.008544198237359524, -0.009772200137376785, -0.1430172473192215, -0.0004599228559527546, -0.0010974343167617917, -6.282132380874828e-05, -0.01534231286495924, -0.00014661667228210717, -0.05051523447036743, -0.05789428576827049, -0.970858633518219], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.025412702932953835, -0.005207071080803871, -0.02073959819972515, -0.00015686711412854493, -0.0016852713888511062, -0.00020787939138244838, -0.04722043499350548, -0.00021407696476671845, -0.037355948239564896, -0.03520098701119423, -0.6094203591346741], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.030749695375561714, -0.0008148210472427309, -0.0008593680104240775, -1.4424220353248529e-05, -5.960462772236497e-07, -0.0002942844294011593, -0.006549677811563015, -6.925819616299123e-05, -0.0013605633284896612, -0.0027703975792974234, -0.00012957210128661245, -0.01913820579648018, -9.190614218823612e-05, -0.0002252801787108183, -0.005027391016483307, -1.0371154530730564e-05, -0.00016246906307060272, -1.3708974620385561e-05, -0.003525234991684556, -0.03619874268770218, -0.27796027064323425], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.2950305640697479, -5.960462772236497e-07, -7.152555099310121e-07, -0.0687088593840599, -0.0015739921946078539, -0.0017492959741503, -0.0033946766052395105, -0.0033494106028229], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.006941844243556261, -1.9073304429184645e-05, -0.005219167564064264, -0.08904676884412766, -0.4119298756122589, -1.0728830375228426e-06, -9.775113539944869e-06, -0.02551555074751377, -0.004977216944098473, -0.014176612719893456, -0.00098204065579921, -0.552722692489624, -0.0009519098093733191, -2.932505594799295e-05, -0.00016091958968900144, -0.45852845907211304, -0.08430416882038116, -0.0009914488764479756, -0.006494960747659206, -0.052162930369377136], "finish_reason": "stop"}, {"text": " df_top.plot.barh(x=\"Word\", y=\"Count\")", "tokens": [" df", "_", "top", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.1210617944598198, -0.00010775939153973013, -5.245195097813848e-06, -0.10933081805706024, -0.009929194115102291, -1.191429853439331, -0.008266046643257141, -0.44634944200515747, -0.004002774599939585, -0.323154091835022, -0.0005171154043637216, -0.0002616301644593477, -0.003786419751122594, -4.410734163684538e-06, -0.0021189635153859854, -0.06001565605401993, -0.0008501967531628907, -1.1801649634435307e-05, -9.727005090098828e-05, -8.702239938429557e-06, -1.3122625350952148, -0.004651677794754505], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.031736426055431366, -1.1086402082582936e-05, -2.2172682292875834e-05, -0.7268300652503967, -0.004323658533394337], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.009684602729976177, -0.0004080893413629383, -7.688703772146255e-05, -0.0013917532050982118, -7.629365427419543e-06, -3.814689989667386e-06, -0.389702707529068, -0.002113491529598832], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.03278321772813797, -0.08766520768404007, -0.039667267352342606], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.01753002591431141, -0.04883325845003128], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.053067270666360855, -3.2066785934148356e-05, -3.933898824470816e-06, -0.37210145592689514, -0.3162575960159302, -0.0011087467428296804, -0.005289962515234947], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found at '{file_path}'\")", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", " at", " '", "{", "file", "_", "path", "}'", "\"", ")", "\n"], "logprobs": [-0.04070693626999855, -9.369411418447271e-05, -1.7404405298293568e-05, -0.004437243565917015, -0.169325053691864, -0.0539369098842144, -0.42073890566825867, -0.4376133382320404, -0.0024509173817932606, -0.9184778332710266, -2.1600332260131836, -0.0001896439935080707, -0.5543802380561829, -0.0004680253332480788, -0.00010835537250386551, -0.007403911091387272, -0.3484481871128082, -0.003390162019059062, -2.602318048477173], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005727428011596203, -0.016500324010849, -0.011942082084715366, -0.001129227806814015, -0.0137788075953722], "finish_reason": "stop"}, {"text": "f\"An error occurred: {e}\")", "tokens": ["f", "\"", "An", " error", " occurred", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.8069077134132385, -0.0013072286965325475, -0.43939927220344543, -0.07108304649591446, -0.023967184126377106, -0.3148355484008789, -0.002036522841081023, -0.46507054567337036, -0.015490688383579254, -0.06485804170370102, -0.055921051651239395], "finish_reason": "stop"}, {"text": " None", "tokens": [" None"], "logprobs": [-0.004134441260248423], "finish_reason": "stop"}], "metrics": {"energy": 1206.6709391059874, "energy_per_token": 5.484867905027215, "time": 32.331367, "time_per_token": 0.14696075909090908}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0013150862650945783, -3.123234637314454e-05, -0.0002615109842736274, -3.2186455882765586e-06, -9.16677454370074e-05, -0.00025018901214934886, -0.007203558459877968, -0.00025340684805996716, -1.1920928244535389e-07, -0.3406352698802948, -0.005871311295777559], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.00031513971043750644, -7.152555099310121e-07, 0.0, -0.020581839606165886, -1.5686216354370117], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.Text.values)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "Text", ".", "values", ")", "\n"], "logprobs": [-0.03802603483200073, -5.006777428206988e-06, -0.0004533693427219987, -0.012586208991706371, -0.0008355463505722582, -0.00019870213873218745, -0.005134017206728458, -0.010850477032363415, -2.5362350940704346, -0.13232825696468353, -2.4878625869750977, -0.23586253821849823, -0.06047606095671654, -0.12715649604797363], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0029526231810450554, -2.622600959512056e-06, -7.510157047363464e-06, -0.00010585224663373083, -0.0025909922551363707, -4.684815212385729e-05, -5.6503606174374e-05, -0.0004577780782710761, -0.00046826363541185856, -0.0007195267826318741, -0.023834729567170143], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.040010012686252594, -0.007467807270586491, -0.010782198049128056, -0.08712777495384216, -0.0003300360403954983, -0.0008356655016541481, -8.391981828026474e-05, -0.012095085345208645, -0.00031871485407464206, -0.04865250736474991, -0.031162993982434273, -0.44937628507614136, -0.007787818089127541, -0.0001311216183239594, -0.0107793677598238, -0.0038095773197710514, -0.012439285404980183, -2.3841855067985307e-07, -0.00017653337272349745, -1.585470999998506e-05, -0.0009473841637372971, -2.861018856492592e-06, -7.283422019099817e-05, -6.6756979322235566e-06, -3.242440288886428e-05, -0.004274873994290829, -0.0013099670177325606], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.012193423695862293, -0.0057523190043866634, -0.029517685994505882, -0.00016091958968900144, -0.0015775627689436078, -0.00039176418795250356, -0.033876966685056686, -0.00027426297310739756, -0.02764299511909485, -0.027713032439351082, -0.5473248958587646], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.027978524565696716, -0.0004040378553327173, -0.000860201776959002, -1.2993727978027891e-05, -9.536738616588991e-07, -0.00024625606602057815, -0.004281995818018913, -6.09140915912576e-05, -0.0015479261055588722, -0.003982708789408207, -9.822363062994555e-05, -0.015594796277582645, -7.152301259338856e-05, -0.00024232311989180744, -0.003563840873539448, -1.156323378381785e-05, -0.00011491115583339706, -1.7165990357170813e-05, -0.0037290574982762337, -0.0250779390335083, -0.2331237643957138], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.22204431891441345, -7.152555099310121e-07, -7.152555099310121e-07, -0.062367092818021774, -0.0016578990034759045, -0.001764646964147687, -0.003315192647278309, -0.0036410484462976456], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.005423593334853649, -1.6569954823353328e-05, -0.004434632603079081, -0.0671587884426117, -2.1219823360443115, -0.027551034465432167, -0.11633484065532684, -8.344646857949556e-07, -2.50339189733495e-06, -0.039291396737098694, -0.0034818758722394705, -0.03334327042102814, -0.0010355116100981832, -0.6435335874557495, -0.0012700833613052964, -3.0278701160568744e-05, -0.00017557987303007394, -0.8304288387298584, -0.05192064493894577, -0.018457679077982903, -0.03564569354057312], "finish_reason": "stop"}, {"text": " (", "tokens": [" (", "\n"], "logprobs": [-2.4646947383880615, -0.005071632098406553], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.031705591827631, -1.0371154530730564e-05, -2.2649508537142538e-05, -0.7810315489768982, -0.0049811312928795815], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=300, bbox_inches=\"tight\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "3", "0", "0", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ")", "\n"], "logprobs": [-0.006140177603811026, -0.000607782625593245, -6.97350042173639e-05, -0.001573873101733625, -7.748573807475623e-06, -2.622600959512056e-06, -0.9843915700912476, -0.8343529105186462, -1.0490362910786644e-05, -0.000797192333266139, -0.27174586057662964, -0.00322370114736259, -9.7508447652217e-05, -1.1879873275756836, -0.08362974971532822, -8.344646857949556e-07, -0.00013958434283267707, -2.1815061700181104e-05, -4.362964682513848e-05, -0.0010609956225380301, -0.0001867835089797154, -5.960462772236497e-07, -5.364403477869928e-06, -0.03368830308318138, -0.003051626728847623], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.038534414023160934, -0.10042095184326172, -0.03600407764315605], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.014542537741363049, -0.04158681631088257], "finish_reason": "stop"}, {"text": " FileNotFoundError:", "tokens": [" File", "NotFound", "Error", ":", "\n"], "logprobs": [-0.04520674794912338, -3.7788631743751466e-05, -3.814689989667386e-06, -1.2660465240478516, -0.0038696660194545984], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found: {exc.filename}\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", ":", " {", "exc", ".", "filename", "}\"", ")", " from", " exc", "\n"], "logprobs": [-0.051358334720134735, -8.868777513271198e-05, -1.5020257706055418e-05, -0.00471824174746871, -0.17897924780845642, -0.06072032079100609, -0.44443878531455994, -0.4287756681442261, -0.0021879561245441437, -0.5175101161003113, -0.007792194839566946, -1.0446622371673584, -0.21616478264331818, -0.002040567807853222, -0.014208698645234108, -0.0039724973030388355, -0.4249465763568878, -0.016272347420454025, -0.011233181692659855], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005834216717630625, -0.01666927896440029, -0.008273376151919365, -0.0010931475553661585, -0.015871386975049973], "finish_reason": "stop"}, {"text": "\"Error encountered in the function:\", str(e))", "tokens": ["\"", "Error", " encountered", " in", " the", " function", ":\"", ",", " str", "(", "e", "))", "\n"], "logprobs": [-0.8385502696037292, -1.503545880317688, -3.8023743629455566, -2.9591071605682373, -0.7255648374557495, -0.30947020649909973, -0.32551032304763794, -0.08025135844945908, -1.0934258699417114, -8.177422569133341e-05, -0.00034731553751043975, -0.009640565142035484, -0.05187729746103287], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.0043265074491500854, -2.1672563552856445, -3.933898824470816e-06, -2.861018856492592e-06, -8.702239938429557e-06, -3.302042750874534e-05, -0.14328338205814362], "finish_reason": "stop"}], "metrics": {"energy": 1195.7155330533976, "energy_per_token": 5.561467595597199, "time": 31.341045, "time_per_token": 0.1457723023255814}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -0.3150692284107208, -0.0062665874138474464], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0002972637885250151, -8.344646857949556e-07, 0.0, -0.02093060314655304, -0.43427664041519165, -5.722029527532868e-06, -2.622600959512056e-06, -0.001531619462184608, -0.001795467222109437, 0.0, -1.0251946150674485e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.0786563903093338, -0.020777195692062378], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.04054668918251991, -4.0531076592742465e-06, -0.0004881620698142797, -0.016124792397022247, -0.0009304723353125155, -0.00011920218821614981, -0.00552021898329258, -0.016785690560936928, -0.14239194989204407, -0.00019393471302464604, -8.177422569133341e-05, -7.986990567587782e-06, -0.06767786294221878, -0.10158687829971313], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.003314004512503743, -2.3841830625315197e-06, -6.318072337307967e-06, -0.00011062010162277147, -0.0015348332235589623, -5.4834770708112046e-05, -4.911301948595792e-05, -0.0005168771021999419, -0.00038258862332440913, -0.0008448368753306568, -0.021385902538895607], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.03694164380431175, -0.010083921253681183, -0.009771727956831455, -0.09819004684686661, -0.00047708096099086106, -0.0011948358733206987, -8.535020606359467e-05, -0.012159035541117191, -0.0002932118659373373, -0.05406385660171509, -0.0565958246588707, -0.4327036738395691, -0.005565505009144545, -0.00013147920253686607, -0.0097267497330904, -0.003017042065039277, -0.016113178804516792, -1.1920928244535389e-07, -0.00015639036428183317, -1.3708974620385561e-05, -0.000929281348362565, -2.3841830625315197e-06, -9.047575440490618e-05, -7.390948667307384e-06, -3.6000557884108275e-05, -0.005747933406382799, -0.0017446548445150256], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.01303132064640522, -0.006835410837084055, -0.020870013162493706, -0.00017009719158522785, -0.0014832934830337763, -0.00019012074335478246, -0.029294626787304878, -0.00018499570433050394, -0.025849122554063797, -0.02563140168786049, -0.9759963750839233, -0.5576236248016357, -0.0001699779968475923, -0.0023877699859440327, -0.0006115949945524335, -0.016057457774877548, -1.1920928244535389e-07, -2.1576648578047752e-05, -3.957670196541585e-05, -0.0005732323625124991, -1.311301275563892e-06, -3.302042750874534e-05, -3.6954811548639555e-06, -1.6331539882230572e-05, -0.00436318339779973], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03130776807665825, -0.0005716835148632526, -0.0010885033989325166, -1.4543427823809907e-05, -7.152555099310121e-07, -0.00033778208307921886, -0.009279809892177582, -8.260862523457035e-05, -0.001408419106155634, -0.0036841630935668945, -0.00010418349120300263, -0.015382109209895134, -8.725739462533966e-05, -0.000219321038457565, -0.00447189761325717, -1.0609570381348021e-05, -0.00010168035078095272, -1.1205610462639015e-05, -0.00351204932667315, -0.03495800867676735, -0.2764085829257965], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.2336001843214035, -7.152555099310121e-07, -1.0728830375228426e-06, -0.053773459047079086, -0.0016355245606973767, -0.001928376266732812, -0.003020013216882944, -0.0038718036375939846], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=top_words, columns=[\"Word\", \"WordCount\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Word", "Count", "\"", "])", "\n"], "logprobs": [-0.005703603383153677, -1.8715683836489916e-05, -0.004955509677529335, -0.06338713318109512, -1.8187098503112793, -0.025904765352606773, -0.08348292112350464, -4.768370445162873e-07, -3.099436753473128e-06, -0.0471242293715477, -0.003125071059912443, -0.03681158274412155, -0.0010521834483370185, -0.6346109509468079, -0.0011587579501792789, -3.1470757676288486e-05, -0.0001456631434848532, -3.282283067703247, -1.9278181791305542, -0.01236946601420641, -0.012093907222151756, -0.0213727168738842], "finish_reason": "stop"}, {"text": " df_top.plot(", "tokens": [" df", "_", "top", ".", "plot", "(", "\n"], "logprobs": [-0.13971704244613647, -0.00014006110723130405, -2.2291887944447808e-05, -0.12152977287769318, -0.010119442827999592, -0.33327364921569824, -0.4851521849632263], "finish_reason": "stop"}, {"text": " save_path is None:", "tokens": [" save", "_", "path", " is", " None", ":", "\n"], "logprobs": [-0.031886547803878784, -1.0371154530730564e-05, -2.3007127310847864e-05, -0.7543001174926758, -1.100020408630371, -0.0004576589271891862, -0.010465739294886589], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.007568257860839367, -0.00036638224264606833, -7.533743337262422e-05, -0.001328658196143806, -8.22540732769994e-06, -3.099436753473128e-06, -0.4558159112930298, -0.0018918488640338182], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.026808204129338264, -0.1219976469874382, -0.04139434173703194], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.012809142470359802, -0.047486480325460434], "finish_reason": "stop"}, {"text": " FileNotFoundError as error:", "tokens": [" File", "NotFound", "Error", " as", " error", ":", "\n"], "logprobs": [-0.047543320804834366, -3.4927710657939315e-05, -4.172316494077677e-06, -0.32667604088783264, -2.2256112098693848, -0.00906223151832819, -0.006933675613254309], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found: {exc.filename}\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", ":", " {", "exc", ".", "filename", "}\"", ")", " from", " exc", "\n"], "logprobs": [-0.059507861733436584, -0.00014709345123264939, -1.6212332411669195e-05, -0.005396323278546333, -0.20595683157444, -0.05186598002910614, -0.4059382677078247, -0.49598491191864014, -0.0022157898638397455, -0.5766387581825256, -0.008918333798646927, -0.7340185642242432, -0.22340235114097595, -0.0020524642895907164, -0.015541746281087399, -0.004091345705091953, -0.49254006147384644, -0.013783746398985386, -0.012365109287202358], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.0062851859256625175, -0.01771860010921955, -0.012521344237029552, -0.0011954312212765217, -0.015020717866718769], "finish_reason": "stop"}, {"text": "f\"Error occurred: {e}\")", "tokens": ["f", "\"", "Error", " occurred", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-1.026624321937561, -0.001932421582750976, -1.3038349151611328, -0.9654174447059631, -0.5309122204780579, -0.0025758917909115553, -0.5154199600219727, -0.017879270017147064, -0.04220619052648544, -0.05879649892449379], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004576446954160929, -0.3109285533428192], "finish_reason": "stop"}], "metrics": {"energy": 1335.9717060604107, "energy_per_token": 5.937652026935159, "time": 34.835835, "time_per_token": 0.15482593333333336}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -0.3150692284107208, -0.0062665874138474464], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0002972637885250151, -8.344646857949556e-07, 0.0, -0.02093060314655304, -0.43427664041519165, -5.722029527532868e-06, -2.622600959512056e-06, -0.001531619462184608, -0.001795467222109437, 0.0, -1.0251946150674485e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.0786563903093338, -0.020777195692062378], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.Text)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "Text", ")", "\n"], "logprobs": [-0.04054668918251991, -4.0531076592742465e-06, -0.0004881620698142797, -0.016124792397022247, -0.0009304723353125155, -0.00011920218821614981, -0.00552021898329258, -0.016785690560936928, -2.479412794113159, -0.13492830097675323, -0.10695178061723709, -0.11250538378953934], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0037882011383771896, -2.7418097943154862e-06, -6.794906312279636e-06, -0.00014494798961095512, -0.0027443624567240477, -4.95898348162882e-05, -4.708655978902243e-05, -0.00043049128726124763, -0.0004854215949308127, -0.0007602662080898881, -0.021119004115462303], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.033330585807561874, -0.00942778866738081, -0.012508982792496681, -0.09414181113243103, -0.0004012971476186067, -0.0010961245279759169, -5.519237674889155e-05, -0.012498034164309502, -0.000286657246761024, -0.03997210040688515, -0.08922865241765976, -0.41367030143737793, -0.00457834592089057, -0.00013422065239865333, -0.010447099804878235, -0.002911735326051712, -0.013971969485282898, -2.3841855067985307e-07, -0.00016139635408762842, -1.9192511899746023e-05, -0.0009371418273076415, -2.50339189733495e-06, -7.986703712958843e-05, -4.172316494077677e-06, -3.194758028257638e-05, -0.004131948109716177, -0.0014775798190385103], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.017741555348038673, -0.006180696655064821, -0.028804462403059006, -0.00013350549852475524, -0.0016678959364071488, -0.0002848696312867105, -0.020891495048999786, -0.0002991705550812185, -0.022686583921313286, -0.024061337113380432, -0.939572811126709, -0.6165913939476013, -0.0002060916303889826, -0.001949435449205339, -0.0005422552349045873, -0.00700600678101182, -2.3841855067985307e-07, -2.002696055569686e-05, -3.862306402879767e-05, -0.00041559641249477863, -1.6689286894688848e-06, -4.0649541915627196e-05, -3.3378546504536644e-06, -1.3708974620385561e-05, -0.004618097562342882], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.038434963673353195, -0.0006735440110787749, -0.0009022455778904259, -9.536697689327411e-06, -4.768370445162873e-07, -0.00031418632715940475, -0.005321030039340258, -7.521823135903105e-05, -0.0020584126468747854, -0.0036336842458695173, -0.0001174142598756589, -0.01502658985555172, -8.618460560683161e-05, -0.0002127659390680492, -0.005762748885899782, -1.0609570381348021e-05, -9.762764238985255e-05, -1.8596476365928538e-05, -0.0034199818037450314, -0.02725028060376644, -0.18267390131950378], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.2178603857755661, -7.152555099310121e-07, -4.768370445162873e-07, -0.07331269979476929, -0.0014577010879293084, -0.0016479019541293383, -0.0030801494140177965, -0.0034623933024704456], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.006038753781467676, -1.6331539882230572e-05, -0.00425825547426939, -0.07849372923374176, -0.5119817852973938, -7.152555099310121e-07, -9.536697689327411e-06, -0.02582588978111744, -0.005353637970983982, -0.011817566119134426, -0.0008318539476022124, -0.6230133771896362, -0.000972155889030546, -2.777537883957848e-05, -0.0001255195093108341, -0.9920729994773865, -0.05106397345662117, -0.007137041538953781, -0.056122586131095886], "finish_reason": "stop"}, {"text": " df_top.plot(", "tokens": [" df", "_", "top", ".", "plot", "(", "\n"], "logprobs": [-0.1731865108013153, -0.00015078838623594493, -9.894321920000948e-06, -0.12234073132276535, -0.011728616431355476, -0.3529928922653198, -0.5337687730789185], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.03153188154101372, -1.3232143828645349e-05, -2.7894584491150454e-05, -0.620388388633728, -0.4835290312767029, -7.629365427419543e-06, -0.0021865288726985455, -0.0038208591286092997], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.008247957564890385, -0.0006127863889560103, -7.211902266135439e-05, -0.0018135543214157224, -7.629365427419543e-06, -2.622600959512056e-06, -0.3740440905094147, -0.0019269485492259264], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.03937884420156479, -0.06901053339242935, -0.035784415900707245], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.012205788865685463, -0.04472671449184418], "finish_reason": "stop"}, {"text": " FileNotFoundError:", "tokens": [" File", "NotFound", "Error", ":", "\n"], "logprobs": [-0.05231510475277901, -3.6954195820726454e-05, -2.7418097943154862e-06, -1.169365406036377, -0.003859572345390916], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found at '{file_path}'\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", " at", " '", "{", "file", "_", "path", "}'", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.05552146956324577, -0.00014149141497910023, -1.6569954823353328e-05, -0.0036178866866976023, -0.18687860667705536, -0.06351736187934875, -0.38508349657058716, -0.43934786319732666, -0.0023266407661139965, -0.9197928309440613, -2.015040874481201, -0.00018106251081917435, -0.5164059400558472, -0.0004124982515349984, -8.77341881277971e-05, -0.008685076609253883, -0.3188829720020294, -0.00359543738886714, -0.08031758666038513, -0.0006847421173006296, -0.008356604725122452], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.004990027751773596, -0.01603880524635315, -0.010107169859111309, -0.0013710394268855453, -0.012763476930558681], "finish_reason": "stop"}, {"text": "\"An error occurred:\", e)", "tokens": ["\"", "An", " error", " occurred", ":\"", ",", " e", ")", "\n"], "logprobs": [-0.7659410834312439, -0.4140537977218628, -0.10716791450977325, -0.016830820590257645, -0.3718695342540741, -0.015189923346042633, -0.2338501513004303, -0.01757090725004673, -0.04947902634739876], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.0054697138257324696, -0.34815382957458496], "finish_reason": "stop"}], "metrics": {"energy": 1183.2476196594234, "energy_per_token": 5.378398271179197, "time": 31.741, "time_per_token": 0.14427727272727273}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0015687552513554692, -3.85038583772257e-05, -0.00039402826223522425, -4.410734163684538e-06, -9.83428253675811e-05, -0.00025829317746683955, -0.007803904823958874, -0.00030894274823367596, -1.1920928244535389e-07, -0.3351997137069702, -0.005409840028733015], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003033416287507862, -9.536738616588991e-07, 0.0, -0.022212404757738113, -0.4935497045516968, -5.8412379075889476e-06, -2.0265558760002023e-06, -0.0014044907875359058, -0.002369812224060297, 0.0, -1.1086402082582936e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.07081931829452515, -0.024796906858682632], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.03638980910181999, -3.6954811548639555e-06, -0.0005988473421894014, -0.011725435964763165, -0.0007169060409069061, -0.00021908267808612436, -0.005237785633653402, -0.012517458759248257, -0.14659245312213898, -0.00018070495571009815, -0.00011121608258690685, -7.390948667307384e-06, -0.06722688674926758, -0.10690689086914062], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.002774439286440611, -2.861018856492592e-06, -5.602820692729438e-06, -0.00011967896716669202, -0.001866623992100358, -7.176141662057489e-05, -5.245071224635467e-05, -0.0005725175142288208, -0.0005341533687897027, -0.0007160721579566598, -0.023884663358330727], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx])", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.030474761500954628, -0.009594990871846676, -0.013130390085279942, -0.06200089305639267, -0.0002992897352669388, -0.000724887300748378, -6.0794889577664435e-05, -0.013993717730045319, -0.00015686711412854493, -0.060722339898347855, -0.047905195504426956, -1.0491762161254883], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.016096990555524826, -0.005676933564245701, -0.022017329931259155, -0.000105375460407231, -0.0009793015196919441, -0.00024875884992070496, -0.01954382099211216, -0.00016389934171456844, -0.029582615941762924, -0.04101968929171562, -0.6171579360961914], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03623415529727936, -0.0006587718962691724, -0.0010580186499282718, -1.0251946150674485e-05, -5.960462772236497e-07, -0.00024673278676345944, -0.011364724487066269, -6.09140915912576e-05, -0.0020772088319063187, -0.004045280162245035, -0.00010561384988250211, -0.015995750203728676, -9.846202738117427e-05, -0.0002153879904653877, -0.003888428211212158, -1.2516897186287679e-05, -0.00011657988943625242, -1.597391747054644e-05, -0.0045225718058645725, -0.03328423574566841, -0.2518579661846161], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.21857379376888275, -7.152555099310121e-07, -5.960462772236497e-07, -0.06844739615917206, -0.001654209685511887, -0.002464830409735441, -0.0022300630807876587, -0.0036656346637755632], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.005794631317257881, -1.645074735279195e-05, -0.004290305078029633, -0.08928818255662918, -1.6626198291778564, -0.02638079598546028, -0.09052421897649765, -4.768370445162873e-07, -4.291525328881107e-06, -0.04869962856173515, -0.003402517642825842, -0.03373406082391739, -0.0006144542712718248, -0.6790410876274109, -0.0011264891363680363, -2.992108420585282e-05, -0.0001705739414319396, -0.5461748242378235, -0.18019114434719086, -0.002331279218196869, -0.015173016116023064, -0.018471604213118553], "finish_reason": "stop"}, {"text": " df_top.plot.barh(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", "_", "top", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.123224176466465, -0.00016735584358684719, -6.198863957251888e-06, -0.11011050641536713, -0.009847749024629593, -1.234179973602295, -0.010930189862847328, -0.48231032490730286, -0.0037557794712483883, -0.32190465927124023, -0.0005046047735959291, -0.0003461238811723888, -0.0021856960374861956, -5.722029527532868e-06, -0.001663492526859045, -0.043903447687625885, -0.0007014198345132172, -1.5735502529423684e-05, -0.00012396997772157192, -6.079655122448457e-06, -0.26659342646598816, -2.309084177017212, -0.0018165291985496879, -0.05195946246385574, -0.3778297007083893, -0.005436398554593325], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.0310582984238863, -9.775113539944869e-06, -2.52720492426306e-05, -0.6946378350257874, -0.004009780008345842], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.009946072474122047, -0.00044705410255119205, -6.794698856538162e-05, -0.0016770598012953997, -9.179073458653875e-06, -3.099436753473128e-06, -0.385356068611145, -0.0017902314430102706], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.027094358578324318, -0.12088961899280548, -0.04019130393862724], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.011287875473499298, -0.04937475547194481], "finish_reason": "stop"}, {"text": " FileNotFoundError:", "tokens": [" File", "NotFound", "Error", ":", "\n"], "logprobs": [-0.05301130935549736, -3.6477376852417365e-05, -3.099436753473128e-06, -1.171924352645874, -0.004239619243890047], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"The file at {exc.filename} was not found.\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "The", " file", " at", " {", "exc", ".", "filename", "}", " was", " not", " found", ".", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.05485951155424118, -0.00013851160474587232, -1.7046782886609435e-05, -0.004342887084931135, -0.18269048631191254, -0.05748367682099342, -1.548077940940857, -0.23259183764457703, -1.645196557044983, -0.32628533244132996, -1.1567037105560303, -0.04206538572907448, -0.0027594605926424265, -0.008737782947719097, -0.5469602346420288, -0.00013469743134919554, -0.00044526674901135266, -0.32990825176239014, -0.004421933554112911, -0.0025064502842724323, -0.4018307328224182, -0.018640460446476936, -0.009877849370241165], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.0040543037466704845, -0.017213234677910805, -0.011460777372121811, -0.0011993603548035026, -0.015650073066353798], "finish_reason": "stop"}, {"text": "\"An error occurred:\", e)", "tokens": ["\"", "An", " error", " occurred", ":\"", ",", " e", ")", "\n"], "logprobs": [-0.6877321600914001, -0.41394922137260437, -0.10763762891292572, -0.020436670631170273, -0.3709978759288788, -0.017039919272065163, -0.21770985424518585, -0.014062601141631603, -0.059593673795461655], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.006012922618538141, -0.2990792393684387], "finish_reason": "stop"}], "metrics": {"energy": 1203.7458336296077, "energy_per_token": 5.624980530979475, "time": 31.703655, "time_per_token": 0.14814792056074766}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0015687552513554692, -3.85038583772257e-05, -0.00039402826223522425, -4.410734163684538e-06, -9.83428253675811e-05, -0.00025829317746683955, -0.007803904823958874, -0.00030894274823367596, -1.1920928244535389e-07, -0.3351997137069702, -0.005409840028733015], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003033416287507862, -9.536738616588991e-07, 0.0, -0.022212404757738113, -0.4935497045516968, -5.8412379075889476e-06, -2.0265558760002023e-06, -0.0014044907875359058, -0.002369812224060297, 0.0, -1.1086402082582936e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.07081931829452515, -0.024796906858682632], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.loc[:, \"Text\"].values)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "loc", "[:", ",", " \"", "Text", "\"", "]", ".", "values", ")", "\n"], "logprobs": [-0.03638980910181999, -3.6954811548639555e-06, -0.0005988473421894014, -0.011725435964763165, -0.0007169060409069061, -0.00021908267808612436, -0.005237785633653402, -0.012517458759248257, -2.4248554706573486, -3.050469160079956, -0.015827033668756485, -0.013872513547539711, -0.10475216060876846, -0.000263894529780373, -6.23445157543756e-05, -1.8555710315704346, -0.0016822961624711752, -0.17062388360500336, -0.0923788845539093, -0.08130959421396255], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.003438277170062065, -2.264974000354414e-06, -5.125986263010418e-06, -9.643566590966657e-05, -0.002041757572442293, -4.410646579344757e-05, -5.686121585313231e-05, -0.0005578630371019244, -0.00040070133400149643, -0.0008450751192867756, -0.02000465989112854], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.030021848157048225, -0.008530250750482082, -0.007234093267470598, -0.06038707122206688, -0.00028701478731818497, -0.0009305914281867445, -6.568216485902667e-05, -0.008418785408139229, -0.00017653337272349745, -0.06028876081109047, -0.05856458470225334, -0.4529182016849518, -0.004421577323228121, -0.00012540031457319856, -0.008525286801159382, -0.002131691901013255, -0.010608706623315811, -1.1920928244535389e-07, -0.00015758226800244302, -1.2397689715726301e-05, -0.0010793342953547835, -2.3841830625315197e-06, -7.414542778860778e-05, -4.529942543740617e-06, -2.8490614567999728e-05, -0.004296477418392897, -0.0017669078661128879], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.016555434092879295, -0.0042457920499145985, -0.014761394821107388, -0.00013481661153491586, -0.0013328249333426356, -0.00038675934774801135, -0.02013481967151165, -0.00020752183627337217, -0.0367262065410614, -0.05064771696925163, -0.5265461802482605], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.037585943937301636, -0.0004866131057497114, -0.0010070496937260032, -1.4662635294371285e-05, -5.960462772236497e-07, -0.0002848696312867105, -0.005664369091391563, -8.415821503149346e-05, -0.0014453213661909103, -0.003662784118205309, -0.00012003655137959868, -0.012922355905175209, -6.353653589030728e-05, -0.00018988236843142658, -0.0036701480858027935, -1.2993727978027891e-05, -0.00010752100206445903, -1.156323378381785e-05, -0.003964779432862997, -0.031823039054870605, -0.20945905148983002], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.18694257736206055, -5.960462772236497e-07, -7.152555099310121e-07, -0.057583052664995193, -0.0015242397785186768, -0.0021709464490413666, -0.0024295118637382984, -0.003839859738945961], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-0.007129939738661051, -1.811964830267243e-05, -0.004767123609781265, -0.07694552093744278, -1.8929096460342407], "finish_reason": "stop"}, {"text": " (", "tokens": [" (", "\n"], "logprobs": [-2.669886589050293, -0.004510348662734032], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03134901449084282, -1.1801649634435307e-05, -2.4199192921514623e-05, -0.77496737241745, -0.004730462562292814], "finish_reason": "stop"}, {"text": "savefig(save_path, bbox_inches=\"tight\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ")", "\n"], "logprobs": [-0.00865234062075615, -0.0004450284468475729, -6.651657167822123e-05, -0.0014104428701102734, -7.867782187531702e-06, -2.50339189733495e-06, -1.1257565021514893, -0.5970304012298584, -8.821448318485636e-06, -0.00018106251081917435, -2.0265373677830212e-05, -3.9934315282152966e-05, -0.003587122540920973, -0.0009885906474664807, -7.152555099310121e-07, -1.6927575416048057e-05, -0.11715860664844513, -0.002237675478681922], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.04424440115690231, -0.09955251961946487, -0.032118380069732666], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.016496455296874046, -0.050390783697366714], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.04226012900471687, -3.182837463100441e-05, -4.768360213347478e-06, -0.3312385380268097, -0.38431471586227417, -0.0011706650257110596, -0.004986943677067757], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File '{file_path}' does not exist.\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " '", "{", "file", "_", "path", "}'", " does", " not", " exist", ".", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.05458672717213631, -0.00012563870404846966, -2.2411095415009186e-05, -0.0042399754747748375, -0.18982186913490295, -0.059307459741830826, -0.44402363896369934, -1.9078395366668701, -0.00041797960875555873, -0.23326556384563446, -0.0003281293320469558, -8.284702198579907e-05, -0.007586714345961809, -2.6385436058044434, -0.0009317824151366949, -0.00792419072240591, -0.24973417818546295, -0.003746634814888239, -0.0017015753546729684, -0.04973449185490608, -0.000545472139492631, -0.007488276809453964], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.004908298142254353, -0.022130554541945457, -0.009407240897417068, -0.0011117237154394388, -0.016462448984384537], "finish_reason": "stop"}, {"text": "f\"An error occurred: {e}\")", "tokens": ["f", "\"", "An", " error", " occurred", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.8218232989311218, -0.0014865073608234525, -0.3888649642467499, -0.07552390545606613, -0.0255690049380064, -0.2917437255382538, -0.0017485818825662136, -0.32218584418296814, -0.011402674950659275, -0.06518656015396118, -0.04420870169997215], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.0044092340394854546, -0.366624116897583], "finish_reason": "stop"}], "metrics": {"energy": 1153.9622707204817, "energy_per_token": 5.574696960002327, "time": 30.053751, "time_per_token": 0.14518720289855072}}
{"outputs": [{"text": " pd.read_csv(file_path, usecols=['Text'])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " use", "cols", "=['", "Text", "'])", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -1.3113775253295898, -1.1394150257110596, -0.0001394651480950415, -0.7570303082466125, -0.0771314948797226, -0.25141704082489014, -0.025707503780722618], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0002475670480635017, -8.344646857949556e-07, 0.0, -0.021683527156710625, -0.3723747730255127, -4.172316494077677e-06, -2.50339189733495e-06, -0.0013260389678180218, -0.0018917298875749111, 0.0, -1.0490362910786644e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.08720720559358597, -0.02087736874818802], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.056222546845674515, -4.410734163684538e-06, -0.00046230596490204334, -0.014868397265672684, -0.001176023157313466, -0.0003513672563713044, -0.005676933564245701, -0.01328238844871521, -0.1510406881570816, -0.00022182388056535274, -0.00010275312524754554, -6.556489552167477e-06, -0.07200771570205688, -0.09292012453079224], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0027132148388773203, -2.622600959512056e-06, -5.602820692729438e-06, -9.929640509653836e-05, -0.0016023189527913928, -6.305972783593461e-05, -5.125868119648658e-05, -0.00038223114097490907, -0.00043883229955099523, -0.0008139872807078063, -0.027737844735383987], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.042856134474277496, -0.007969368249177933, -0.007348767947405577, -0.09377740323543549, -0.00030250742565840483, -0.0008887869771569967, -7.092700980138034e-05, -0.00934783834964037, -0.00020203932945150882, -0.044889941811561584, -0.07361863553524017, -0.42553117871284485, -0.004869269207119942, -0.0001461399078834802, -0.009596997871994972, -0.0035271355882287025, -0.016070831567049026, -2.3841855067985307e-07, -0.0001486429391661659, -1.7881233361549675e-05, -0.0007823029300197959, -2.0265558760002023e-06, -8.022463589441031e-05, -5.245195097813848e-06, -2.3841574147809297e-05, -0.004599349107593298, -0.0018204560037702322], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.011630939319729805, -0.007829926908016205, -0.03286442905664444, -0.00012230125139467418, -0.0009154658182524145, -0.00018225439998786896, -0.01957830600440502, -0.0003587556129787117, -0.02082996815443039, -0.022825367748737335, -0.8783473372459412, -0.9179384708404541, -2.6464111215318553e-05, -0.005074004177004099, -0.000436925794929266, -0.0022953611332923174, -9.011816291604191e-05, -0.0024807651061564684, -1.1920928244535389e-07, -6.079655122448457e-06, -0.0006582954083569348, -2.3841855067985307e-07, -6.305972783593461e-05, 0.0, -2.50339189733495e-06, -0.42666780948638916, -0.001572206849232316], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.049313366413116455, -0.0005845506675541401, -0.0010912423022091389, -1.1801649634435307e-05, -1.0728830375228426e-06, -0.0002557904226705432, -0.006481222342699766, -6.997340824455023e-05, -0.0015606615925207734, -0.0046440837904810905, -0.00013553177996072918, -0.014385444112122059, -6.961580220377073e-05, -0.00022599527437705547, -0.005144928116351366, -1.2159273865108844e-05, -0.00010871296399272978, -1.645074735279195e-05, -0.003868834814056754, -0.04184087738394737, -0.2956714332103729], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.22318494319915771, -5.960462772236497e-07, -7.152555099310121e-07, -0.052408210933208466, -0.0016202905680984259, -0.00188137823715806, -0.002551040844991803, -0.0036820252425968647], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.007023881655186415, -1.6093124941107817e-05, -0.0034935176372528076, -0.08546407520771027, -0.5374536514282227, -1.0728830375228426e-06, -9.417489309271332e-06, -0.030642643570899963, -0.004536456428468227, -0.010874297469854355, -0.0008831891464069486, -0.4734048545360565, -0.0010352734243497252, -3.302042750874534e-05, -0.00014804698002990335, -1.3381707668304443, -0.04584221541881561, -0.007216222118586302, -0.059329479932785034], "finish_reason": "stop"}, {"text": " df_top.plot(x=\"Word\", y=\"Count\", kind=\"bar\", legend=False)", "tokens": [" df", "_", "top", ".", "plot", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " kind", "=", "\"", "bar", "\"", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.1256902813911438, -0.00015400654228869826, -1.0132738680113107e-05, -0.10415971279144287, -0.012803140096366405, -0.4060553312301636, -1.0180343389511108, -0.0005040090763941407, -0.0004223884898237884, -0.0007550249574705958, -2.9802276912960224e-06, -5.328513361746445e-05, -0.004554375074803829, -0.0030246484093368053, -1.6689160474925302e-05, -0.00011014331539627165, -1.7523612768854946e-05, -0.0013733012601733208, -0.011564127169549465, -0.00014840454969089478, -5.447716102935374e-05, -0.00025960413040593266, -0.1572834551334381, -0.1540534645318985, -0.7372294068336487, -0.00028725311858579516, -0.11696243286132812, -0.40222588181495667, -0.0056693474762141705], "finish_reason": "stop"}, {"text": " save_path is None:", "tokens": [" save", "_", "path", " is", " None", ":", "\n"], "logprobs": [-0.03091360069811344, -1.1205610462639015e-05, -2.0265373677830212e-05, -0.6021664142608643, -1.005351185798645, -0.00044371772673912346, -0.010223757475614548], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=300)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "3", "0", "0", ")", "\n"], "logprobs": [-0.008323858492076397, -0.0004266782198101282, -6.460934673668817e-05, -0.0015730400336906314, -8.821448318485636e-06, -2.861018856492592e-06, -1.2247910499572754, -0.9346672892570496, -1.0013530300057027e-05, -0.0006868863711133599, -0.25639772415161133, -0.0033246977254748344, -8.439661905867979e-05, -0.3539468050003052, -0.009031515568494797], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.04037610813975334, -0.10612234473228455, -0.036888450384140015], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.012069056741893291, -0.04780178144574165], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.04917798563838005, -3.0874729418428615e-05, -3.933898824470816e-06, -0.36313676834106445, -0.3624539375305176, -0.0011208925861865282, -0.0053639537654817104], "finish_reason": "stop"}, {"text": " FileNotFoundError(", "tokens": [" File", "NotFound", "Error", "(", "\n"], "logprobs": [-0.061272572726011276, -9.798523387871683e-05, -1.5020257706055418e-05, -0.005139946937561035, -2.1752843856811523], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.004642422776669264, -0.019425859674811363, -0.00862291269004345, -0.0010288427583873272, -0.012997784651815891], "finish_reason": "stop"}, {"text": "f\"An error occurred: {e}\")", "tokens": ["f", "\"", "An", " error", " occurred", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.8388675451278687, -0.0021967582870274782, -0.3377372622489929, -0.06896958500146866, -0.02082950249314308, -0.34000375866889954, -0.0019319456769153476, -0.36576399207115173, -0.01431305706501007, -0.06869783997535706, -0.04543471336364746], "finish_reason": "stop"}, {"text": " None", "tokens": [" None"], "logprobs": [-0.004817545413970947], "finish_reason": "stop"}], "metrics": {"energy": 1259.5704830722802, "energy_per_token": 5.162174110951968, "time": 33.012356, "time_per_token": 0.13529654098360655}}
{"outputs": [{"text": " pd.read_csv(file_path, usecols=[0])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " use", "cols", "=[", "0", "])", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -1.3113775253295898, -1.1394150257110596, -0.0001394651480950415, -0.7150304913520813, -0.9823880791664124, -0.2514266073703766, -0.03908311948180199], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.00032276666024699807, -9.536738616588991e-07, 0.0, -0.01778465136885643, -0.30579182505607605, -6.079655122448457e-06, -1.6689286894688848e-06, -0.0012911563972011209, -0.0019757291302084923, 0.0, -1.3828182090946939e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.08639862388372421, -0.02197103388607502], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.05961074307560921, -6.9141146923357155e-06, -0.0004727914638351649, -0.019924147054553032, -0.001176023157313466, -0.0002461368858348578, -0.005957228597253561, -0.012572906911373138, -0.1467260718345642, -0.00019238528329879045, -0.00010239553375868127, -1.2278481335670222e-05, -0.0710267499089241, -0.1034153550863266], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.003197559155523777, -2.9802276912960224e-06, -5.006777428206988e-06, -0.00013004888023715466, -0.0019731116481125355, -6.23445157543756e-05, -4.005352093372494e-05, -0.0004397855664137751, -0.0004291805380489677, -0.0006368515896610916, -0.02152801863849163], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx])", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.03812346234917641, -0.0068888068199157715, -0.008875326253473759, -0.10070501267910004, -0.0003554189461283386, -0.0008592489175498486, -4.5536911784438416e-05, -0.008837515488266945, -0.00021789084712509066, -0.03312890604138374, -0.11046154052019119, -1.059768795967102], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.00939023494720459, -0.008250322192907333, -0.026794161647558212, -0.00014351768186315894, -0.001212814822793007, -0.00020919041708111763, -0.017354656010866165, -0.00033909291960299015, -0.028846390545368195, -0.035023968666791916, -0.8508056998252869, -0.7476301193237305, -0.00014768941036891192, -0.002742817159742117, -0.0006009918288327754, -0.004907230380922556, -1.1920928244535389e-07, -1.645074735279195e-05, -2.8132995794294402e-05, -0.0007701530121266842, -1.9073468138230965e-06, -2.634490556374658e-05, -3.4570634852570947e-06, -1.7404405298293568e-05, -0.005060245748609304], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.031802136451005936, -0.0006554362480528653, -0.0010181248653680086, -1.168244216387393e-05, -4.768370445162873e-07, -0.00024232311989180744, -0.007035837508738041, -6.48477507638745e-05, -0.0013469918631017208, -0.0033489353954792023, -0.00010489867418073118, -0.013025672174990177, -6.663577369181439e-05, -0.00021646064124070108, -0.0037212190218269825, -1.0132738680113107e-05, -0.0001392267586197704, -1.5616295058862306e-05, -0.003853991162031889, -0.030961334705352783, -1.4245214462280273, -0.004146312829107046, -0.002048181602731347, -0.0006642519147135317, -0.0032740819733589888], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.21094298362731934, -9.536738616588991e-07, -5.960462772236497e-07, -0.051923926919698715, -0.0017360866768285632, -0.001979655120521784, -0.0030677898321300745, -0.0035473296884447336], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.006288384553045034, -1.3232143828645349e-05, -0.004844593815505505, -0.09781230241060257, -0.4668854773044586, -1.0728830375228426e-06, -6.198863957251888e-06, -0.0317455492913723, -0.0050806463696062565, -0.011391478590667248, -0.0009756095823831856, -0.5562880039215088, -0.0008043391280807555, -2.4437606043647975e-05, -0.00013255194062367082, -0.45808476209640503, -0.08959700912237167, -0.0011859057703986764, -0.007730212062597275, -0.03900802507996559], "finish_reason": "stop"}, {"text": " df_top.plot(", "tokens": [" df", "_", "top", ".", "plot", "(", "\n"], "logprobs": [-0.09849635511636734, -0.00011646069469861686, -1.2040065485052764e-05, -0.12189729511737823, -0.009251818060874939, -0.44374656677246094, -0.570319414138794], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.030842747539281845, -1.0490362910786644e-05, -2.4676019165781327e-05, -0.6950979828834534, -0.004673984833061695], "finish_reason": "stop"}, {"text": "savefig(save_path, bbox_inches=\"tight\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ")", "\n"], "logprobs": [-0.008175007067620754, -0.0007491880678571761, -7.211902266135439e-05, -0.0010439666220918298, -7.152531907195225e-06, -2.7418097943154862e-06, -1.1270636320114136, -0.6480143666267395, -1.0132738680113107e-05, -0.00019226610311307013, -3.755022044060752e-05, -4.577531944960356e-05, -0.0030651751440018415, -0.0013043713988736272, -5.960462772236497e-07, -1.8715683836489916e-05, -0.1021343544125557, -0.0032083725091069937], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.029644301161170006, -0.08309289067983627, -0.03327351063489914], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.014910443685948849, -0.048087332397699356], "finish_reason": "stop"}, {"text": " FileNotFoundError:", "tokens": [" File", "NotFound", "Error", ":", "\n"], "logprobs": [-0.05001472681760788, -3.802703940891661e-05, -3.6954811548639555e-06, -1.2374175786972046, -0.003568473504856229], "finish_reason": "stop"}, {"text": " FileNotFoundError(", "tokens": [" File", "NotFound", "Error", "(", "\n"], "logprobs": [-0.048220932483673096, -0.00011002412065863609, -1.7762025890988298e-05, -0.005506347864866257, -2.181076765060425], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.004143225960433483, -0.016290878877043724, -0.010990087874233723, -0.001179595128633082, -0.012502389959990978], "finish_reason": "stop"}, {"text": "e)", "tokens": ["e", ")", "\n"], "logprobs": [-2.4336373805999756, -0.023817269131541252, -0.059166233986616135], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.0044942088425159454, -0.3199703097343445], "finish_reason": "stop"}], "metrics": {"energy": 1101.2224793510431, "energy_per_token": 5.4515964324309065, "time": 29.80566, "time_per_token": 0.1475527722772277}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0012635351158678532, -3.635817120084539e-05, -0.00030179237364791334, -2.3841830625315197e-06, -8.761498611420393e-05, -0.0003165697562508285, -0.007341667544096708, -0.00023445718397852033, -1.1920928244535389e-07, -0.3689063787460327, -0.005357313901185989], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.00035470392322167754, -1.0728830375228426e-06, 0.0, -0.022356268018484116, -0.3347424566745758, -5.722029527532868e-06, -2.3841830625315197e-06, -0.0015200738562271, -0.002044017892330885, 0.0, -1.168244216387393e-05, -2.3841855067985307e-07, 0.0, -2.3841855067985307e-07, -0.08013968169689178, -0.022979043424129486], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.Text)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "Text", ")", "\n"], "logprobs": [-0.04974549263715744, -3.576272320060525e-06, -0.0004407388041727245, -0.017527567222714424, -0.000990496133454144, -0.00020489977032411844, -0.008006620220839977, -0.011959515511989594, -2.4434056282043457, -0.12359002232551575, -0.10117550939321518, -0.12075471878051758], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.004050504416227341, -2.861018856492592e-06, -5.8412379075889476e-06, -0.00013660451804753393, -0.0028651398606598377, -6.4490144723095e-05, -5.030505417380482e-05, -0.0005153281381353736, -0.0006547214579768479, -0.000617432699073106, -0.018620336428284645], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.025988280773162842, -0.007757536601275206, -0.009764408692717552, -0.09950762242078781, -0.0004948345012962818, -0.0007401349139399827, -6.353653589030728e-05, -0.013697675429284573, -0.00016592556494288146, -0.04485791549086571, -0.06468194723129272, -0.405522882938385, -0.0034848456270992756, -9.989239333663136e-05, -0.011486350558698177, -0.0030618475284427404, -0.010731486603617668, -2.3841855067985307e-07, -0.0001401803019689396, -1.6927575416048057e-05, -0.0007213136414065957, -2.50339189733495e-06, -7.319182623177767e-05, -7.033323527139146e-06, -3.9934315282152966e-05, -0.0049392590299248695, -0.0017767847748473287], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.025407472625374794, -0.006842277944087982, -0.02831347845494747, -0.00015293381875380874, -0.00134151556994766, -0.0004568248405121267, -0.038995757699012756, -0.0003182381624355912, -0.03467122092843056, -0.025527868419885635, -0.8892667293548584, -0.6386250257492065, -0.000163541772053577, -0.0023280680179595947, -0.0005031750188209116, -0.0038713286630809307, -1.1920928244535389e-07, -1.9550132492440753e-05, -2.312633478140924e-05, -0.0005507144378498197, -1.5497195136049413e-06, -4.1960789531003684e-05, -2.9802276912960224e-06, -1.5020257706055418e-05, -0.004991451278328896], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.04033420607447624, -0.0005349873681552708, -0.0012225781101733446, -1.3112935448589269e-05, -7.152555099310121e-07, -0.00027843413408845663, -0.008123807609081268, -7.64102369430475e-05, -0.0019209994934499264, -0.003653638530522585, -0.00015925093612167984, -0.015691030770540237, -9.297892393078655e-05, -0.00020716428116429597, -0.005458569619804621, -1.0371154530730564e-05, -0.00011014331539627165, -1.645074735279195e-05, -0.003891990752890706, -0.03961639478802681, -1.404030442237854, -0.004239500500261784, -0.002162262797355652, -0.0006039702566340566, -0.002927900291979313], "finish_reason": "stop"}, {"text": " [word for (word, count) in words_freq[:10]]", "tokens": [" [", "word", " for", " (", "word", ",", " count", ")", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-1.8370531797409058, -0.5788435935974121, -0.7185369729995728, -0.8998256921768188, -5.864924969500862e-05, -5.495397272170521e-05, -0.9886060953140259, -0.0002843929105438292, -8.523101132595912e-05, -0.0010367024224251509, -1.9073468138230965e-06, -4.768370445162873e-07, -0.2636304497718811, -0.001555662602186203, -0.0022819212172180414, -0.005437465384602547, -0.002989706117659807], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-0.008071187883615494, -1.5616295058862306e-05, -0.005016241688281298, -0.06762360036373138, -1.433557152748108], "finish_reason": "stop"}, {"text": " df_top.plot(", "tokens": [" df", "_", "top", ".", "plot", "(", "\n"], "logprobs": [-0.11262138187885284, -0.00015567521040793508, -2.3007127310847864e-05, -0.09444523602724075, -0.010169949382543564, -0.39322566986083984, -0.6227954030036926], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03192257136106491, -1.0967194612021558e-05, -2.3245540432981215e-05, -0.6998090744018555, -0.0047227502800524235], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.008706112392246723, -0.0003375437227077782, -8.165503095369786e-05, -0.001255558105185628, -6.794906312279636e-06, -3.2186455882765586e-06, -0.393210232257843, -0.0019802500028163195], "finish_reason": "stop"}, {"text": "close(ax.figure)", "tokens": ["close", "(", "ax", ".", "figure", ")", "\n"], "logprobs": [-0.03265700116753578, -2.1864888668060303, -0.0769561156630516, -0.06474810093641281, -0.04527977854013443, -0.0014990055933594704, -0.04728911444544792], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.01595187373459339, -0.04310096427798271], "finish_reason": "stop"}, {"text": " FileNotFoundError:", "tokens": [" File", "NotFound", "Error", ":", "\n"], "logprobs": [-0.05615301430225372, -4.756337511935271e-05, -3.576272320060525e-06, -1.1418969631195068, -0.004066176246851683], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File {file_path} was not found\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " {", "file", "_", "path", "}", " was", " not", " found", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.04980889707803726, -0.00011681827891152352, -2.2053474822314456e-05, -0.005164140835404396, -0.1913306713104248, -0.056013818830251694, -0.41980668902397156, -1.9369850158691406, -0.20910127460956573, -0.0006899837171658874, -3.111314072157256e-05, -0.013759642839431763, -2.1997733116149902, -0.0009807306341826916, -0.004191423766314983, -0.8615144491195679, -0.006151670124381781, -0.055534664541482925, -0.0010094314347952604, -0.008491836488246918], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005500301718711853, -0.017080584540963173, -0.009723915718495846, -0.0011179156135767698, -0.01535499095916748], "finish_reason": "stop"}, {"text": "f\"Error occurred: {e}\")", "tokens": ["f", "\"", "Error", " occurred", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.8669435977935791, -0.0016061275964602828, -1.457124948501587, -1.0091545581817627, -0.487263023853302, -0.002728075487539172, -0.36957311630249023, -0.01612209342420101, -0.0379456989467144, -0.040958017110824585], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004455164074897766, -0.340632826089859], "finish_reason": "stop"}], "metrics": {"energy": 1372.4353625879282, "energy_per_token": 6.238342557217855, "time": 35.913628, "time_per_token": 0.16324376363636364}}
{"outputs": [{"text": " pd.read_csv(file_path, names=['Text'])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " names", "=['", "Text", "'])", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -1.3113775253295898, -3.4515316486358643, -0.9173890352249146, -0.07769251614809036, -0.2710687816143036, -0.03338996693491936], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0003149013500660658, -1.311301275563892e-06, 0.0, -0.018393076956272125, -1.593775987625122], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.03901100531220436, -5.245195097813848e-06, -0.0005038899253122509, -0.011835943907499313, -0.0009464313625358045, -0.00016604475968051702, -0.006224531680345535, -0.010714031755924225, -0.1494336724281311, -0.00025376438861712813, -0.00010442188795423135, -9.417489309271332e-06, -0.07489176094532013, -0.10443176329135895], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.004310127347707748, -3.099436753473128e-06, -6.794906312279636e-06, -0.00016592556494288146, -0.007000679615885019, -5.1377883210079744e-05, -5.817244164063595e-05, -0.0005610798834823072, -0.0004412154376041144, -0.0007742029847577214, -0.031055986881256104], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.0416354164481163, -0.010487798601388931, -0.009851998649537563, -0.07960931211709976, -0.0003978414461016655, -0.0008517451351508498, -0.00014149141497910023, -0.01126807276159525, -0.0001858300092862919, -0.05126207321882248, -0.09003648161888123, -0.32390227913856506, -0.0034196253400295973, -0.00011884459672728553, -0.010275202803313732, -0.0020354522857815027, -0.009521312080323696, -1.1920928244535389e-07, -0.00018761781393550336, -1.4305012882687151e-05, -0.0008334023877978325, -2.7418097943154862e-06, -0.00010466027742950246, -8.34461570775602e-06, -4.494089080253616e-05, -0.0052846260368824005, -0.0015692312736064196], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.013354378752410412, -0.006402101833373308, -0.0465410053730011, -0.00015245705435518175, -0.0012235306203365326, -0.00040725519647821784, -0.0201480221003294, -0.000226472009671852, -0.03526681289076805, -0.04027901589870453, -1.0846244096755981, -0.7345534563064575, -3.0397906812140718e-05, -0.006673548836261034, -0.0003743662964552641, -0.004657017532736063, -0.00010358751023886725, -0.0046955798752605915, -1.1920928244535389e-07, -9.179073458653875e-06, -0.001606365549378097, -2.3841855067985307e-07, -5.924526340095326e-05, 0.0, -3.4570634852570947e-06, -0.5389611124992371, -0.0019721598364412785], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.04791224002838135, -0.0006567466771230102, -0.0010643299901857972, -8.583032467868179e-06, -1.0728830375228426e-06, -0.0002646096108946949, -0.007099993526935577, -8.415821503149346e-05, -0.00193813256919384, -0.004558528773486614, -0.00014447122521232814, -0.02810904197394848, -0.00013529339048545808, -0.00026901919045485556, -0.005102113354951143, -1.3470558769768104e-05, -9.83428253675811e-05, -1.4424220353248529e-05, -0.0037448531948029995, -0.03850654140114784, -0.28250205516815186], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.31735527515411377, -4.768370445162873e-07, -3.576278118089249e-07, -0.0646943524479866, -0.0016021999763324857, -0.0019081495702266693, -0.0023401989601552486, -0.004273212049156427], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.006783077958971262, -1.9311717551317997e-05, -0.005156906321644783, -0.09159927815198898, -1.3438167572021484, -0.017107656225562096, -0.12592469155788422, -9.536738616588991e-07, -2.861018856492592e-06, -0.0399678610265255, -0.002648895373567939, -0.03229302912950516, -0.0007662221323698759, -0.6373786926269531, -0.0012284121476113796, -3.123234637314454e-05, -0.00018988236843142658, -0.8809686899185181, -0.044742558151483536, -0.018914105370640755, -0.04767244681715965], "finish_reason": "stop"}, {"text": " df_top.plot(x=\"Word\", y=\"Count\", kind=\"bar\", legend=False, rot=0)", "tokens": [" df", "_", "top", ".", "plot", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " kind", "=", "\"", "bar", "\"", ",", " legend", "=", "False", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.11318424344062805, -0.0001411338453181088, -4.410734163684538e-06, -0.11811750382184982, -0.018132284283638, -0.3381180763244629, -1.2891731262207031, -0.0004551566671580076, -0.00042417587246745825, -0.0007512131123803556, -3.099436753473128e-06, -4.124556289752945e-05, -0.00655962573364377, -0.002651748713105917, -2.1457441107486375e-05, -0.00012838016846217215, -1.8000440832111053e-05, -0.0011511372867971659, -0.012549126520752907, -0.00010787858627736568, -5.0424259825376794e-05, -0.0002640137099660933, -0.13635648787021637, -0.1716197431087494, -0.8603552579879761, -0.00025388356880284846, -0.1256130188703537, -1.0834002494812012, -0.6303811073303223, -0.008288509212434292, -1.224753737449646, -0.0577724389731884, -0.0038006706163287163], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.03116426430642605, -1.1801649634435307e-05, -1.9073304429184645e-05, -0.6266478300094604, -0.48507606983184814, -7.271740287251305e-06, -0.0030518644489347935, -0.003731314092874527], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=300)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "3", "0", "0", ")", "\n"], "logprobs": [-0.00980265624821186, -0.00040987672400660813, -7.486063259420916e-05, -0.0018556771101430058, -9.417489309271332e-06, -2.7418097943154862e-06, -1.0942126512527466, -0.7601745128631592, -8.4638240878121e-06, -0.0006412595394067466, -0.23579303920269012, -0.0031884093768894672, -8.284702198579907e-05, -0.3254950940608978, -0.008333434350788593], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.03852340206503868, -0.08001842349767685, -0.034389302134513855], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.013846883550286293, -0.040999095886945724], "finish_reason": "stop"}, {"text": " FileNotFoundError:", "tokens": [" File", "NotFound", "Error", ":", "\n"], "logprobs": [-0.043209660798311234, -3.671578815556131e-05, -3.814689989667386e-06, -1.2480038404464722, -0.0034911418333649635], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found: {str(exc)}\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", ":", " {", "str", "(", "exc", ")}", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.05523611977696419, -0.0001209901092806831, -1.811964830267243e-05, -0.004766767844557762, -0.24401268362998962, -0.060599151998758316, -0.388772189617157, -0.41721272468566895, -0.001958953682333231, -0.5520630478858948, -0.007388527970761061, -2.9393274784088135, -0.00018761781393550336, -0.01831173151731491, -0.029015008360147476, -0.010023732669651508, -0.004418135620653629, -0.46555963158607483, -0.02734481915831566, -0.011373092420399189], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.004872828256338835, -0.02000056952238083, -0.011568251997232437, -0.0012991330586373806, -0.014689739793539047], "finish_reason": "stop"}, {"text": "f\"Error occurred: {e}\")", "tokens": ["f", "\"", "Error", " occurred", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.6060162782669067, -0.0016236230731010437, -1.2167574167251587, -0.8745102882385254, -0.46542245149612427, -0.0028161650989204645, -0.441671222448349, -0.01945462077856064, -0.044732868671417236, -0.06417428702116013], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004654881544411182, -0.3464224934577942], "finish_reason": "stop"}], "metrics": {"energy": 1392.9252236242303, "energy_per_token": 5.527481046127898, "time": 35.50796, "time_per_token": 0.14090460317460315}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -0.3150692284107208, -0.0062665874138474464], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0002972637885250151, -8.344646857949556e-07, 0.0, -0.02093060314655304, -0.43427664041519165, -5.722029527532868e-06, -2.622600959512056e-06, -0.001531619462184608, -0.001795467222109437, 0.0, -1.0251946150674485e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.0786563903093338, -0.020777195692062378], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.04054668918251991, -4.0531076592742465e-06, -0.0004881620698142797, -0.016124792397022247, -0.0009304723353125155, -0.00011920218821614981, -0.00552021898329258, -0.016785690560936928, -0.14239194989204407, -0.00019393471302464604, -8.177422569133341e-05, -7.986990567587782e-06, -0.06767786294221878, -0.10158687829971313], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.003314004512503743, -2.3841830625315197e-06, -6.318072337307967e-06, -0.00011062010162277147, -0.0015348332235589623, -5.4834770708112046e-05, -4.911301948595792e-05, -0.0005168771021999419, -0.00038258862332440913, -0.0008448368753306568, -0.021385902538895607], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.03694164380431175, -0.010083921253681183, -0.009771727956831455, -0.09819004684686661, -0.00047708096099086106, -0.0011948358733206987, -8.535020606359467e-05, -0.012159035541117191, -0.0002932118659373373, -0.05406385660171509, -0.0565958246588707, -0.4327036738395691, -0.005565505009144545, -0.00013147920253686607, -0.0097267497330904, -0.003017042065039277, -0.016113178804516792, -1.1920928244535389e-07, -0.00015639036428183317, -1.3708974620385561e-05, -0.000929281348362565, -2.3841830625315197e-06, -9.047575440490618e-05, -7.390948667307384e-06, -3.6000557884108275e-05, -0.005747933406382799, -0.0017446548445150256], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.01303132064640522, -0.006835410837084055, -0.020870013162493706, -0.00017009719158522785, -0.0014832934830337763, -0.00019012074335478246, -0.029294626787304878, -0.00018499570433050394, -0.025849122554063797, -0.02563140168786049, -0.47617948055267334], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03321228176355362, -0.0004885195521637797, -0.000776347063947469, -1.1205610462639015e-05, -7.152555099310121e-07, -0.0002615109842736274, -0.008234006352722645, -8.129743218887597e-05, -0.0016776548000052571, -0.0036651596892625093, -0.00010680581908673048, -0.012563959695398808, -7.390703103737906e-05, -0.00018225439998786896, -0.004113784059882164, -1.0371154530730564e-05, -9.738924563862383e-05, -1.4543427823809907e-05, -0.003725138260051608, -0.031762294471263885, -0.25050678849220276], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.23267468810081482, -4.768370445162873e-07, -4.768370445162873e-07, -0.06688108295202255, -0.0016511153662577271, -0.0017353727016597986, -0.0023670769296586514, -0.0033858849201351404], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-0.00451580761000514, -1.5735502529423684e-05, -0.00532185984775424, -0.06722109019756317, -1.5832445621490479], "finish_reason": "stop"}, {"text": " df_top.plot.bar(", "tokens": [" df", "_", "top", ".", "plot", ".", "bar", "(", "\n"], "logprobs": [-0.12458066642284393, -0.00013279033009894192, -6.556489552167477e-06, -0.1261843740940094, -0.010762031190097332, -1.0724424123764038, -0.007601384539157152, -1.0115710496902466, -1.578237533569336], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.031729843467473984, -1.0967194612021558e-05, -1.883488948806189e-05, -0.7860667109489441, -0.00447225384414196], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.007554888725280762, -0.0004015354788862169, -8.070142939686775e-05, -0.0018330690218135715, -7.867782187531702e-06, -2.3841830625315197e-06, -0.43048393726348877, -0.002079706871882081], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.03610020875930786, -0.11288613080978394, -0.03036305494606495], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.017883367836475372, -0.04734448343515396], "finish_reason": "stop"}, {"text": " FileNotFoundError:", "tokens": [" File", "NotFound", "Error", ":", "\n"], "logprobs": [-0.054899007081985474, -2.6702524337451905e-05, -3.814689989667386e-06, -1.1091341972351074, -0.004125418607145548], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found at: {file_path}\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", " at", ":", " {", "file", "_", "path", "}\"", ")", " from", " exc", "\n"], "logprobs": [-0.06334483623504639, -0.00018010901112575084, -2.098061486321967e-05, -0.004214216023683548, -0.18959476053714752, -0.04658674821257591, -0.45075228810310364, -0.42420440912246704, -0.0024825488217175007, -1.0786967277526855, -1.8774356842041016, -0.013453769497573376, -0.566621720790863, -0.0004962643142789602, -6.878139538457617e-05, -0.0179736465215683, -0.0037576796021312475, -0.060893408954143524, -0.0004864939546678215, -0.007695551961660385], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.0047979699447751045, -0.017122656106948853, -0.011571433395147324, -0.001256034360267222, -0.016681237146258354], "finish_reason": "stop"}, {"text": "f\"An error occurred: {e}\")", "tokens": ["f", "\"", "An", " error", " occurred", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.7938539385795593, -0.001940036192536354, -0.3718966543674469, -0.0646698847413063, -0.025201745331287384, -0.3558036684989929, -0.001796657219529152, -0.5250134468078613, -0.013071090914309025, -0.07358762621879578, -0.049922872334718704], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004062970634549856, -0.33867090940475464], "finish_reason": "stop"}], "metrics": {"energy": 1092.2507642412188, "energy_per_token": 5.630158578562984, "time": 29.171835, "time_per_token": 0.15037028350515463}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0013150862650945783, -3.123234637314454e-05, -0.0002615109842736274, -3.2186455882765586e-06, -9.16677454370074e-05, -0.00025018901214934886, -0.007203558459877968, -0.00025340684805996716, -1.1920928244535389e-07, -0.3406352698802948, -0.005871311295777559], "finish_reason": "stop"}, {"text": " CountVectorizer(lowercase=True, stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "low", "erc", "ase", "=", "True", ",", " stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.00031513971043750644, -7.152555099310121e-07, 0.0, -0.020581839606165886, -3.3220701217651367, -0.0001436368766007945, -5.245195097813848e-06, -0.002880949294194579, -0.029063759371638298, -0.00886965449899435, -0.16275392472743988, -4.410734163684538e-06, -1.5497195136049413e-06, -0.001551615772768855, -0.008973744697868824, -2.3841855067985307e-07, -1.3828182090946939e-05, -7.152555099310121e-07, 0.0, -1.1920928244535389e-07, -0.08890935033559799, -0.01742108166217804], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.Text)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "Text", ")", "\n"], "logprobs": [-0.03802603483200073, -5.006777428206988e-06, -0.0004533693427219987, -0.012586208991706371, -0.0008355463505722582, -0.00019870213873218745, -0.005134017206728458, -0.010850477032363415, -2.5362350940704346, -0.13232825696468353, -0.0873384103178978, -0.09151855856180191], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0029526231810450554, -2.622600959512056e-06, -7.510157047363464e-06, -0.00010585224663373083, -0.0025909922551363707, -4.684815212385729e-05, -5.6503606174374e-05, -0.0004577780782710761, -0.00046826363541185856, -0.0007195267826318741, -0.023834729567170143], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx])", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.040010012686252594, -0.007467807270586491, -0.010782198049128056, -0.08712777495384216, -0.0003300360403954983, -0.0008356655016541481, -8.391981828026474e-05, -0.012095085345208645, -0.00031871485407464206, -0.04865250736474991, -0.031162993982434273, -1.0273220539093018], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.012193423695862293, -0.0057523190043866634, -0.029517685994505882, -0.00016091958968900144, -0.0015775627689436078, -0.00039176418795250356, -0.033876966685056686, -0.00027426297310739756, -0.02764299511909485, -0.027713032439351082, -0.8689572811126709, -0.6383247971534729, -0.0001463782973587513, -0.0015335239004343748, -0.0005061537376604974, -0.007052172906696796, -1.1920928244535389e-07, -1.728519782773219e-05, -2.706014311115723e-05, -0.0005760917556472123, -1.5497195136049413e-06, -3.0278701160568744e-05, -2.861018856492592e-06, -1.4185804502631072e-05, -0.005842038430273533], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.048772748559713364, -0.0005184260080568492, -0.0008852138998918235, -1.0847986231965479e-05, -5.960462772236497e-07, -0.0002739054325502366, -0.005200904794037342, -5.4596363042946905e-05, -0.001128989621065557, -0.003491854527965188, -0.00010847456724150106, -0.012325428426265717, -6.365573790390044e-05, -0.00020037073409184813, -0.004344786051660776, -1.0967194612021558e-05, -0.00012206286191940308, -1.3470558769768104e-05, -0.003585459664463997, -0.03457506746053696, -1.461055874824524, -0.003609334584325552, -0.002191167790442705, -0.0007482351502403617, -0.00256423931568861], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.18585774302482605, -4.768370445162873e-07, -4.768370445162873e-07, -0.04386841878294945, -0.0016796779818832874, -0.0017383478116244078, -0.0033248167019337416, -0.003317212453112006], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Words\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Words", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.004196884576231241, -1.3112935448589269e-05, -0.004119482357054949, -0.07810748368501663, -0.6622969508171082, -1.1920922133867862e-06, -8.4638240878121e-06, -0.02782944031059742, -0.004451247863471508, -0.01405554823577404, -0.0006943913758732378, -2.947383403778076, -0.0008474572678096592, -0.00021002470748499036, -0.0001833270798670128, -0.803286075592041, -0.22159376740455627, -0.002609897404909134, -0.007484964095056057, -0.027565298601984978], "finish_reason": "stop"}, {"text": " df_top.plot.barh(", "tokens": [" df", "_", "top", ".", "plot", ".", "bar", "h", "(", "\n"], "logprobs": [-0.1446629762649536, -0.00011836781777674332, -2.2649508537142538e-05, -0.1164284273982048, -0.014587417244911194, -1.059183120727539, -0.0110326511785388, -0.67128986120224, -0.003741527907550335, -1.539847493171692], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.03193596750497818, -9.894321920000948e-06, -2.6940935640595853e-05, -0.6731456518173218, -0.4383998513221741, -8.22540732769994e-06, -0.0024408092722296715, -0.0038756034336984158], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.00724154943600297, -0.0004962643142789602, -6.615896563744172e-05, -0.0014934111386537552, -9.179073458653875e-06, -4.0531076592742465e-06, -0.48490697145462036, -0.0017403707606717944], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.03668851777911186, -0.08982445299625397, -0.03335675969719887], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.01772714965045452, -0.04947267100214958], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.0427536815404892, -3.2305197237292305e-05, -3.4570634852570947e-06, -0.3284890949726105, -0.3441404402256012, -0.001053493469953537, -0.004808647558093071], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"The file path {exc.filename} does not exist\")", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "The", " file", " path", " {", "exc", ".", "filename", "}", " does", " not", " exist", "\"", ")", "\n"], "logprobs": [-0.058550529181957245, -0.0001102625101339072, -1.7881233361549675e-05, -0.004857643507421017, -0.220983624458313, -0.05357450246810913, -1.4893418550491333, -0.24463722109794617, -2.386096954345703, -0.6663790345191956, -1.2193061113357544, -0.3274340033531189, -0.01146596297621727, -0.005121089518070221, -0.877323567867279, -0.00042226933874189854, -0.004109628964215517, -1.575426459312439, -0.004990383516997099, -0.4698653519153595], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005022290628403425, -0.01741909049451351, -0.011681252159178257, -0.0012856797548010945, -0.01744486391544342], "finish_reason": "stop"}, {"text": "f\"An error occurred while executing the function: {e}\")", "tokens": ["f", "\"", "An", " error", " occurred", " while", " executing", " the", " function", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.7100607752799988, -0.0017733338754624128, -0.3452547788619995, -0.06666765362024307, -0.026522433385252953, -2.0083470344543457, -0.7062035799026489, -0.2701148986816406, -0.11661813408136368, -0.08972267061471939, -0.0047456491738557816, -0.3605009913444519, -0.008330951444804668, -0.037980932742357254, -0.0669935792684555], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004470829851925373, -0.3636539578437805], "finish_reason": "stop"}], "metrics": {"energy": 1210.7089305934915, "energy_per_token": 5.357119161918104, "time": 30.699291, "time_per_token": 0.13583757079646017}}
{"outputs": [{"text": " pd.read_csv(file_path, usecols=['Text'], names=['Text'])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " use", "cols", "=['", "Text", "']", ",", " names", "=['", "Text", "'])", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -1.3113775253295898, -1.1394150257110596, -0.0001394651480950415, -0.7570303082466125, -0.0771314948797226, -1.5617045164108276, -0.06036866828799248, -2.444084882736206, -0.04592032730579376, -0.03739269822835922, -0.5146707892417908, -0.028286129236221313], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0002475670480635017, -8.344646857949556e-07, 0.0, -0.021683527156710625, -0.3723747730255127, -4.172316494077677e-06, -2.50339189733495e-06, -0.0013260389678180218, -0.0018917298875749111, 0.0, -1.0490362910786644e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.08720720559358597, -0.02087736874818802], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.056222546845674515, -4.410734163684538e-06, -0.00046230596490204334, -0.014868397265672684, -0.001176023157313466, -0.0003513672563713044, -0.005676933564245701, -0.01328238844871521, -0.1510406881570816, -0.00022182388056535274, -0.00010275312524754554, -6.556489552167477e-06, -0.07200771570205688, -0.09292012453079224], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0027132148388773203, -2.622600959512056e-06, -5.602820692729438e-06, -9.929640509653836e-05, -0.0016023189527913928, -6.305972783593461e-05, -5.125868119648658e-05, -0.00038223114097490907, -0.00043883229955099523, -0.0008139872807078063, -0.027737844735383987], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx])", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.042856134474277496, -0.007969368249177933, -0.007348767947405577, -0.09377740323543549, -0.00030250742565840483, -0.0008887869771569967, -7.092700980138034e-05, -0.00934783834964037, -0.00020203932945150882, -0.044889941811561584, -0.07361863553524017, -1.0735032558441162], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names_out())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "())", "\n"], "logprobs": [-0.011630939319729805, -0.007829926908016205, -0.03286442905664444, -0.00012230125139467418, -0.0009154658182524145, -0.00018225439998786896, -0.01957830600440502, -0.0003587556129787117, -0.02082996815443039, -0.022825367748737335, -0.8783473372459412, -0.9179384708404541, -2.6464111215318553e-05, -0.005074004177004099, -0.000436925794929266, -0.0022953611332923174, -9.011816291604191e-05, -0.0024807651061564684, -1.1920928244535389e-07, -6.079655122448457e-06, -0.0006582954083569348, -2.3841855067985307e-07, -6.305972783593461e-05, 0.0, -2.50339189733495e-06, -1.0938698053359985, -0.002249212935566902, -0.0031116423197090626, -0.005146469920873642], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.049313366413116455, -0.0005845506675541401, -0.0010912423022091389, -1.1801649634435307e-05, -1.0728830375228426e-06, -0.0002557904226705432, -0.006481222342699766, -6.997340824455023e-05, -0.0015606615925207734, -0.0046440837904810905, -0.00013553177996072918, -0.014385444112122059, -6.961580220377073e-05, -0.00022599527437705547, -0.005144928116351366, -1.2159273865108844e-05, -0.00010871296399272978, -1.645074735279195e-05, -0.003868834814056754, -0.04184087738394737, -1.3674297332763672, -0.004122925456613302, -0.00212479243054986, -0.0007448997348546982, -0.0028389885555952787], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.22318494319915771, -5.960462772236497e-07, -7.152555099310121e-07, -0.052408210933208466, -0.0016202905680984259, -0.00188137823715806, -0.002551040844991803, -0.0036820252425968647], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-0.007023881655186415, -1.6093124941107817e-05, -0.0034935176372528076, -0.08546407520771027, -1.5174341201782227], "finish_reason": "stop"}, {"text": " df_top.plot.barh(x=\"Word\", y=\"Count\", figsize=(10, 5), legend=False)", "tokens": [" df", "_", "top", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " fig", "size", "=(", "1", "0", ",", " ", "5", ")", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.1392061412334442, -0.00010644822759786621, -1.5139465176616795e-05, -0.11367231607437134, -0.015341021120548248, -1.058459997177124, -0.009085385128855705, -0.5344895720481873, -0.0034806879702955484, -0.3964732885360718, -0.0004328744253143668, -0.00026651646476238966, -0.0020021409727633, -3.814689989667386e-06, -0.0015550674870610237, -0.049926843494176865, -0.0006843847222626209, -1.168244216387393e-05, -0.00010930894495686516, -5.602820692729438e-06, -0.2988651394844055, -1.3453574180603027, -3.290122185717337e-05, -0.011460541747510433, -0.4271664023399353, -0.12497599422931671, -0.0007272697403095663, -0.0017957051750272512, -0.6714990139007568, -0.16891123354434967, -0.00013886917440686375, -1.0653159618377686, -0.0002494739310350269, -0.07708513736724854, -0.0514739453792572, -0.001958834705874324], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.03133688494563103, -1.1086402082582936e-05, -1.8954096958623268e-05, -0.6590904593467712, -0.45199745893478394, -7.271740287251305e-06, -0.0026562665589153767, -0.0032511495519429445], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.007212316617369652, -0.0004724340105894953, -7.009260298218578e-05, -0.0021567910443991423, -7.390948667307384e-06, -2.264974000354414e-06, -0.3999820649623871, -0.0020214139949530363], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.041029758751392365, -0.09072750061750412, -0.0336439274251461], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.019447488710284233, -0.046321142464876175], "finish_reason": "stop"}, {"text": " FileNotFoundError:", "tokens": [" File", "NotFound", "Error", ":", "\n"], "logprobs": [-0.04574565216898918, -4.005352093372494e-05, -3.814689989667386e-06, -1.210243821144104, -0.0036266762763261795], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found: {exc}\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", ":", " {", "exc", "}\"", ")", " from", " exc", "\n"], "logprobs": [-0.06538490206003189, -0.00010787858627736568, -1.823885577323381e-05, -0.004493259359151125, -0.1806451976299286, -0.056796323508024216, -0.4346509873867035, -0.5135044455528259, -0.002260274486616254, -0.5023706555366516, -0.008273021318018436, -1.0666608810424805, -1.753261923789978, -0.003812071168795228, -0.4771399199962616, -0.043049585074186325, -0.01762373186647892], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.004620589315891266, -0.015235008671879768, -0.011920525692403316, -0.0012525817146524787, -0.015604302287101746], "finish_reason": "stop"}, {"text": "\"An error occurred while executing the function:\", str(e))", "tokens": ["\"", "An", " error", " occurred", " while", " executing", " the", " function", ":\"", ",", " str", "(", "e", "))", "\n"], "logprobs": [-0.7729399800300598, -0.39868804812431335, -0.08435522764921188, -0.014939568936824799, -1.663496732711792, -0.42387276887893677, -0.1347741186618805, -0.1095532774925232, -0.17775371670722961, -0.11289624869823456, -0.9920425415039062, -8.976056415122002e-05, -0.0006225554971024394, -0.004952781833708286, -0.038130808621644974], "finish_reason": "stop"}, {"text": " None", "tokens": [" None"], "logprobs": [-0.004623081535100937], "finish_reason": "stop"}], "metrics": {"energy": 1275.8921221599574, "energy_per_token": 5.294158183236338, "time": 34.359135, "time_per_token": 0.14256902489626558}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0013150862650945783, -3.123234637314454e-05, -0.0002615109842736274, -3.2186455882765586e-06, -9.16677454370074e-05, -0.00025018901214934886, -0.007203558459877968, -0.00025340684805996716, -1.1920928244535389e-07, -0.3406352698802948, -0.005871311295777559], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.00031513971043750644, -7.152555099310121e-07, 0.0, -0.020581839606165886, -0.3669080436229706, -3.576272320060525e-06, -1.9073468138230965e-06, -0.0017615529941394925, -0.0017888034926727414, 0.0, -9.775113539944869e-06, -2.3841855067985307e-07, 0.0, 0.0, -0.09418249875307083, -0.01776966080069542], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.047026537358760834, -4.6491513785440475e-06, -0.0005260513862594962, -0.013224156573414803, -0.0009834696538746357, -0.00014411364099942148, -0.006750279571861029, -0.014151225797832012, -0.14669787883758545, -0.00016830935783218592, -9.798523387871683e-05, -9.179073458653875e-06, -0.07927162200212479, -0.09820539504289627], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.003460017265751958, -2.50339189733495e-06, -5.483612312673358e-06, -0.00010918975021922961, -0.002976632211357355, -5.1616290875244886e-05, -5.638440416078083e-05, -0.0005127069307491183, -0.0004690977220889181, -0.0008061258122324944, -0.020685536786913872], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.034603051841259, -0.007916976697742939, -0.010579925961792469, -0.061468660831451416, -0.000446696620201692, -0.0007874249131418765, -6.806619057897478e-05, -0.010268830694258213, -0.0001854724541772157, -0.04177296534180641, -0.08983121067285538, -0.3701256215572357, -0.005074359942227602, -0.00010644822759786621, -0.00825091265141964, -0.002277877414599061, -0.015078962780535221, -1.1920928244535389e-07, -0.00016175392374861985, -1.5258672647178173e-05, -0.0006914132391102612, -2.0265558760002023e-06, -6.937739817658439e-05, -5.960446742392378e-06, -3.313963316031732e-05, -0.0043703047558665276, -0.001642784452997148], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.01665814220905304, -0.00524098752066493, -0.019540781155228615, -0.00013052565918769687, -0.001277702976949513, -0.00018487652414478362, -0.017204564064741135, -0.00020132421923335642, -0.027975277975201607, -0.02828114666044712, -0.8078472018241882, -0.7814541459083557, -2.098061486321967e-05, -0.004925142508000135, -0.0003810394846368581, -0.0024519874714314938, -8.594620157964528e-05, -0.002978414995595813, -1.1920928244535389e-07, -6.6756979322235566e-06, -0.000648883986286819, -1.1920928244535389e-07, -6.23445157543756e-05, 0.0, -2.50339189733495e-06, -0.4231163561344147, -0.0015330478781834245], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.031958941370248795, -0.0005584587925113738, -0.0011035073548555374, -1.0490362910786644e-05, -4.768370445162873e-07, -0.0002826052950695157, -0.006354247219860554, -6.663577369181439e-05, -0.0018412794452160597, -0.003940200433135033, -0.00012289722508285195, -0.01580696925520897, -6.401333666872233e-05, -0.00018046658078674227, -0.0054043857380747795, -9.894321920000948e-06, -0.0001267114421352744, -1.5735502529423684e-05, -0.004381699021905661, -0.030390115454792976, -0.2602342665195465], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.24334175884723663, -4.768370445162873e-07, -5.960462772236497e-07, -0.06424963474273682, -0.0016530195716768503, -0.0018127213697880507, -0.002885347232222557, -0.0028084381483495235], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-0.005659864749759436, -1.490105023549404e-05, -0.0038441347423940897, -0.09501700848340988, -1.7083830833435059], "finish_reason": "stop"}, {"text": " df_top.plot(", "tokens": [" df", "_", "top", ".", "plot", "(", "\n"], "logprobs": [-0.11590094119310379, -0.00016532962035853416, -7.510157047363464e-06, -0.14656609296798706, -0.015064284205436707, -0.38817185163497925, -0.6987123489379883], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.0314057394862175, -1.1086402082582936e-05, -2.2291887944447808e-05, -0.7646961808204651, -0.005339053459465504], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.007002573926001787, -0.00034409802174195647, -8.260862523457035e-05, -0.0013574680779129267, -8.106198947643861e-06, -3.576272320060525e-06, -0.39475783705711365, -0.0019990478176623583], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.02894436940550804, -0.10813098400831223, -0.039172302931547165], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.016986124217510223, -0.04213670641183853], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.052995823323726654, -2.9444261599564925e-05, -4.0531076592742465e-06, -0.34263038635253906, -0.3788301944732666, -0.0012155532604083419, -0.005808024201542139], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"The file at {file_path} was not found.\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "The", " file", " at", " {", "file", "_", "path", "}", " was", " not", " found", ".", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.06741411983966827, -0.00010132275929208845, -1.9192511899746023e-05, -0.005036287009716034, -0.21440087258815765, -0.05384677276015282, -1.442021131515503, -0.23836036026477814, -1.6001633405685425, -0.3626638948917389, -0.3639853000640869, -0.0006538875168189406, -4.815939246327616e-05, -0.027318604290485382, -0.4914604425430298, -0.00023803261865396053, -0.0011232740944251418, -0.26828739047050476, -0.0059393346309661865, -0.002360773738473654, -0.057480864226818085, -0.0007122601964510977, -0.007940630428493023], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.0055889771319925785, -0.016826367005705833, -0.010449813678860664, -0.0013259199913591146, -0.01550289522856474], "finish_reason": "stop"}, {"text": "f\"An error occurred: {e}\")", "tokens": ["f", "\"", "An", " error", " occurred", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.8565773963928223, -0.001729065552353859, -0.37753725051879883, -0.058443717658519745, -0.023191403597593307, -0.36010199785232544, -0.0020251020323485136, -0.47485092282295227, -0.014001005329191685, -0.06702200323343277, -0.05477069690823555], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.003959198947995901, -0.3498155474662781], "finish_reason": "stop"}], "metrics": {"energy": 1167.9032912712098, "energy_per_token": 5.483114043526807, "time": 31.07188, "time_per_token": 0.14587737089201877}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -0.3150692284107208, -0.0062665874138474464], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0002972637885250151, -8.344646857949556e-07, 0.0, -0.02093060314655304, -0.43427664041519165, -5.722029527532868e-06, -2.622600959512056e-06, -0.001531619462184608, -0.001795467222109437, 0.0, -1.0251946150674485e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.0786563903093338, -0.020777195692062378], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.04054668918251991, -4.0531076592742465e-06, -0.0004881620698142797, -0.016124792397022247, -0.0009304723353125155, -0.00011920218821614981, -0.00552021898329258, -0.016785690560936928, -0.14239194989204407, -0.00019393471302464604, -8.177422569133341e-05, -7.986990567587782e-06, -0.06767786294221878, -0.10158687829971313], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.003314004512503743, -2.3841830625315197e-06, -6.318072337307967e-06, -0.00011062010162277147, -0.0015348332235589623, -5.4834770708112046e-05, -4.911301948595792e-05, -0.0005168771021999419, -0.00038258862332440913, -0.0008448368753306568, -0.021385902538895607], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.03694164380431175, -0.010083921253681183, -0.009771727956831455, -0.09819004684686661, -0.00047708096099086106, -0.0011948358733206987, -8.535020606359467e-05, -0.012159035541117191, -0.0002932118659373373, -0.05406385660171509, -0.0565958246588707, -0.4327036738395691, -0.005565505009144545, -0.00013147920253686607, -0.0097267497330904, -0.003017042065039277, -0.016113178804516792, -1.1920928244535389e-07, -0.00015639036428183317, -1.3708974620385561e-05, -0.000929281348362565, -2.3841830625315197e-06, -9.047575440490618e-05, -7.390948667307384e-06, -3.6000557884108275e-05, -0.005747933406382799, -0.0017446548445150256], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.01303132064640522, -0.006835410837084055, -0.020870013162493706, -0.00017009719158522785, -0.0014832934830337763, -0.00019012074335478246, -0.029294626787304878, -0.00018499570433050394, -0.025849122554063797, -0.02563140168786049, -0.47617948055267334], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.03321228176355362, -0.0004885195521637797, -0.000776347063947469, -1.1205610462639015e-05, -7.152555099310121e-07, -0.0002615109842736274, -0.008234006352722645, -8.129743218887597e-05, -0.0016776548000052571, -0.0036651596892625093, -0.00010680581908673048, -0.012563959695398808, -7.390703103737906e-05, -0.00018225439998786896, -0.004113784059882164, -1.0371154530730564e-05, -9.738924563862383e-05, -1.4543427823809907e-05, -0.003725138260051608, -0.031762294471263885, -1.5116609334945679, -0.0039431690238416195, -0.002190573140978813, -0.0006076634745113552, -0.003145510796457529], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.23267468810081482, -4.768370445162873e-07, -4.768370445162873e-07, -0.06688108295202255, -0.0016511153662577271, -0.0017353727016597986, -0.0023670769296586514, -0.0033858849201351404], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "frequency", "\"", "])", "\n"], "logprobs": [-0.00451580761000514, -1.5735502529423684e-05, -0.00532185984775424, -0.06722109019756317, -0.43341854214668274, -1.0728830375228426e-06, -7.390948667307384e-06, -0.023407327011227608, -0.004314637742936611, -0.012179998680949211, -0.0008925982983782887, -1.2484912872314453, -0.0018958942964673042, -2.0503786799963564e-05, -0.0001419681793777272, -0.9769873023033142, -0.0015588762471452355, -0.009882098063826561, -0.041486408561468124], "finish_reason": "stop"}, {"text": " df_top[\"Count\"].plot.barh(x=\"Word\", rot=0, title=\"Top 10 words\")", "tokens": [" df", "_", "top", "[", "\"", "Count", "\"", "]", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " rot", "=", "0", ",", " title", "=", "\"", "Top", " ", "1", "0", " words", "\"", ")", "\n"], "logprobs": [-0.1321263611316681, -0.0001308832288486883, -1.6927575416048057e-05, -2.0626118183135986, -0.00965201761573553, -0.008470205590128899, -1.3470558769768104e-05, -0.016342012211680412, -0.001412823679856956, -0.02445053681731224, -1.40462064743042, -0.027426939457654953, -0.6429712176322937, -0.05358252301812172, -0.3732350766658783, -0.0016969341086223722, -0.06745858490467072, -0.0007244108128361404, -7.748573807475623e-06, -0.16950015723705292, -2.397792100906372, -0.013630530796945095, -0.0820361003279686, -0.7789551615715027, -1.0358192920684814, -0.0017903504194691777, -0.011112821288406849, -0.6293545365333557, -0.2844291031360626, -0.0004901876673102379, -0.00011789103882620111, -1.2584768533706665, -0.14996758103370667, -0.037915974855422974, -0.004330542869865894], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.03159113600850105, -1.0490362910786644e-05, -1.5616295058862306e-05, -0.6370614767074585, -0.4661144018173218, -7.271740287251305e-06, -0.0022947664838284254, -0.0037859445437788963], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=300)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "3", "0", "0", ")", "\n"], "logprobs": [-0.005337274633347988, -0.00032300499151460826, -7.879423355916515e-05, -0.0016451646806672215, -7.390948667307384e-06, -3.6954811548639555e-06, -1.0690045356750488, -0.9008133411407471, -1.0490362910786644e-05, -0.0006911749369464815, -0.26558101177215576, -0.003589973319321871, -0.00010179955279454589, -0.3436979651451111, -0.007106385659426451], "finish_reason": "stop"}, {"text": "close(ax.figure)", "tokens": ["close", "(", "ax", ".", "figure", ")", "\n"], "logprobs": [-0.036761943250894547, -2.3690688610076904, -0.09961942583322525, -0.08524768054485321, -0.04479636251926422, -0.0014413930475711823, -0.04425181448459625], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.013186980038881302, -0.05409039929509163], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.04577776417136192, -3.576214658096433e-05, -3.814689989667386e-06, -0.34177327156066895, -0.32726240158081055, -0.0011566146276891232, -0.0042860317043960094], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found at {exc.filename}\")", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", " at", " {", "exc", ".", "filename", "}\"", ")", "\n"], "logprobs": [-0.05659954249858856, -0.0001456631434848532, -1.883488948806189e-05, -0.004321285057812929, -0.18184316158294678, -0.05288965627551079, -0.40831759572029114, -0.505210816860199, -0.0023123687133193016, -1.014536738395691, -0.638140082359314, -1.5904350280761719, -0.031777653843164444, -0.000546425289940089, -0.18309149146080017, -0.0045014480128884315, -1.2355482578277588], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005269922316074371, -0.019189076498150826, -0.014576726593077183, -0.001259487122297287, -0.01893165148794651], "finish_reason": "stop"}, {"text": "\"Error occurred:\", e)", "tokens": ["\"", "Error", " occurred", ":\"", ",", " e", ")", "\n"], "logprobs": [-0.7474114894866943, -1.4130131006240845, -0.9859543442726135, -0.5938349962234497, -0.006227256264537573, -0.34941157698631287, -0.020337270572781563, -0.04209773242473602], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.0032427129335701466, -0.3284693658351898], "finish_reason": "stop"}], "metrics": {"energy": 1325.7735501012796, "energy_per_token": 5.345861089118063, "time": 34.07287, "time_per_token": 0.13739060483870968}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0012635351158678532, -3.635817120084539e-05, -0.00030179237364791334, -2.3841830625315197e-06, -8.761498611420393e-05, -0.0003165697562508285, -0.007341667544096708, -0.00023445718397852033, -1.1920928244535389e-07, -0.3689063787460327, -0.005357313901185989], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.00035470392322167754, -1.0728830375228426e-06, 0.0, -0.022356268018484116, -0.3347424566745758, -5.722029527532868e-06, -2.3841830625315197e-06, -0.0015200738562271, -0.002044017892330885, 0.0, -1.168244216387393e-05, -2.3841855067985307e-07, 0.0, -2.3841855067985307e-07, -0.08013968169689178, -0.022979043424129486], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.04974549263715744, -3.576272320060525e-06, -0.0004407388041727245, -0.017527567222714424, -0.000990496133454144, -0.00020489977032411844, -0.008006620220839977, -0.011959515511989594, -0.14125102758407593, -0.00018130090029444546, -9.97731985989958e-05, -8.702239938429557e-06, -0.06667768955230713, -0.10188858956098557], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0022934582084417343, -2.50339189733495e-06, -6.556489552167477e-06, -0.0001227780303452164, -0.002614772180095315, -6.222531374078244e-05, -4.076874756719917e-05, -0.0004487222759053111, -0.00047541281674057245, -0.0007418026216328144, -0.037934910506010056], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.05741749331355095, -0.00829985924065113, -0.011405504308640957, -0.0936480164527893, -0.00040058218291960657, -0.0008114859228953719, -5.8410845667822286e-05, -0.01436499785631895, -0.00033539868309162557, -0.05726991966366768, -0.107554130256176, -0.4663268029689789, -0.005827105604112148, -0.00011228884250158444, -0.013071678578853607, -0.0020140379201620817, -0.011065545491874218, -1.1920928244535389e-07, -0.00014518637908622622, -1.6927575416048057e-05, -0.0013600870734080672, -3.2186455882765586e-06, -7.235741941258311e-05, -6.318072337307967e-06, -2.6464111215318553e-05, -0.004707919433712959, -0.0017949911998584867], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.027308976277709007, -0.006094443611800671, -0.026424333453178406, -0.0001565095444675535, -0.0015347142471000552, -0.000226472009671852, -0.02094613015651703, -0.00017128908075392246, -0.02684975229203701, -0.03420822322368622, -0.9550097584724426, -0.5038481950759888, -0.00013624693383462727, -0.0018208129331469536, -0.0006422125734388828, -0.009966019541025162, -1.1920928244535389e-07, -1.5616295058862306e-05, -2.7656173188006505e-05, -0.0006459057331085205, -1.4305104514278355e-06, -2.8371408916427754e-05, -2.9802276912960224e-06, -1.5139465176616795e-05, -0.00502774678170681], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03163548931479454, -0.0006974886637181044, -0.0015929164364933968, -1.2636104656849056e-05, -1.0728830375228426e-06, -0.0002613918040879071, -0.005131763871759176, -6.4490144723095e-05, -0.0014694855781272054, -0.0031907858792692423, -0.00013362467871047556, -0.013285799883306026, -0.00011359999916749075, -0.00025567124248482287, -0.004739242140203714, -1.4185804502631072e-05, -0.00012885693286079913, -1.3589766240329482e-05, -0.003267784370109439, -0.04768529161810875, -0.18265314400196075], "finish_reason": "stop"}, {"text": " words_freq[0:10]", "tokens": [" words", "_", "freq", "[", "0", ":", "1", "0", "]", "\n"], "logprobs": [-0.23820094764232635, -5.960462772236497e-07, -5.960462772236497e-07, -2.7860798835754395, -0.04110344126820564, -0.0026200036518275738, -0.0031569187995046377, -0.007103662937879562, -0.02513083443045616, -0.005282847676426172], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-0.004129573702812195, -1.3351351299206726e-05, -0.004071875009685755, -0.08078644424676895, -1.491346836090088], "finish_reason": "stop"}, {"text": " df_top.plot(kind=\"bar\", x=\"Word\", y=\"Count\")", "tokens": [" df", "_", "top", ".", "plot", "(", "kind", "=", "\"", "bar", "\"", ",", " x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.11939355731010437, -0.00014828535495325923, -9.65590606938349e-06, -0.1331930011510849, -0.010782433673739433, -0.30224692821502686, -2.1770894527435303, -0.002896758494898677, -8.976056415122002e-05, -0.00030560590676032007, -1.2404260635375977, -0.0030979756265878677, -0.02773749642074108, -0.0011753087164834142, -0.00033790123416110873, -0.00011777184408856556, -4.410734163684538e-06, -0.003458829363808036, -0.010605404153466225, -0.0011392300948500633, -1.1086402082582936e-05, -0.00013815402053296566, -1.2636104656849056e-05, -1.5340912342071533, -0.004889436531811953], "finish_reason": "stop"}, {"text": " save_path is None:", "tokens": [" save", "_", "path", " is", " None", ":", "\n"], "logprobs": [-0.031363800168037415, -1.1444026313256472e-05, -1.585470999998506e-05, -0.6579663753509521, -1.049343466758728, -0.0004956685588695109, -0.010780192911624908], "finish_reason": "stop"}, {"text": "savefig(save_path, format=\"png\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " format", "=", "\"", "png", "\"", ")", "\n"], "logprobs": [-0.009144448675215244, -0.000491021724883467, -6.41325386823155e-05, -0.0017873755423352122, -7.510157047363464e-06, -3.099436753473128e-06, -1.0941205024719238, -3.0867538452148438, -0.01502658985555172, -0.04189129173755646, -0.007922298274934292, -4.482168878894299e-05, -0.6517056226730347, -0.0068825320340693], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.043442193418741226, -0.11436431854963303, -0.03337382525205612], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.014464757405221462, -0.04217613488435745], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.05203540250658989, -2.90866428258596e-05, -3.6954811548639555e-06, -0.36172837018966675, -0.3912009298801422, -0.0011516136582940817, -0.005255454685539007], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File {file_path} not found!\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " {", "file", "_", "path", "}", " not", " found", "!", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.05325480177998543, -9.560128091834486e-05, -2.3603161025675945e-05, -0.0048226467333734035, -0.21403878927230835, -0.048919424414634705, -0.4445019066333771, -1.8997466564178467, -0.17838971316814423, -0.0006136203301139176, -3.361645576660521e-05, -0.015351117588579655, -0.18028044700622559, -0.002701088320463896, -2.731621265411377, -0.012990489602088928, -0.006059134379029274, -0.04913508519530296, -0.0009310677996836603, -0.007974689826369286], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005065464414656162, -0.021655414253473282, -0.010608352720737457, -0.0010351543314754963, -0.015037041157484055], "finish_reason": "stop"}, {"text": "\"An error occurred during function execution.\")", "tokens": ["\"", "An", " error", " occurred", " during", " function", " execution", ".", "\"", ")", "\n"], "logprobs": [-0.8003934025764465, -0.42542845010757446, -0.11469334363937378, -0.018214579671621323, -1.929027795791626, -0.8395535945892334, -0.0014627005439251661, -2.7797861099243164, -0.22251880168914795, -0.09039800614118576, -0.0033585589844733477], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004829527344554663, -0.37480878829956055], "finish_reason": "stop"}], "metrics": {"energy": 1399.6084779405599, "energy_per_token": 5.93054439805322, "time": 37.1435, "time_per_token": 0.15738771186440678}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0012013844680041075, -3.790783375734463e-05, -0.00020346954988781363, -3.6954811548639555e-06, -8.821098163025454e-05, -0.0002817710628733039, -0.006209604442119598, -0.00022230061586014926, -1.1920928244535389e-07, -0.3150692284107208, -0.0062665874138474464], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0002972637885250151, -8.344646857949556e-07, 0.0, -0.02093060314655304, -0.43427664041519165, -5.722029527532868e-06, -2.622600959512056e-06, -0.001531619462184608, -0.001795467222109437, 0.0, -1.0251946150674485e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.0786563903093338, -0.020777195692062378], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"].values)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "]", ".", "values", ")", "\n"], "logprobs": [-0.04054668918251991, -4.0531076592742465e-06, -0.0004881620698142797, -0.016124792397022247, -0.0009304723353125155, -0.00011920218821614981, -0.00552021898329258, -0.016785690560936928, -0.14239194989204407, -0.00019393471302464604, -8.177422569133341e-05, -7.986990567587782e-06, -2.728250026702881, -0.0014907924924045801, -0.6260198950767517, -0.11594095826148987, -0.09974242746829987], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.004048723261803389, -2.861018856492592e-06, -5.125986263010418e-06, -0.00013362467871047556, -0.0017279945313930511, -6.16293036728166e-05, -4.6132929128361866e-05, -0.00042691652197390795, -0.0005614373367279768, -0.0007842087652534246, -0.029067697003483772], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.03708799555897713, -0.00794796273112297, -0.010013583116233349, -0.055333010852336884, -0.0004183370911050588, -0.0008418591460213065, -9.30981186684221e-05, -0.013063913211226463, -0.0004866131057497114, -0.05157652497291565, -0.12079708278179169, -0.43337759375572205, -0.005390750709921122, -0.0001426833332516253, -0.014660724438726902, -0.0021879561245441437, -0.015904121100902557, -1.1920928244535389e-07, -0.00018273114983458072, -1.7881233361549675e-05, -0.0007127366843633354, -2.9802276912960224e-06, -6.985420623095706e-05, -4.529942543740617e-06, -3.659658250398934e-05, -0.0046014850959181786, -0.0017368007684126496], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.013375316746532917, -0.006773960776627064, -0.025435248389840126, -0.00019047829846385866, -0.0014261561445891857, -0.0003053675754927099, -0.02501806430518627, -0.00016962042718660086, -0.036397624760866165, -0.03137004002928734, -0.6055975556373596], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.029455069452524185, -0.0006618693005293608, -0.0013916341122239828, -9.536697689327411e-06, -5.960462772236497e-07, -0.0002836778585333377, -0.008330241777002811, -7.915183232398704e-05, -0.0016625404823571444, -0.0029471556190401316, -0.00010871296399272978, -0.018305763602256775, -9.786603914108127e-05, -0.00019941726350225508, -0.0050485036335885525, -1.2636104656849056e-05, -9.059495641849935e-05, -1.4305012882687151e-05, -0.0038399784825742245, -0.035242993384599686, -0.20892764627933502], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.28621241450309753, -4.768370445162873e-07, -4.768370445162873e-07, -0.05207104608416557, -0.0016843193443492055, -0.002039021346718073, -0.0027331875171512365, -0.0037595797330141068], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0061373342759907246, -1.549708758830093e-05, -0.003767893183976412, -0.06839830428361893, -0.6013808846473694, -1.7881377516459906e-06, -8.583032467868179e-06, -0.026455331593751907, -0.005461177788674831, -0.011100560426712036, -0.0007924277451820672, -0.5557420253753662, -0.0008760428754612803, -2.539125671319198e-05, -0.00014172980445437133, -0.45179587602615356, -0.09274663031101227, -0.0009901388548314571, -0.007278591860085726, -0.038061268627643585], "finish_reason": "stop"}, {"text": " df_top.plot(x=\"Word\", y=\"Count\", kind=\"bar\", legend=False)", "tokens": [" df", "_", "top", ".", "plot", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " kind", "=", "\"", "bar", "\"", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.13424965739250183, -0.00013457823661156, -7.748573807475623e-06, -0.07356946170330048, -0.011042908765375614, -0.3512261211872101, -0.9818637371063232, -0.0004996005445718765, -0.00040141629870049655, -0.0009107018122449517, -2.7418097943154862e-06, -3.58813522325363e-05, -0.005724465008825064, -0.00374770350754261, -1.8596476365928538e-05, -0.00011252723925281316, -1.811964830267243e-05, -0.0012418661499395967, -0.009039667434990406, -0.00013863079948350787, -5.5549986427649856e-05, -0.0002307625545654446, -0.15938425064086914, -0.20619848370552063, -0.7745464444160461, -0.0002636561985127628, -0.11627594381570816, -0.4604170024394989, -0.005862422753125429], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.03202960267663002, -9.894321920000948e-06, -1.847726889536716e-05, -0.6409609913825989, -0.4451053738594055, -6.6756979322235566e-06, -0.0019238550448790193, -0.0030680273193866014], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.008821799419820309, -0.0004984090337529778, -7.64102369430475e-05, -0.0018958942964673042, -6.6756979322235566e-06, -2.9802276912960224e-06, -0.4031444191932678, -0.0017027654685080051], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.02994653396308422, -0.10283728688955307, -0.03832632303237915], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.015310028567910194, -0.04213864728808403], "finish_reason": "stop"}, {"text": " FileNotFoundError:", "tokens": [" File", "NotFound", "Error", ":", "\n"], "logprobs": [-0.05280236527323723, -3.0874729418428615e-05, -3.6954811548639555e-06, -1.215827226638794, -0.003967510536313057], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"File not found: {exc}\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "File", " not", " found", ":", " {", "exc", "}\"", ")", " from", " exc", "\n"], "logprobs": [-0.0661364495754242, -0.00012933371181134135, -1.764281842042692e-05, -0.004088496323674917, -0.2366306036710739, -0.04936431720852852, -0.4352157711982727, -0.46109744906425476, -0.002075067488476634, -0.5304720401763916, -0.009128266014158726, -0.8332628011703491, -1.6318080425262451, -0.0035992383491247892, -0.5062315464019775, -0.033374860882759094, -0.016791434958577156], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.004153554327785969, -0.01701425202190876, -0.009018165990710258, -0.001105888863094151, -0.014621252194046974], "finish_reason": "stop"}, {"text": "e)", "tokens": ["e", ")", "\n"], "logprobs": [-2.5089476108551025, -0.026555519551038742, -0.052637144923210144], "finish_reason": "stop"}, {"text": " None", "tokens": [" None"], "logprobs": [-0.0036208562087267637], "finish_reason": "stop"}], "metrics": {"energy": 1205.4667354316714, "energy_per_token": 5.405680427944715, "time": 32.15977, "time_per_token": 0.14421421524663677}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0013150862650945783, -3.123234637314454e-05, -0.0002615109842736274, -3.2186455882765586e-06, -9.16677454370074e-05, -0.00025018901214934886, -0.007203558459877968, -0.00025340684805996716, -1.1920928244535389e-07, -0.3406352698802948, -0.005871311295777559], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.00031513971043750644, -7.152555099310121e-07, 0.0, -0.020581839606165886, -0.3669080436229706, -3.576272320060525e-06, -1.9073468138230965e-06, -0.0017615529941394925, -0.0017888034926727414, 0.0, -9.775113539944869e-06, -2.3841855067985307e-07, 0.0, 0.0, -0.09418249875307083, -0.01776966080069542], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.047026537358760834, -4.6491513785440475e-06, -0.0005260513862594962, -0.013224156573414803, -0.0009834696538746357, -0.00014411364099942148, -0.006750279571861029, -0.014151225797832012, -0.14669787883758545, -0.00016830935783218592, -9.798523387871683e-05, -9.179073458653875e-06, -0.07927162200212479, -0.09820539504289627], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.003460017265751958, -2.50339189733495e-06, -5.483612312673358e-06, -0.00010918975021922961, -0.002976632211357355, -5.1616290875244886e-05, -5.638440416078083e-05, -0.0005127069307491183, -0.0004690977220889181, -0.0008061258122324944, -0.020685536786913872], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.034603051841259, -0.007916976697742939, -0.010579925961792469, -0.061468660831451416, -0.000446696620201692, -0.0007874249131418765, -6.806619057897478e-05, -0.010268830694258213, -0.0001854724541772157, -0.04177296534180641, -0.08983121067285538, -0.3701256215572357, -0.005074359942227602, -0.00010644822759786621, -0.00825091265141964, -0.002277877414599061, -0.015078962780535221, -1.1920928244535389e-07, -0.00016175392374861985, -1.5258672647178173e-05, -0.0006914132391102612, -2.0265558760002023e-06, -6.937739817658439e-05, -5.960446742392378e-06, -3.313963316031732e-05, -0.0043703047558665276, -0.001642784452997148], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.01665814220905304, -0.00524098752066493, -0.019540781155228615, -0.00013052565918769687, -0.001277702976949513, -0.00018487652414478362, -0.017204564064741135, -0.00020132421923335642, -0.027975277975201607, -0.02828114666044712, -0.8078472018241882, -0.6216374039649963, -0.00015031162183731794, -0.0013948483392596245, -0.00044050050200894475, -0.004449942149221897, -1.1920928244535389e-07, -1.6212332411669195e-05, -2.074220174108632e-05, -0.0007426364463753998, -1.0728830375228426e-06, -4.053033626405522e-05, -2.7418097943154862e-06, -1.3351351299206726e-05, -0.005171730648726225], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.036731839179992676, -0.0006093314150348306, -0.0007845661020837724, -8.22540732769994e-06, -4.768370445162873e-07, -0.0002261144545627758, -0.00533549627289176, -7.354942499659956e-05, -0.0021061161532998085, -0.002790606813505292, -0.00011383838864276186, -0.010625219903886318, -0.00010108436981681734, -0.00021717573690693825, -0.004289592616260052, -1.2755313036905136e-05, -0.00010918975021922961, -1.5616295058862306e-05, -0.004020584747195244, -0.02752552181482315, -0.23147495090961456], "finish_reason": "stop"}, {"text": " [pair[0] for pair in words_freq[:10]]", "tokens": [" [", "pair", "[", "0", "]", " for", " pair", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-1.9022835493087769, -2.1872522830963135, -0.06398085504770279, -0.0034855585545301437, -0.0002450642641633749, -0.000821014866232872, -0.0007964776013977826, -1.3828182090946939e-05, -0.0011035073548555374, -1.1920922133867862e-06, -3.576278118089249e-07, -0.1836584359407425, -0.0017326356610283256, -0.002654958749189973, -0.010859321802854538, -0.00439855270087719], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-0.004503702744841576, -1.6331539882230572e-05, -0.003246158827096224, -0.08493772894144058, -1.2759480476379395], "finish_reason": "stop"}, {"text": " df_top.plot.barh(", "tokens": [" df", "_", "top", ".", "plot", ".", "bar", "h", "(", "\n"], "logprobs": [-0.11008819192647934, -0.0001597276859683916, -1.823885577323381e-05, -0.10681936889886856, -0.010497118346393108, -1.2155828475952148, -0.007890365086495876, -0.4599722623825073, -0.0042650215327739716, -1.5517587661743164], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03178007900714874, -1.156323378381785e-05, -2.5987286790041253e-05, -0.7662878036499023, -0.0044997865334153175], "finish_reason": "stop"}, {"text": "savefig(save_path, bbox_inches=\"tight\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ")", "\n"], "logprobs": [-0.007356578018516302, -0.0005327236140146852, -6.23445157543756e-05, -0.0011242267210036516, -7.510157047363464e-06, -2.145764938177308e-06, -1.1605244874954224, -0.6091657876968384, -1.2755313036905136e-05, -0.0001497156627010554, -2.4318398573086597e-05, -2.884823152271565e-05, -0.004530285485088825, -0.001046229270286858, -5.960462772236497e-07, -1.597391747054644e-05, -0.1150851622223854, -0.00208125333301723], "finish_reason": "stop"}, {"text": "close(ax.figure)", "tokens": ["close", "(", "ax", ".", "figure", ")", "\n"], "logprobs": [-0.04187231510877609, -2.425367832183838, -0.0881083682179451, -0.0660867989063263, -0.056704189628362656, -0.0016401661559939384, -0.047598108649253845], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.01712031103670597, -0.042267899960279465], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.052376873791217804, -3.4689302992774174e-05, -3.6954811548639555e-06, -0.3507993817329407, -0.31443068385124207, -0.0011682836338877678, -0.005611618980765343], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"The file '{file_path}' was not found.\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "The", " file", " '", "{", "file", "_", "path", "}'", " was", " not", " found", ".", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.05625590309500694, -0.00011324241495458409, -1.4662635294371285e-05, -0.005296128336340189, -0.21562951803207397, -0.0616895891726017, -1.5766403675079346, -0.24970287084579468, -1.1307934522628784, -0.0006255338666960597, -0.1871802657842636, -0.000263894529780373, -8.821098163025454e-05, -0.005694832187145948, -0.5251770615577698, -0.00020930961181875318, -0.0008491248008795083, -0.1347101479768753, -0.005864081904292107, -0.0014265133067965508, -0.03953756392002106, -0.0006382812280207872, -0.007750794291496277], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005598579533398151, -0.016952136531472206, -0.012066700495779514, -0.0012274596374481916, -0.017499687150120735], "finish_reason": "stop"}, {"text": "f\"An error occurred: {e}\")", "tokens": ["f", "\"", "An", " error", " occurred", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.8123108744621277, -0.0018732872558757663, -0.4199417233467102, -0.08717574179172516, -0.0264606736600399, -0.34330683946609497, -0.0019113620510324836, -0.44887349009513855, -0.015559233725070953, -0.0764257088303566, -0.050057362765073776], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.004352026619017124, -0.37968116998672485], "finish_reason": "stop"}], "metrics": {"energy": 1261.8816918888097, "energy_per_token": 5.346956321562753, "time": 31.92179, "time_per_token": 0.1352618220338983}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.0015687552513554692, -3.85038583772257e-05, -0.00039402826223522425, -4.410734163684538e-06, -9.83428253675811e-05, -0.00025829317746683955, -0.007803904823958874, -0.00030894274823367596, -1.1920928244535389e-07, -0.3351997137069702, -0.005409840028733015], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003033416287507862, -9.536738616588991e-07, 0.0, -0.022212404757738113, -0.4935497045516968, -5.8412379075889476e-06, -2.0265558760002023e-06, -0.0014044907875359058, -0.002369812224060297, 0.0, -1.1086402082582936e-05, -2.3841855067985307e-07, 0.0, 0.0, -0.07081931829452515, -0.024796906858682632], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.03638980910181999, -3.6954811548639555e-06, -0.0005988473421894014, -0.011725435964763165, -0.0007169060409069061, -0.00021908267808612436, -0.005237785633653402, -0.012517458759248257, -0.14659245312213898, -0.00018070495571009815, -0.00011121608258690685, -7.390948667307384e-06, -0.06722688674926758, -0.10690689086914062], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.002774439286440611, -2.861018856492592e-06, -5.602820692729438e-06, -0.00011967896716669202, -0.001866623992100358, -7.176141662057489e-05, -5.245071224635467e-05, -0.0005725175142288208, -0.0005341533687897027, -0.0007160721579566598, -0.023884663358330727], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.030474761500954628, -0.009594990871846676, -0.013130390085279942, -0.06200089305639267, -0.0002992897352669388, -0.000724887300748378, -6.0794889577664435e-05, -0.013993717730045319, -0.00015686711412854493, -0.060722339898347855, -0.047905195504426956, -0.4398851990699768, -0.006031762808561325, -0.00015722469834145159, -0.012963660061359406, -0.0023185533937066793, -0.008521740324795246, -1.1920928244535389e-07, -0.0001578206429257989, -1.811964830267243e-05, -0.0012825843878090382, -2.264974000354414e-06, -7.795983401592821e-05, -7.390948667307384e-06, -2.9444261599564925e-05, -0.004704953171312809, -0.0016245751176029444], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.016096990555524826, -0.005676933564245701, -0.022017329931259155, -0.000105375460407231, -0.0009793015196919441, -0.00024875884992070496, -0.01954382099211216, -0.00016389934171456844, -0.029582615941762924, -0.04101968929171562, -0.7792444229125977, -0.4277057349681854, -2.169585604860913e-05, -0.0038220465648919344, -0.00033909291960299015, -0.0016708712792024016, -7.748303323751315e-05, -0.002537128748372197, -1.1920928244535389e-07, -6.556489552167477e-06, -0.0007384672062471509, -1.1920928244535389e-07, -5.757642793469131e-05, 0.0, -1.5497195136049413e-06, -0.42022767663002014, -0.0015493544051423669], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.024519750848412514, -0.0006936766440048814, -0.0012867513578385115, -1.490105023549404e-05, -1.0728830375228426e-06, -0.00025674383505247533, -0.006061148829758167, -6.341733387671411e-05, -0.0014382981462404132, -0.0030841901898384094, -0.00010668662434909493, -0.015747711062431335, -7.986703712958843e-05, -0.00019071667338721454, -0.005204224959015846, -1.1205610462639015e-05, -0.00010156115604331717, -1.3232143828645349e-05, -0.003631665138527751, -0.041187528520822525, -1.341861367225647, -0.0038101710379123688, -0.0022619394585490227, -0.000662822334561497, -0.002577675273641944], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.2597407400608063, -4.768370445162873e-07, -7.152555099310121e-07, -0.06817673146724701, -0.0015397133538499475, -0.0017998700495809317, -0.0024246361572295427, -0.00385244726203382], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=top_words, columns=[\"Word\", \"WordCount\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Word", "Count", "\"", "])", "\n"], "logprobs": [-0.005055857356637716, -1.537788011773955e-05, -0.005234465003013611, -0.08282188326120377, -1.9686179161071777, -0.02930261567234993, -0.09866508096456528, -1.0728830375228426e-06, -3.814689989667386e-06, -0.04704428091645241, -0.0032131255138665438, -0.02692471817135811, -0.000842692912556231, -0.6534519791603088, -0.0012238877825438976, -3.361645576660521e-05, -0.0001958416833076626, -3.0111615657806396, -1.8901549577713013, -0.012850098311901093, -0.014252769760787487, -0.022129273042082787], "finish_reason": "stop"}, {"text": " df_top.plot.bar(x=\"Word\", y=\"Count\")", "tokens": [" df", "_", "top", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.13390718400478363, -0.00012158608296886086, -4.768360213347478e-06, -0.09315865486860275, -0.013441184535622597, -0.9274628758430481, -0.009291148744523525, -0.8782364726066589, -0.2834716737270355, -0.0005224770284257829, -0.00034588552080094814, -8.5588610090781e-05, -3.933898824470816e-06, -0.006280684377998114, -0.020992940291762352, -0.006922902539372444, -1.4662635294371285e-05, -8.856858039507642e-05, -9.65590606938349e-06, -1.4048774242401123, -0.0046506100334227085], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03332885727286339, -1.0013530300057027e-05, -1.8596476365928538e-05, -0.7338287234306335, -0.004041955806314945], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=300)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "3", "0", "0", ")", "\n"], "logprobs": [-0.010908493772149086, -0.0005397531786002219, -7.83174327807501e-05, -0.0016445695655420423, -7.152531907195225e-06, -2.622600959512056e-06, -1.1325421333312988, -0.9017188549041748, -1.07287787614041e-05, -0.0006649667047895491, -0.25709110498428345, -0.0032920234370976686, -8.940297266235575e-05, -0.3251574635505676, -0.00783903431147337], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.033213432878255844, -0.09109475463628769, -0.038786958903074265], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.013543271459639072, -0.05135754123330116], "finish_reason": "stop"}, {"text": " FileNotFoundError as err:", "tokens": [" File", "NotFound", "Error", " as", " err", ":", "\n"], "logprobs": [-0.05323287099599838, -3.015949550899677e-05, -4.410734163684538e-06, -0.34110212326049805, -2.6162970066070557, -0.004250183701515198, -0.008281651884317398], "finish_reason": "stop"}, {"text": " FileNotFoundError(", "tokens": [" File", "NotFound", "Error", "(", "\n"], "logprobs": [-0.040314167737960815, -0.00011753345461329445, -1.8715683836489916e-05, -0.005300278775393963, -1.9879205226898193], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.0054590436629951, -0.01927023194730282, -0.013248979113996029, -0.0011793570592999458, -0.011970116756856441], "finish_reason": "stop"}, {"text": "\"An error occurred:\", e)", "tokens": ["\"", "An", " error", " occurred", ":\"", ",", " e", ")", "\n"], "logprobs": [-0.749309778213501, -0.3196091651916504, -0.10844270884990692, -0.015216458588838577, -0.42760521173477173, -0.010830076411366463, -0.257986456155777, -0.011714949272572994, -0.0480264350771904], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.003973922226577997, -0.3875340521335602], "finish_reason": "stop"}], "metrics": {"energy": 1210.389134859086, "energy_per_token": 5.150592063230153, "time": 30.908354, "time_per_token": 0.13152491063829788}}
{"outputs": [{"text": " pd.read_csv(file_path, header=0, encoding='cp1251')", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " header", "=", "0", ",", " encoding", "='", "cp", "1", "2", "5", "1", "')", "\n"], "logprobs": [-0.0012635351158678532, -3.635817120084539e-05, -0.00030179237364791334, -2.3841830625315197e-06, -8.761498611420393e-05, -0.0003165697562508285, -0.007341667544096708, -0.00023445718397852033, -1.1920928244535389e-07, -1.1791023015975952, -1.4670428037643433, -0.2356538474559784, -0.3915850520133972, -1.4049911499023438, -1.2626585960388184, -0.276674747467041, -3.12835955619812, -0.017784416675567627, -0.0008592489175498486, -0.00012468514614738524, -0.2355407327413559, -0.13587675988674164, -0.038406167179346085], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0002795067266561091, -8.344646857949556e-07, 0.0, -0.020083177834749222, -0.37282952666282654, -5.483612312673358e-06, -1.7881377516459906e-06, -0.001211147871799767, -0.001910767168737948, 0.0, -1.0251946150674485e-05, -1.1920928244535389e-07, 0.0, 0.0, -0.08047279715538025, -0.018533626571297646], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.04664693772792816, -4.529942543740617e-06, -0.0004892344586551189, -0.015351704321801662, -0.0010313435923308134, -0.0002644904307089746, -0.006122405640780926, -0.017289631068706512, -0.1619243025779724, -0.00017510310863144696, -8.844937838148326e-05, -1.0013530300057027e-05, -0.06374320387840271, -0.10924617946147919], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0038175340741872787, -3.2186455882765586e-06, -4.410734163684538e-06, -0.0001006075763143599, -0.0036551826633512974, -5.6622808187967166e-05, -5.328513361746445e-05, -0.0005093707586638629, -0.0004332319076638669, -0.0010084786918014288, -0.03646935150027275], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx], rank) for idx, word in enumerate(vectorizer.get_feature_names_out())]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "]", ",", " rank", ")", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "()", ")]", "\n"], "logprobs": [-0.039150409400463104, -0.006941134110093117, -0.012104860506951809, -0.07126622647047043, -0.0004439560289029032, -0.0014881737297400832, -8.022463589441031e-05, -0.014975623227655888, -0.000348268891684711, -0.06252063810825348, -2.489391565322876, -0.46275627613067627, -1.075026512145996, -0.10895159095525742, -0.13491684198379517, -1.7549104690551758, -2.884823152271565e-05, -0.09254962205886841, -0.02074345201253891, -0.009021592326462269, -0.00020847532141488045, -0.002415717113763094, -1.1920928244535389e-07, -5.924526340095326e-05, -0.00012361239350866526, 0.0, -2.47952248173533e-05, 0.0, -3.015949550899677e-05, -0.33148014545440674, -0.0013317534467205405, -0.05761197581887245, -0.0013462775386869907, -0.0045184181071817875], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.02065599337220192, -0.004279859364032745, -0.023021450266242027, -0.00022015532886143774, -0.0013311582151800394, -0.00034564718953333795, -0.02267015166580677, -0.0002592465898487717, -0.03309338539838791, -0.018410982564091682, -0.5775576233863831], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.035378433763980865, -0.0005137792322784662, -0.0009477414423599839, -1.1444026313256472e-05, -5.960462772236497e-07, -0.0002562671434134245, -0.007816797122359276, -6.603976362384856e-05, -0.0014072287594899535, -0.0038823720533400774, -0.00012611546844709665, -0.013186980038881302, -6.139089964563027e-05, -0.000270091783022508, -0.004039343912154436, -1.2993727978027891e-05, -9.214453893946484e-05, -1.3232143828645349e-05, -0.0037962766364216805, -0.03550545871257782, -0.19551017880439758], "finish_reason": "stop"}, {"text": " [word for (word, freq) in words_freq][:10]", "tokens": [" [", "word", " for", " (", "word", ",", " fre", "q", ")", " in", " words", "_", "freq", "][", ":", "1", "0", "]", "\n"], "logprobs": [-2.041356325149536, -0.6261391639709473, -0.7076005339622498, -0.8921211361885071, -6.19869097135961e-05, -5.209310256759636e-05, -2.021469831466675, -0.0001958416833076626, -0.0014413930475711823, -8.415821503149346e-05, -0.001262344536371529, -9.536738616588991e-07, -3.576278118089249e-07, -2.124858856201172, -0.1701708883047104, -0.0018149822717532516, -0.00203473842702806, -0.004737936891615391, -0.00304473377764225], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.005944548640400171, -1.645074735279195e-05, -0.004068788141012192, -0.07189778238534927, -0.5167873501777649, -1.0728830375228426e-06, -1.0609570381348021e-05, -0.02660636231303215, -0.004707326181232929, -0.01170540601015091, -0.0008401916129514575, -1.3501982688903809, -0.0018585327779874206, -2.1576648578047752e-05, -0.00011467275908216834, -0.728296160697937, -0.0025676873046904802, -0.00848025269806385, -0.06543246656656265], "finish_reason": "stop"}, {"text": " df_top.plot(x=\"Word\", y=\"Count\", kind=\"bar\", title=\"Ten most frequent words\")", "tokens": [" df", "_", "top", ".", "plot", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " kind", "=", "\"", "bar", "\"", ",", " title", "=", "\"", "Ten", " most", " frequent", " words", "\"", ")", "\n"], "logprobs": [-0.09550134837627411, -0.0001308832288486883, -2.2053474822314456e-05, -0.1367868036031723, -0.010383039712905884, -0.4555579721927643, -0.9378789663314819, -0.0004769618099089712, -0.0003859252028632909, -0.0005944392178207636, -2.9802276912960224e-06, -9.131014667218551e-05, -0.006905854679644108, -0.002620598068460822, -2.586808113846928e-05, -0.00011836781777674332, -1.597391747054644e-05, -0.0016504012746736407, -0.011789880692958832, -0.00014184899919200689, -6.0794889577664435e-05, -0.0003133521240670234, -0.14970137178897858, -0.20087529718875885, -1.414093255996704, -0.0008056493825279176, -0.011155379004776478, -3.216063976287842, -0.8459879159927368, -2.123718500137329, -0.02872280217707157, -0.012236524373292923, -0.008725493215024471, -0.0007838514284230769], "finish_reason": "stop"}, {"text": " save_path is None:", "tokens": [" save", "_", "path", " is", " None", ":", "\n"], "logprobs": [-0.03186345472931862, -9.536697689327411e-06, -2.3603161025675945e-05, -0.679137110710144, -1.0093331336975098, -0.0005189026123844087, -0.012649774551391602], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=1000)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "1", "0", "0", "0", ")", "\n"], "logprobs": [-0.006654602009803057, -0.000697965151630342, -6.5205356804654e-05, -0.0016299308044835925, -7.748573807475623e-06, -2.861018856492592e-06, -1.1656556129455566, -1.0409018993377686, -9.894321920000948e-06, -0.0005683475756086409, -2.5920920372009277, -0.7379753589630127, -0.001105888863094151, -2.614485025405884, -0.09302571415901184, -0.009404642507433891], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.044819388538599014, -0.12440299242734909, -0.03455710411071777], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.011391361244022846, -0.04976704344153404], "finish_reason": "stop"}, {"text": " FileNotFoundError as e:", "tokens": [" File", "NotFound", "Error", " as", " e", ":", "\n"], "logprobs": [-0.051151078194379807, -4.649054244509898e-05, -3.576272320060525e-06, -0.3457079231739044, -0.3807331621646881, -0.001404966926202178, -0.005773771554231644], "finish_reason": "stop"}, {"text": " FileNotFoundError(f\"The file path '{file_path}' does not exist.\") from exc", "tokens": [" File", "NotFound", "Error", "(", "f", "\"", "The", " file", " path", " '", "{", "file", "_", "path", "}'", " does", " not", " exist", ".", "\"", ")", " from", " exc", "\n"], "logprobs": [-0.05674169585108757, -0.00012587709352374077, -1.8000440832111053e-05, -0.004608367569744587, -0.17762327194213867, -0.060777533799409866, -1.529518485069275, -0.23366765677928925, -1.931818962097168, -1.0245516300201416, -0.0003401654539629817, -0.46142515540122986, -0.00043084874050691724, -7.760223525110632e-05, -0.00699203833937645, -0.5010455846786499, -0.00042489083716645837, -0.0036659911274909973, -0.13582192361354828, -0.004065463785082102, -0.002717613708227873, -0.11284396797418594, -0.0006541258189827204, -0.006903250236064196], "finish_reason": "stop"}, {"text": " Exception as exc:", "tokens": [" Exception", " as", " exc", ":", "\n"], "logprobs": [-0.005290199536830187, -0.018562648445367813, -0.01140220370143652, -0.0013078240444883704, -0.016266249120235443], "finish_reason": "stop"}, {"text": "f\"Error occurred: {e}\")", "tokens": ["f", "\"", "Error", " occurred", ":", " {", "e", "}\"", ")", "\n"], "logprobs": [-0.817969560623169, -0.0015950588276609778, -1.3272912502288818, -0.8412832617759705, -0.5245664119720459, -0.0022367238998413086, -0.5861569046974182, -0.01617417484521866, -0.049587368965148926, -0.054829154163599014], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.0036202622577548027, -0.3333881199359894], "finish_reason": "stop"}], "metrics": {"energy": 1392.60987056446, "energy_per_token": 5.009388023613165, "time": 36.277138, "time_per_token": 0.13049330215827337}}
