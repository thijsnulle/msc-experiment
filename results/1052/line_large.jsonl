{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.replace(\"[^a-zA-Z]+\", \" \", regex=True)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "replace", "(", "\"", "[", "^", "a", "-", "z", "A", "-", "Z", "]+", "\"", ",", " \"", " \"", ",", " regex", "=", "True", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8530066013336182, -0.03686662018299103, -0.9176292419433594, -0.6719261407852173, -1.0810647010803223, -0.031654197722673416, -5.1020273531321436e-05, -0.00012981049076188356, -0.006551217287778854, -2.002696055569686e-05, -0.0013404440833255649, -2.702899932861328, -0.008263918571174145, -2.1576648578047752e-05, -0.004097875207662582, -0.01335590798407793, -0.10943233221769333, -0.02126302383840084, -0.0003943857445847243, -0.0012710357550531626, -0.001602675998583436, -0.3700464367866516], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -0.41199398040771484, -3.2186455882765586e-06, -4.6491513785440475e-06, -0.001842707279138267, -0.017574068158864975, 0.0, -8.4638240878121e-06, -2.145764938177308e-06, 0.0, 0.0, -0.19700554013252258, -0.016263552010059357], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).sum(axis=0)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.3623025119304657, -2.5987286790041253e-05, -7.664863369427621e-05, -0.006578811444342136, -0.003461324144154787, -0.00025328766787424684, -0.014238080009818077, -0.018174076452851295, -0.08900271356105804, -0.0017191881779581308, -0.0005447572330012918, -0.00019298121333122253, -0.14113116264343262, -0.8871932625770569, -0.7439634203910828, -2.8609820219571702e-05, -0.0035499429795891047, -0.0002762889489531517, -0.0005152089870534837, -9.011816291604191e-05, -0.8864033818244934], "finish_reason": "stop"}, {"text": " ValueError as e:", "tokens": [" Value", "Error", " as", " e", ":", "\n"], "logprobs": [-0.43807297945022583, -1.7881377516459906e-06, -1.7402613162994385, -0.32844188809394836, -0.0009355935617350042, -0.0141521655023098], "finish_reason": "stop"}, {"text": "\"The input file is empty or contains only stop words.\")", "tokens": ["\"", "The", " input", " file", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.05903150141239166, -1.2511265277862549, -0.4638153910636902, -0.6231507658958435, -0.13593535125255585, -0.33635011315345764, -0.019554808735847473, -0.10166247934103012, -0.005517255049198866, -0.004265377763658762, -0.023792125284671783, -0.13306330144405365, -0.07193782925605774, -0.0004362108593340963, -0.00026222606538794935], "finish_reason": "stop"}, {"text": "", "tokens": ["\n"], "logprobs": [-2.567951202392578], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.011049393564462662, -7.629365427419543e-06, -1.585470999998506e-05, -0.00010799778101500124, -0.0004976941272616386, -3.85038583772257e-05, -3.40932747349143e-05, -0.00017557987303007394, -0.0007849234389141202, -0.0004817279113922268, -0.08137322217226028], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.005846897605806589, -0.015138613060116768, -0.012514280155301094, -0.08032550662755966, -0.00025519452174194157, -0.00041237910045310855, -4.589452510117553e-05, -0.021085156127810478, -0.0002641328901518136, -0.022082164883613586, -0.018657661974430084, -0.03145702928304672, -0.0022573007736355066, -6.627816765103489e-05, -0.0014217516873031855, -0.0006846229662187397, -0.0011308948742225766, 0.0, -1.5020257706055418e-05, -1.2516897186287679e-05, -0.00027211778797209263, -2.9802276912960224e-06, -7.60526381782256e-05, -5.483612312673358e-06, -5.900685573578812e-05, -0.004048604518175125, -0.0017156180692836642], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names_out())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "())", "\n"], "logprobs": [-0.011675714515149593, -0.009232447482645512, -0.06690829247236252, -0.0001426833332516253, -0.0011300613405182958, -0.0001776060671545565, -0.040424883365631104, -0.00029571453342214227, -0.01948244497179985, -0.017174098640680313, -0.5976024270057678, -0.8066670894622803, -2.9682672902708873e-05, -0.006786511745303869, -0.00031120702624320984, -0.020405486226081848, -4.076874756719917e-05, -0.0028250806499272585, -1.1920928244535389e-07, -7.390948667307384e-06, -0.0013934198068454862, -1.1920928244535389e-07, -4.815939246327616e-05, 0.0, -6.6756979322235566e-06, -1.4005964994430542, -0.021006831899285316, -0.0014867454301565886, -0.0039409128949046135], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.030879158526659012, -0.0010898133041337132, -0.0023011888843029737, -2.729855441430118e-05, -2.50339189733495e-06, -0.0006598440813831985, -0.009954452514648438, -0.00014709345123264939, -0.0009893052047118545, -0.0031619099900126457, -0.0001174142598756589, -0.006386703345924616, -5.8053239627042785e-05, -0.0003649522550404072, -0.007291846442967653, -1.2993727978027891e-05, -0.00010895135346800089, -1.966933996300213e-05, -0.0058422754518687725, -0.020669421181082726, -0.0729665532708168], "finish_reason": "stop"}, {"text": " [word[0] for word in words_freq[:10]]", "tokens": [" [", "word", "[", "0", "]", " for", " word", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.48269903659820557, -0.6133418679237366, -1.5363752841949463, -0.007668816950172186, -0.0003415954706724733, -0.0011524471919983625, -0.0017245433991774917, -0.00023338454775512218, -0.0012487717904150486, -2.264974000354414e-06, -5.960462772236497e-07, -0.31102824211120605, -0.009332721121609211, -0.0031884093768894672, -0.0211478341370821, -0.009976995177567005], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0010356307029724121, -2.0265558760002023e-06, -1.1324817933200393e-05, -0.008019155822694302, -0.12099482864141464, -8.344646857949556e-07, -4.172316494077677e-06, -0.004727496299892664, -0.00114827963989228, -0.004340038634836674, -0.0002356490003876388, -0.6758884191513062, -0.0011614966206252575, -7.629365427419543e-06, -6.007967749610543e-05, -0.11247289180755615, -0.0018331881146878004, -0.0016243371646851301, -0.023518182337284088], "finish_reason": "stop"}, {"text": " df.plot.barh(x=\"Word\", y=\"Count\", legend=False)", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.050311435014009476, -0.08793948590755463, -0.0012700833613052964, -0.28240418434143066, -0.0006943913758732378, -0.9145986437797546, -0.0014676999999210238, -0.08645012229681015, -0.0003240775258745998, -7.152301259338856e-05, -0.007819872349500656, -3.6954811548639555e-06, -0.0007794441189616919, -0.02338322065770626, -0.0006505518686026335, -7.152531907195225e-06, -0.00013076403411105275, -1.156323378381785e-05, -0.13635815680027008, -0.4347717761993408, -1.728519782773219e-05, -0.0025613855104893446, -0.3833005130290985, -0.002723557874560356], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03908586874604225, -5.483612312673358e-06, -2.2291887944447808e-05, -0.451612263917923, -0.004838662222027779], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.006045981775969267, -0.00037531962152570486, -5.817244164063595e-05, -0.0009504806366749108, -6.318072337307967e-06, -1.7881377516459906e-06, -0.16492736339569092, -0.00785015244036913], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.07241207361221313, -0.06834029406309128, -0.09959147125482559], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05334284529089928, -0.007530161179602146], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.005304903257638216, -1.7699916362762451, -1.4305012882687151e-05, -1.7881377516459906e-06, -1.0371154530730564e-05, -2.8490614567999728e-05, -0.14362657070159912], "finish_reason": "stop"}], "metrics": {"energy": 1297.7397513952249, "energy_per_token": 4.753625462986172, "time": 33.520607, "time_per_token": 0.12278610622710621}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.strip()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "strip", "()", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -2.5981218814849854, -0.014604452066123486, -0.4549829363822937], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -0.41199398040771484, -3.2186455882765586e-06, -4.6491513785440475e-06, -0.001842707279138267, -0.017574068158864975, 0.0, -8.4638240878121e-06, -2.145764938177308e-06, 0.0, 0.0, -0.19700554013252258, -0.016263552010059357], "finish_reason": "stop"}, {"text": " pd.Series(np.sum(vectorizer.fit_transform(df[\"Text\"]).toarray(), axis=0), index=vectorizer.get_feature_names_out()).sort_values(ascending=False)", "tokens": [" pd", ".", "Series", "(", "np", ".", "sum", "(", "vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", ",", " axis", "=", "0", ")", ",", " index", "=", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "())", ".", "sort", "_", "values", "(", "asc", "ending", "=", "False", ")", "\n"], "logprobs": [-1.7207505702972412, -7.116541382856667e-05, -0.2719030976295471, -0.007540218066424131, -2.4260318279266357, -1.1086402082582936e-05, -0.12671686708927155, -0.0030776537023484707, -0.27927371859550476, -0.0002859421947505325, -0.00011526874004630372, -0.006182000041007996, -2.1934269170742482e-05, -1.8596476365928538e-05, -0.001551615772768855, -0.03975491225719452, -0.1125091090798378, -0.001321276999078691, -0.0006463822210207582, -4.9828242481453344e-05, -0.019386224448680878, -0.00913062784820795, -0.019680002704262733, -4.768370445162873e-07, -0.0006027788622304797, -0.0009663202799856663, -0.004162814002484083, -9.500529267825186e-05, -0.00069165148306638, -0.6845689415931702, -0.004984808620065451, -0.3077791929244995, -0.0013680632691830397, -0.006419868674129248, -2.3841855067985307e-07, -0.00017975145601667464, -0.0009606037638150156, -2.622600959512056e-06, -1.6689160474925302e-05, 0.0, -1.4066597032069694e-05, -0.1721632182598114, -7.462222856702283e-05, -0.003029045881703496, -0.753239631652832, -0.18982481956481934, -6.556489552167477e-06, -4.577531944960356e-05, -0.008250203914940357, -0.010469986125826836, -1.1920928244535389e-07, -0.0003943857445847243, -0.001263773301616311, -0.041690643876791, -0.37261784076690674], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.3377721905708313, -2.0265558760002023e-06, -0.2113458514213562, -0.08696550875902176], "finish_reason": "stop"}, {"text": "\"The data is empty or contains only stop words.\")", "tokens": ["\"", "The", " data", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.057558294385671616, -1.2475522756576538, -3.030071258544922, -0.8979212045669556, -0.4208934009075165, -0.04268058016896248, -0.3047407865524292, -0.006684206426143646, -0.0113344332203269, -0.09163343161344528, -0.19071969389915466, -0.1849663257598877, -0.0005050813779234886, -0.000402250443585217], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07427504658699036, -0.01735582761466503], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.015684576705098152, -5.8412379075889476e-06, -1.2993727978027891e-05, -0.00010918975021922961, -0.00035172473872080445, -4.565611743601039e-05, -2.9444261599564925e-05, -0.00016091958968900144, -0.0009607228566892445, -0.0005926521262153983, -0.08168307691812515], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.010353073477745056, -0.018611092120409012, -0.014110793359577656, -0.083980992436409, -0.00023231192608363926, -0.0004557524516712874, -4.184158387943171e-05, -0.027635689824819565, -0.00040951924165710807, -0.022404644638299942, -0.028948074206709862, -0.0329645536839962, -0.0035159694962203503, -6.425174069590867e-05, -0.0016580180963501334, -0.0007150000892579556, -0.0012447236804291606, 0.0, -1.549708758830093e-05, -1.2755313036905136e-05, -0.00044169207103550434, -3.4570634852570947e-06, -9.285972191719338e-05, -5.125986263010418e-06, -6.317892984952778e-05, -0.004291847813874483, -0.001755483914166689], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.008418312296271324, -0.007923481054604053, -0.06423185765743256, -0.00012039413559250534, -0.000993830617517233, -0.00021205084340181202, -0.043868761509656906, -0.0003813969960901886, -0.021138962358236313, -0.01604197360575199, -0.8936886191368103], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.04377474635839462, -0.0013910388806834817, -0.0030882307328283787, -4.434487345861271e-05, -3.933898824470816e-06, -0.0009479796281084418, -0.004957645200192928, -0.0001528146385680884, -0.0008694920688867569, -0.002946917898952961, -0.00010108436981681734, -0.00670670485123992, -7.354942499659956e-05, -0.0003798478574026376, -0.006840857211500406, -1.5616295058862306e-05, -8.606540359323844e-05, -1.6569954823353328e-05, -0.005940400995314121, -0.02140072174370289, -0.15499092638492584], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.8108786344528198, -9.536738616588991e-07, -4.768370445162873e-07, -0.014347606338560581, -0.00658935122191906, -0.0023418639320880175, -0.002131691901013255, -0.03318391367793083], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0010881462367251515, -1.6689286894688848e-06, -1.537788011773955e-05, -0.012317304499447346, -0.1032133474946022, -3.576278118089249e-07, -3.2186455882765586e-06, -0.003202312160283327, -0.0008811643347144127, -0.0038316657301038504, -0.00026008085114881396, -0.6487334966659546, -0.0012515101116150618, -6.794906312279636e-06, -5.590759246842936e-05, -0.1170976385474205, -0.001534000039100647, -0.001508409040980041, -0.02397696115076542], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", legend=False)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.05203483626246452, -0.06703037023544312, -0.0011981697753071785, -0.5047981142997742, -0.0006934384000487626, -0.47237637639045715, -0.08000653982162476, -0.00013934595335740596, -4.362964682513848e-05, -0.00012981049076188356, -2.264974000354414e-06, -0.00302524259313941, -0.005980099085718393, -0.0018550821114331484, -9.894321920000948e-06, -9.059495641849935e-05, -1.3112935448589269e-05, -0.11411718279123306, -3.002758264541626, -0.00014399446081370115, -0.0033862413838505745, -0.27596941590309143, -0.0023242621682584286], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.026173060759902, -6.6756979322235566e-06, -1.7523612768854946e-05, -1.0150262117385864, -0.43546488881111145, -6.079655122448457e-06, -0.012308119796216488, -0.004926922265440226], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.005661168601363897, -0.0003351603518240154, -5.090107151772827e-05, -0.001213648240081966, -5.602820692729438e-06, -2.0265558760002023e-06, -0.1770634800195694, -0.0065071601420640945], "finish_reason": "stop"}, {"text": "close(ax.figure)", "tokens": ["close", "(", "ax", ".", "figure", ")", "\n"], "logprobs": [-0.07482904940843582, -2.627204656600952, -0.2139749825000763, -0.21713541448116302, -0.01646995358169079, -0.0008480527903884649, -0.18531592190265656], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.0528901070356369, -0.010379618965089321], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.005775194149464369, -0.6392155289649963], "finish_reason": "stop"}], "metrics": {"energy": 1247.5983222103107, "energy_per_token": 4.780070200039504, "time": 32.173145, "time_per_token": 0.12326875478927202}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.replace(\"[^\\w\\s]\", \"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "replace", "(", "\"", "[", "^\\", "w", "\\", "s", "]\"", ",", " \"", "\"", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8530066013336182, -0.03686662018299103, -0.9176292419433594, -0.6719261407852173, -0.4418778419494629, -0.002056509256362915, -0.014608211815357208, -0.0033687767572700977, -0.022391121834516525, -7.998623186722398e-05, -0.00933602824807167, -0.046057745814323425, -0.10099869966506958, -0.8507019281387329], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS, min_df=1)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " min", "_", "df", "=", "1", ")", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -0.41199398040771484, -3.2186455882765586e-06, -4.6491513785440475e-06, -0.001842707279138267, -0.017574068158864975, 0.0, -8.4638240878121e-06, -2.145764938177308e-06, 0.0, 0.0, -1.7437431812286377, -2.163450002670288, -5.602820692729438e-06, -1.7523612768854946e-05, -0.0012332937913015485, -0.7992579340934753, -0.2214736044406891, -0.01074787974357605], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.loc[:, \"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "loc", "[:", ",", " \"", "Text", "\"", "])", "\n"], "logprobs": [-0.4379560649394989, -4.1126360883936286e-05, -6.258291978156194e-05, -0.00910026952624321, -0.0036235882434993982, -0.0002586507180240005, -0.015056181699037552, -0.01636112853884697, -2.800536632537842, -2.620142698287964, -0.4433346092700958, -0.029963426291942596, -0.15855661034584045, -0.00031799983116798103, -6.472854875028133e-05, -0.3182778060436249, -0.3049294054508209], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.34000861644744873, -2.0265558760002023e-06, -0.19538046419620514, -0.07664170116186142], "finish_reason": "stop"}, {"text": "\"The input file is empty or contains only stop words.\")", "tokens": ["\"", "The", " input", " file", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.05205792188644409, -1.3041329383850098, -0.4016249179840088, -0.6958590745925903, -0.13349482417106628, -0.40999725461006165, -0.017795424908399582, -0.10055992007255554, -0.004872353747487068, -0.0041725486516952515, -0.02921115607023239, -0.11205755919218063, -0.07379946857690811, -0.00040844682371243834, -0.000259365770034492], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.06928630918264389, -0.01680128090083599], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.009980890899896622, -6.794906312279636e-06, -1.811964830267243e-05, -8.225102646974847e-05, -0.0006455483380705118, -5.23315102327615e-05, -2.9444261599564925e-05, -0.00018654513405635953, -0.0007520469953306019, -0.00042655906872823834, -0.06359902024269104], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.006777749862521887, -0.009011550806462765, -0.012265139259397984, -0.049352627247571945, -0.00034481301554478705, -0.00047195740626193583, -2.610649426060263e-05, -0.015084012411534786, -0.00024029705673456192, -0.026254581287503242, -0.020825881510972977, -0.03233642503619194, -0.005578189622610807, -5.2569914259947836e-05, -0.001293656532652676, -0.000710592488758266, -0.0006853376980870962, 0.0, -1.168244216387393e-05, -1.2278481335670222e-05, -0.00020883286197204143, -3.099436753473128e-06, -6.0437283536884934e-05, -6.198863957251888e-06, -7.188061863416806e-05, -0.0045225718058645725, -0.001878165639936924], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.010475766845047474, -0.006929413881152868, -0.05526014417409897, -0.00017391123401466757, -0.0012016226537525654, -0.00018571082910057157, -0.03267707675695419, -0.0003620922507252544, -0.022170664742588997, -0.01896463707089424, -1.0531471967697144], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.036482568830251694, -0.0011811431031674147, -0.001865434111095965, -4.2199197196168825e-05, -3.2186455882765586e-06, -0.0005881248507648706, -0.0074435509741306305, -0.00013469743134919554, -0.001135419704951346, -0.0034411284141242504, -9.738924563862383e-05, -0.004513196647167206, -6.031808152329177e-05, -0.00037722624256275594, -0.00666111521422863, -1.3112935448589269e-05, -0.00011324241495458409, -1.9788545614574105e-05, -0.005607114639133215, -0.02136058174073696, -0.10566414147615433], "finish_reason": "stop"}, {"text": " [word for word, freq in words_freq[:10]]", "tokens": [" [", "word", " for", " word", ",", " fre", "q", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.700725257396698, -0.6841026544570923, -0.4389130175113678, -0.07083941996097565, -0.0021075436379760504, -1.2070168256759644, -1.966933996300213e-05, -0.00027783826226368546, -0.0013162767281755805, -5.960462772236497e-07, -4.768370445162873e-07, -0.14186494052410126, -0.007639478426426649, -0.0034027553629130125, -0.014998053200542927, -0.0051416074857115746], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0010673070792108774, -2.50339189733495e-06, -1.0132738680113107e-05, -0.007014885079115629, -0.09053575992584229, -7.152555099310121e-07, -3.814689989667386e-06, -0.004251608159393072, -0.000996331567876041, -0.0037561357021331787, -0.00026794656878337264, -0.6201170682907104, -0.001260677701793611, -7.152531907195225e-06, -4.970903682988137e-05, -0.0909876674413681, -0.0016607552533969283, -0.0019382515456527472, -0.018306933343410492], "finish_reason": "stop"}, {"text": " df.plot(", "tokens": [" df", ".", "plot", "(", "\n"], "logprobs": [-0.05159565806388855, -0.058627430349588394, -0.0013080621138215065, -1.1913357973098755, -0.7842571139335632], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.030263828113675117, -8.4638240878121e-06, -1.8954096958623268e-05, -0.4912182092666626, -0.004231547005474567], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.007171010598540306, -0.00023326536756940186, -6.425174069590867e-05, -0.001303895260207355, -6.556489552167477e-06, -1.7881377516459906e-06, -0.1931658685207367, -0.007028734777122736], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.08272235095500946, -0.05308513343334198, -0.10386362671852112], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04469616338610649, -0.007256816141307354], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.005661049857735634, -0.5726336240768433], "finish_reason": "stop"}], "metrics": {"energy": 1164.7658332576752, "energy_per_token": 5.199847469900336, "time": 29.848633, "time_per_token": 0.13325282589285714}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -0.30648258328437805], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"].to_list())", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "]", ".", "to", "_", "list", "())", "\n"], "logprobs": [-0.40920716524124146, -2.7417760065873154e-05, -5.0424259825376794e-05, -0.008076036348938942, -0.004196765832602978, -0.00030155404238030314, -0.01675954833626747, -0.020926518365740776, -0.09813385456800461, -0.0014137759571895003, -0.0006649667047895491, -0.00011586471373448148, -1.9431697130203247, -0.009866044856607914, -2.7628533840179443, -5.3881147323409095e-05, -0.13085371255874634, -0.003987576812505722, -0.45815810561180115], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.41866397857666016, -1.6689286894688848e-06, -0.21533997356891632, -0.07270398736000061], "finish_reason": "stop"}, {"text": "\"Input file is either empty or only contains stop words.\")", "tokens": ["\"", "Input", " file", " is", " either", " empty", " or", " only", " contains", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.058882370591163635, -2.1248884201049805, -0.48413246870040894, -0.08086033165454865, -1.5293928384780884, -0.007411011029034853, -0.02673833630979061, -1.999967336654663, -0.06322846561670303, -0.020659729838371277, -0.08688190579414368, -0.25555330514907837, -0.058544907718896866, -0.000362807244528085, -0.00026639728457666934], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07874982804059982, -0.01581764593720436], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.01081686932593584, -8.940656698541716e-06, -1.966933996300213e-05, -9.738924563862383e-05, -0.00046433156239800155, -3.9457496313843876e-05, -2.1576648578047752e-05, -0.0001248043408850208, -0.0008880723617039621, -0.0004886387032456696, -0.06302265077829361], "finish_reason": "stop"}, {"text": " [(word, int(sum_words[0, idx])) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " int", "(", "sum", "_", "words", "[", "0", ",", " idx", "]))", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.010862741619348526, -0.013342027552425861, -0.017664842307567596, -2.7458295822143555, -0.002033072989434004, -0.008552944287657738, -0.00034314466756768525, -0.00039295581518672407, -6.067568756407127e-05, -0.0064897495321929455, -0.0007009433466009796, -0.05640001967549324, -0.04105355963110924, -0.08394350111484528, -0.019784726202487946, -4.875540980719961e-05, -0.0010093123419210315, -0.001479603466577828, -0.0011941214324906468, 0.0, -2.539125671319198e-05, -2.169585604860913e-05, -0.0004085659747943282, -2.264974000354414e-06, -7.533743337262422e-05, -5.602820692729438e-06, -7.247662142617628e-05, -0.006894844584167004, -0.0026246407069265842], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.009421411901712418, -0.00798734463751316, -0.04936329275369644, -0.00010513706365600228, -0.001279846066609025, -0.0001793938863556832, -0.053009163588285446, -0.0002488780301064253, -0.02407367341220379, -0.0197683647274971, -0.5492863655090332, -0.7648715972900391, -0.00014506718434859067, -0.0010026433737948537, -0.0007687236065976322, -0.014248069375753403, -1.1920928244535389e-07, -1.4305012882687151e-05, -1.3708974620385561e-05, -0.0002615109842736274, -2.145764938177308e-06, -1.9550132492440753e-05, -5.722029527532868e-06, -1.0967194612021558e-05, -0.012852098792791367], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.03201089799404144, -0.0011199399596080184, -0.0020158225670456886, -4.172238186583854e-05, -2.7418097943154862e-06, -0.0007619338575750589, -0.010976764373481274, -0.00011288482346571982, -0.001077428925782442, -0.003802689490839839, -8.034383063204587e-05, -0.006768277380615473, -6.067568756407127e-05, -0.00046456989366561174, -0.008283779956400394, -1.6331539882230572e-05, -7.795983401592821e-05, -2.288792165927589e-05, -0.007839389145374298, -0.020134469494223595, -2.4637796878814697, -0.006491407752037048, -0.001935396110638976, -0.0005211663665249944, -0.005461414810270071], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.829675018787384, -1.1920922133867862e-06, -4.768370445162873e-07, -0.018855499103665352, -0.006943264976143837, -0.0020611488725990057, -0.0023267597425729036, -0.033205244690179825], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Text\", \"Counts\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Text", "\"", ",", " \"", "Count", "s", "\"", "])", "\n"], "logprobs": [-0.0008771148277446628, -2.9802276912960224e-06, -1.0847986231965479e-05, -0.007335158996284008, -0.074459969997406, -5.960462772236497e-07, -3.933898824470816e-06, -0.00516912154853344, -0.0010804060148075223, -0.003658151952549815, -0.0003280101518612355, -2.6971395015716553, -0.004096450749784708, -3.0636318115284666e-05, -0.00027426297310739756, -0.3672693371772766, -1.6372146606445312, -0.002447825390845537, -0.0039026776794344187, -0.026100710034370422], "finish_reason": "stop"}, {"text": " df.plot(", "tokens": [" df", ".", "plot", "(", "\n"], "logprobs": [-0.04966938495635986, -0.061566632241010666, -0.0010243174619972706, -1.1167694330215454, -0.8671532273292542], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.041324682533741, -6.437280717364047e-06, -1.7165990357170813e-05, -1.0304765701293945, -0.3611971139907837, -8.583032467868179e-06, -0.013436715118587017, -0.0054759974591434], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0059650493785738945, -0.0003328961320221424, -5.9126061387360096e-05, -0.0015410225605592132, -6.437280717364047e-06, -2.7418097943154862e-06, -0.17296172678470612, -0.008692403323948383], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.07702267169952393, -0.08621829748153687, -0.0848214328289032], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04203440994024277, -0.010102331638336182], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.003950055688619614, -0.6086910963058472], "finish_reason": "stop"}], "metrics": {"energy": 1138.0869084014898, "energy_per_token": 5.0357827805375655, "time": 29.427872, "time_per_token": 0.13021182300884956}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].fillna(\"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "fill", "na", "(", "\"", "\"", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -1.9498767852783203, -2.3841855067985307e-07, -0.014480502344667912, -0.004825493786484003, -0.03064403124153614, -6.90197994117625e-05, -0.35571086406707764], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -1.618343710899353], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.Text)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "Text", ")", "\n"], "logprobs": [-0.40831175446510315, -3.433168603805825e-05, -5.781483559985645e-05, -0.010303991846740246, -0.0041359844617545605, -0.0002450642641633749, -0.01690947264432907, -0.016153765842318535, -2.7351748943328857, -0.27251046895980835, -0.145610511302948, -0.583045482635498], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.39566633105278015, -1.9073468138230965e-06, -0.2253197580575943, -0.07378319650888443], "finish_reason": "stop"}, {"text": "\"Input data is empty or contains only stop words.\")", "tokens": ["\"", "Input", " data", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.04857835918664932, -2.1838717460632324, -1.5813853740692139, -0.08068230748176575, -0.17789390683174133, -0.018453465774655342, -0.26672327518463135, -0.003884153440594673, -0.0072104232385754585, -0.06524084508419037, -0.18130910396575928, -0.13149043917655945, -0.000339569611242041, -0.00030727434204891324], "finish_reason": "stop"}, {"text": "", "tokens": ["\n"], "logprobs": [-2.517595052719116], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.008148521184921265, -6.437280717364047e-06, -1.3351351299206726e-05, -0.0001102625101339072, -0.0002733095607254654, -7.581423415103927e-05, -2.825220326485578e-05, -0.00015901254664640874, -0.0007161913090385497, -0.0006903410539962351, -0.09012069553136826], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.007733879145234823, -0.007426038850098848, -0.01172107644379139, -0.044442255049943924, -0.00016699827392585576, -0.0003250309091527015, -1.9430925021879375e-05, -0.011144062504172325, -0.00016127715934999287, -0.02846631407737732, -0.022187454625964165, -0.04462103545665741, -0.00266078463755548, -5.495397272170521e-05, -0.0019828674849122763, -0.0005091324565000832, -0.0012480573495849967, 0.0, -1.168244216387393e-05, -1.1801649634435307e-05, -0.0003997480380348861, -3.3378546504536644e-06, -8.701899787411094e-05, -4.529942543740617e-06, -6.353653589030728e-05, -0.004205313045531511, -0.0015605426160618663], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.00877559743821621, -0.006591127719730139, -0.05592048913240433, -0.00013374387344811112, -0.0012417471734806895, -0.00013505500101018697, -0.04182429984211922, -0.00032300499151460826, -0.017387928441166878, -0.017283186316490173, -0.5739281177520752, -0.682007908821106, -2.7417760065873154e-05, -0.006739859934896231, -0.00033885458833537996, -0.011454530991613865, -2.706014311115723e-05, -0.0018158152233809233, -1.1920928244535389e-07, -5.960446742392378e-06, -0.0008181562297977507, -1.1920928244535389e-07, -4.434487345861271e-05, 0.0, -7.033323527139146e-06, -0.4356043338775635, -0.0016814630944281816], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.034421321004629135, -0.0011566146276891232, -0.001825810642912984, -3.3854863431770355e-05, -2.0265558760002023e-06, -0.0008451942121610045, -0.005378893576562405, -9.97731985989958e-05, -0.0012290074955672026, -0.002342220861464739, -7.116541382856667e-05, -0.006388006266206503, -9.07141511561349e-05, -0.0003713871701620519, -0.008162236772477627, -1.3947389561508317e-05, -9.643566590966657e-05, -1.9788545614574105e-05, -0.005774601362645626, -0.023463455960154533, -0.12149535864591599], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.6807305812835693, -1.0728830375228426e-06, -2.3841855067985307e-07, -0.015050193294882774, -0.007065074983984232, -0.00233258749358356, -0.0022137677296996117, -0.024949699640274048], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0009740613750182092, -2.3841830625315197e-06, -1.7762025890988298e-05, -0.006360762286931276, -0.12911656498908997, -7.152555099310121e-07, -3.814689989667386e-06, -0.005923218093812466, -0.0010707604233175516, -0.0037384398747235537, -0.0002445875434204936, -1.116990089416504, -0.0004435985756572336, -1.7046782886609435e-05, -7.4741430580616e-05, -0.3874431252479553, -0.022825833410024643, -0.0014230611268430948, -0.003927257843315601, -0.01828083209693432], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.058656997978687286, -0.06414610892534256, -0.0012365083675831556, -0.32187891006469727, -0.0007801587926223874, -0.41938433051109314, -0.08633279800415039, -0.00012337400403339416, -5.7338023907504976e-05, -0.00013493580627255142, -1.9073468138230965e-06, -0.0043443115428090096, -0.0063432310707867146, -0.0016788449138402939, -8.821448318485636e-06, -0.00011097769311163574, -9.059865078597795e-06, -0.16235460340976715, -0.08957836776971817, -0.0012553200358524919, -0.06386041641235352, -0.2940462827682495, -0.005189401097595692], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.02903168462216854, -6.6756979322235566e-06, -1.537788011773955e-05, -0.44807544350624084, -0.005141725763678551], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.004664967302232981, -0.00021979777375236154, -6.472854875028133e-05, -0.0010093123419210315, -5.483612312673358e-06, -1.9073468138230965e-06, -0.18310607969760895, -0.0061776163056492805], "finish_reason": "stop"}, {"text": "close(fig)", "tokens": ["close", "(", "fig", ")", "\n"], "logprobs": [-0.0573599748313427, -2.6603591442108154, -2.555980682373047, -1.3362553119659424, -0.31223398447036743], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04730082303285599, -0.008194870315492153], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.004574310965836048, -0.6241136789321899], "finish_reason": "stop"}], "metrics": {"energy": 1103.288955352783, "energy_per_token": 5.0149497970581045, "time": 28.610012, "time_per_token": 0.1300455090909091}}
{"outputs": [{"text": " pd.read_csv(file_path)  # Read the CSV file into a DataFrame", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "  ", "#", " Read", " the", " CS", "V", " file", " into", " a", " Data", "Frame", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -2.8299341201782227, -0.00016282663273159415, -0.5233489274978638, -0.626197338104248, -0.16687114536762238, -1.0609570381348021e-05, -0.052112236618995667, -1.2539047002792358, -0.007967830635607243, -0.6577944755554199, -0.004448043182492256, -0.10372479259967804], "finish_reason": "stop"}, {"text": " df[\"Text\"].fillna(\"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "fill", "na", "(", "\"", "\"", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -1.9498767852783203, -2.3841855067985307e-07, -0.014480502344667912, -0.004825493786484003, -0.03064403124153614, -6.90197994117625e-05, -0.35571086406707764], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.40920716524124146, -2.7417760065873154e-05, -5.0424259825376794e-05, -0.008076036348938942, -0.004196765832602978, -0.00030155404238030314, -0.01675954833626747, -0.020926518365740776, -0.09813385456800461, -0.0014137759571895003, -0.0006649667047895491, -0.00011586471373448148, -0.15590016543865204, -0.5511701703071594], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.41866397857666016, -1.6689286894688848e-06, -0.21533997356891632, -0.07270398736000061], "finish_reason": "stop"}, {"text": "\"The input file is empty or contains only stop words.\")", "tokens": ["\"", "The", " input", " file", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.058882370591163635, -1.3733607530593872, -0.43683314323425293, -0.6032936573028564, -0.13253939151763916, -0.4078882336616516, -0.01779741607606411, -0.10523760318756104, -0.005926417652517557, -0.004316655918955803, -0.029218681156635284, -0.12486531585454941, -0.07381840795278549, -0.00041654970846138895, -0.0002580548170953989], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07874982804059982, -0.01581764593720436], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.01081686932593584, -8.940656698541716e-06, -1.966933996300213e-05, -9.738924563862383e-05, -0.00046433156239800155, -3.9457496313843876e-05, -2.1576648578047752e-05, -0.0001248043408850208, -0.0008880723617039621, -0.0004886387032456696, -0.06302265077829361], "finish_reason": "stop"}, {"text": " [(word, int(sum_words[0, idx])) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " int", "(", "sum", "_", "words", "[", "0", ",", " idx", "]))", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.010862741619348526, -0.013342027552425861, -0.017664842307567596, -2.7458295822143555, -0.002033072989434004, -0.008552944287657738, -0.00034314466756768525, -0.00039295581518672407, -6.067568756407127e-05, -0.0064897495321929455, -0.0007009433466009796, -0.05640001967549324, -0.04105355963110924, -0.08394350111484528, -0.019784726202487946, -4.875540980719961e-05, -0.0010093123419210315, -0.001479603466577828, -0.0011941214324906468, 0.0, -2.539125671319198e-05, -2.169585604860913e-05, -0.0004085659747943282, -2.264974000354414e-06, -7.533743337262422e-05, -5.602820692729438e-06, -7.247662142617628e-05, -0.006894844584167004, -0.0026246407069265842], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.009421411901712418, -0.00798734463751316, -0.04936329275369644, -0.00010513706365600228, -0.001279846066609025, -0.0001793938863556832, -0.053009163588285446, -0.0002488780301064253, -0.02407367341220379, -0.0197683647274971, -0.5492863655090332, -0.6425609588623047, -3.838465272565372e-05, -0.008101341314613819, -0.0004326361231505871, -0.015740200877189636, -3.421248038648628e-05, -0.0025234543718397617, -1.1920928244535389e-07, -9.65590606938349e-06, -0.000753476400859654, -1.1920928244535389e-07, -6.3774932641536e-05, 0.0, -5.960446742392378e-06, -0.2840566039085388, -0.0014023480471223593], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.04509507864713669, -0.001210671616718173, -0.0023670769296586514, -3.2782016205601394e-05, -3.2186455882765586e-06, -0.0010101459920406342, -0.006982212886214256, -0.00013016807497479022, -0.001105412608012557, -0.0031414704862982035, -0.00011145447206217796, -0.006253082305192947, -6.317892984952778e-05, -0.0004650464979931712, -0.007629777304828167, -1.3232143828645349e-05, -8.415821503149346e-05, -2.074220174108632e-05, -0.006294663064181805, -0.01541016437113285, -0.0860418826341629], "finish_reason": "stop"}, {"text": " [x[0] for x in words_freq[:10]]", "tokens": [" [", "x", "[", "0", "]", " for", " x", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.7050791382789612, -2.1546685695648193, -0.05089767277240753, -0.006503725424408913, -0.00035363141796551645, -0.0006376855308189988, -0.0012898467248305678, -8.987976616481319e-05, -0.0013019903562963009, -1.6689286894688848e-06, -8.344646857949556e-07, -0.2922210693359375, -0.005768675357103348, -0.00345918582752347, -0.017815684899687767, -0.011925119906663895], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0009763241396285594, -3.099436753473128e-06, -1.585470999998506e-05, -0.005819876212626696, -0.14673277735710144, -5.960462772236497e-07, -2.9802276912960224e-06, -0.003905290039256215, -0.0014861501986160874, -0.0035583768039941788, -0.0002833203470800072, -0.6480712890625, -0.001208409434184432, -9.059865078597795e-06, -5.98412734689191e-05, -0.0751611664891243, -0.001960857305675745, -0.0017561980057507753, -0.02610732987523079], "finish_reason": "stop"}, {"text": " df.plot.barh(x=\"Word\", y=\"Count\", figsize=(10, 6), legend=False)", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " fig", "size", "=(", "1", "0", ",", " ", "6", ")", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.05178731307387352, -0.07008939236402512, -0.001416275859810412, -0.43409231305122375, -0.0006074252305552363, -1.2587732076644897, -0.002479576040059328, -0.09020380675792694, -0.0002489972102921456, -5.924526340095326e-05, -0.006446874234825373, -4.172316494077677e-06, -0.001209838199429214, -0.0379699170589447, -0.0007621721015311778, -8.106198947643861e-06, -0.00011443436960689723, -1.4662635294371285e-05, -0.2052028924226761, -1.1858811378479004, -2.7418097943154862e-06, -0.002484808210283518, -0.11710760742425919, -0.03594968467950821, -0.0006113567505963147, -0.000251142424531281, -1.536389708518982, -0.1673247367143631, -2.8609820219571702e-05, -1.0082409381866455, -0.00014661667228210717, -0.00933697260916233, -0.11674659699201584, -0.001927424455061555], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03197591379284859, -7.152531907195225e-06, -1.5139465176616795e-05, -0.4537251889705658, -0.00432888139039278], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0058440533466637135, -0.0003457663697190583, -7.283422019099817e-05, -0.0011703077470883727, -5.722029527532868e-06, -1.9073468138230965e-06, -0.16135960817337036, -0.007242141291499138], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.07085762917995453, -0.0671306923031807, -0.08524560183286667], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04894871264696121, -0.009726394899189472], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.007741212844848633, -0.6501850485801697], "finish_reason": "stop"}], "metrics": {"energy": 1305.6019583158482, "energy_per_token": 4.945461963317607, "time": 33.617836, "time_per_token": 0.12734028787878787}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8732607364654541, -0.00027926836628466845, -0.3627833425998688], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -0.41199398040771484, -3.2186455882765586e-06, -4.6491513785440475e-06, -0.001842707279138267, -0.017574068158864975, 0.0, -8.4638240878121e-06, -2.145764938177308e-06, 0.0, 0.0, -0.19700554013252258, -0.016263552010059357], "finish_reason": "stop"}, {"text": " pd.Series(", "tokens": [" pd", ".", "Series", "(", "\n"], "logprobs": [-1.7207505702972412, -7.116541382856667e-05, -0.2719030976295471, -0.007540218066424131, -2.3936374187469482], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.3377721905708313, -2.0265558760002023e-06, -0.2113458514213562, -0.08696550875902176], "finish_reason": "stop"}, {"text": "\"Input is empty or contains only stop words.\")", "tokens": ["\"", "Input", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.057558294385671616, -2.2869162559509277, -2.2982654571533203, -0.36691415309906006, -0.029324494302272797, -0.3163072168827057, -0.006227848585695028, -0.010680772364139557, -0.06677660346031189, -0.36223578453063965, -0.1861885040998459, -0.0003673355677165091, -0.0004291805380489677], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07427504658699036, -0.01735582761466503], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.015684576705098152, -5.8412379075889476e-06, -1.2993727978027891e-05, -0.00010918975021922961, -0.00035172473872080445, -4.565611743601039e-05, -2.9444261599564925e-05, -0.00016091958968900144, -0.0009607228566892445, -0.0005926521262153983, -0.08168307691812515], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.010353073477745056, -0.018611092120409012, -0.014110793359577656, -0.083980992436409, -0.00023231192608363926, -0.0004557524516712874, -4.184158387943171e-05, -0.027635689824819565, -0.00040951924165710807, -0.022404644638299942, -0.028948074206709862, -0.0329645536839962, -0.0035159694962203503, -6.425174069590867e-05, -0.0016580180963501334, -0.0007150000892579556, -0.0012447236804291606, 0.0, -1.549708758830093e-05, -1.2755313036905136e-05, -0.00044169207103550434, -3.4570634852570947e-06, -9.285972191719338e-05, -5.125986263010418e-06, -6.317892984952778e-05, -0.004291847813874483, -0.001755483914166689], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names_out())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "())", "\n"], "logprobs": [-0.008418312296271324, -0.007923481054604053, -0.06423185765743256, -0.00012039413559250534, -0.000993830617517233, -0.00021205084340181202, -0.043868761509656906, -0.0003813969960901886, -0.021138962358236313, -0.01604197360575199, -0.5295338034629822, -0.7510894536972046, -3.8265450712060556e-05, -0.008877807296812534, -0.00044228785554878414, -0.02468828670680523, -3.4927710657939315e-05, -0.003112830687314272, -1.1920928244535389e-07, -8.583032467868179e-06, -0.000552263343706727, -2.3841855067985307e-07, -5.781483559985645e-05, 0.0, -6.318072337307967e-06, -1.1890037059783936, -0.010885263793170452, -0.0016263603465631604, -0.004135865718126297], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03297066688537598, -0.0012730597518384457, -0.0022826348431408405, -3.755022044060752e-05, -3.6954811548639555e-06, -0.0007930232677608728, -0.004922651685774326, -0.00010918975021922961, -0.0012884180760011077, -0.003105700248852372, -0.00011228884250158444, -0.007003521081060171, -5.864924969500862e-05, -0.00045563330058939755, -0.006819900590926409, -1.4781842764932662e-05, -0.00011598391574807465, -2.396077979938127e-05, -0.004933209158480167, -0.01924859918653965, -0.10475172847509384], "finish_reason": "stop"}, {"text": " [word for word, freq in words_freq[:10]]", "tokens": [" [", "word", " for", " word", ",", " fre", "q", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.8513798713684082, -0.5889737606048584, -0.4391010105609894, -0.07629803568124771, -0.002896758494898677, -0.8918699622154236, -2.8490614567999728e-05, -0.00022265815641731024, -0.0019233791390433908, -5.960462772236497e-07, -4.768370445162873e-07, -0.12441688776016235, -0.007849442772567272, -0.00339788431301713, -0.013665338978171349, -0.006938056088984013], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0008286379743367434, -3.4570634852570947e-06, -2.4914430468925275e-05, -0.00644794013351202, -0.09983553737401962, -4.768370445162873e-07, -3.4570634852570947e-06, -0.0044417534954845905, -0.001142206834629178, -0.003447899827733636, -0.0003307510633021593, -0.6784581542015076, -0.0010308672208338976, -6.318072337307967e-06, -4.970903682988137e-05, -0.10646513104438782, -0.001957288011908531, -0.0013448490062728524, -0.019418727606534958], "finish_reason": "stop"}, {"text": " df[\"Count\"].plot(", "tokens": [" df", "[", "\"", "Count", "\"", "]", ".", "plot", "(", "\n"], "logprobs": [-0.05041050538420677, -2.7115869522094727, -0.000993711524643004, -0.01146066002547741, -3.814689989667386e-06, -0.0009585791267454624, -0.00047350639943033457, -0.015516159124672413, -0.17116738855838776, -0.19642536342144012], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.022121110931038857, -6.318072337307967e-06, -2.729855441430118e-05, -0.4709233045578003, -0.004353569354861975], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0069320183247327805, -0.0003259842633269727, -6.627816765103489e-05, -0.001209242851473391, -4.6491513785440475e-06, -2.264974000354414e-06, -0.1611543744802475, -0.008732110261917114], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.08922222256660461, -0.07474320381879807, -0.08804679661989212], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.06675876677036285, -0.009082785807549953], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.004991569556295872, -0.6430912613868713], "finish_reason": "stop"}], "metrics": {"energy": 1140.648653120994, "energy_per_token": 5.280780801486083, "time": 30.25649, "time_per_token": 0.14007634259259258}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.replace(r'[^\\w\\s]', '').str.replace('\\d', '').str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "replace", "(", "r", "'", "[", "^\\", "w", "\\", "s", "]", "'", ",", " '", "')", ".", "str", ".", "replace", "('", "\\", "d", "'", ",", " '", "')", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8530066013336182, -0.03686662018299103, -0.5351066589355469, -2.998760223388672, -0.15806522965431213, -0.10659544169902802, -0.005211459007114172, -0.00548666762188077, -0.0023173640947788954, -0.0685105174779892, -0.0041642384603619576, -0.00012706902634818107, -0.5054711699485779, -0.237039253115654, -1.621251106262207, -0.04785337671637535, -0.0004978132783435285, -1.129089117050171, -0.6229743957519531, -0.489063024520874, -0.29067498445510864, -0.08071793615818024, -0.00011979816190432757, -0.06815346330404282, -0.0039411503821611404, -0.2922901511192322, -0.011636242270469666, -0.0004303721070755273, -0.7121877074241638, -0.0037013848777860403, -0.47292059659957886], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -1.6521062850952148], "finish_reason": "stop"}, {"text": " pd.Series(", "tokens": [" pd", ".", "Series", "(", "\n"], "logprobs": [-1.7048015594482422, -6.115249561844394e-05, -0.2346159964799881, -0.006272391881793737, -1.7236499786376953], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.3947112262248993, -2.0265558760002023e-06, -0.20360705256462097, -0.08278786391019821], "finish_reason": "stop"}, {"text": "\"No valid words found in the input. Please provide a valid input.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " input", ".", " Please", " provide", " a", " valid", " input", ".", "\"", ")", "\n"], "logprobs": [-0.05465219169855118, -1.4365090131759644, -0.34271764755249023, -0.5099523067474365, -0.2166391760110855, -0.11545771360397339, -0.051349613815546036, -0.15052282810211182, -0.8152585625648499, -2.0326457023620605, -1.4260395765304565, -1.0905416011810303, -0.2092496156692505, -1.7648073434829712, -0.4663967192173004, -0.0021377585362643003, -0.0002431573811918497, -0.00020168177434243262], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07446572184562683, -0.01575029268860817], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.011827344074845314, -9.179073458653875e-06, -1.823885577323381e-05, -9.703165414975956e-05, -0.0004037995240651071, -5.1616290875244886e-05, -4.303362584323622e-05, -0.00017033556650858372, -0.0010433712741360068, -0.0006246999255381525, -0.08802060037851334], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.010524485260248184, -0.014230675995349884, -0.012055394239723682, -0.07537566870450974, -0.00037674958002753556, -0.00031883400515653193, -3.194758028257638e-05, -0.034992773085832596, -0.0005949157639406621, -0.029974069446325302, -0.023658830672502518, -0.042048580944538116, -0.005660338792949915, -5.936446541454643e-05, -0.0011147005716338754, -0.0007781338645145297, -0.0009053422254510224, 0.0, -1.549708758830093e-05, -9.775113539944869e-06, -0.00030620177858509123, -3.814689989667386e-06, -6.794698856538162e-05, -5.8412379075889476e-06, -6.294052582234144e-05, -0.004163170233368874, -0.0018725732807070017], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.009523083455860615, -0.0101201506331563, -0.06415259838104248, -0.00015805903240107, -0.0011917401570826769, -0.0001501924270996824, -0.03841982036828995, -0.0005289109540171921, -0.029035968706011772, -0.029302731156349182, -0.8334839344024658], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.033184949308633804, -0.0011818574275821447, -0.0020384264644235373, -3.671578815556131e-05, -2.50339189733495e-06, -0.0006834316882304847, -0.007827679626643658, -0.00011824862303910777, -0.0011188682401552796, -0.0028241295367479324, -9.119095193454996e-05, -0.006494960747659206, -5.924526340095326e-05, -0.0003871168300975114, -0.005402251612395048, -1.0490362910786644e-05, -8.964136941358447e-05, -1.680836794548668e-05, -0.005256047938019037, -0.017913930118083954, -2.1251745223999023, -0.00635626120492816, -0.0019437245791777968, -0.0006151691195555031, -0.004937479272484779], "finish_reason": "stop"}, {"text": " [word[0] for word in words_freq[:10]]", "tokens": [" [", "word", "[", "0", "]", " for", " word", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.7802918553352356, -0.6062749028205872, -1.628554105758667, -0.00791141763329506, -0.000341476290486753, -0.0009563163621351123, -0.0020393780432641506, -0.00014029949670657516, -0.0015238827327266335, -1.7881377516459906e-06, -5.960462772236497e-07, -0.28128132224082947, -0.007761440239846706, -0.003301528748124838, -0.025024808943271637, -0.00830470584332943], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0009976415894925594, -2.0265558760002023e-06, -1.9788545614574105e-05, -0.006554177962243557, -0.14964348077774048, -5.960462772236497e-07, -4.172316494077677e-06, -0.005036879796534777, -0.0010125277331098914, -0.0031627416610717773, -0.0001941730733960867, -1.037755012512207, -0.000506511190906167, -1.3947389561508317e-05, -7.855583680793643e-05, -0.38671740889549255, -0.02426695078611374, -0.0010066924151033163, -0.0034690459724515676, -0.015081428922712803], "finish_reason": "stop"}, {"text": " df.plot.barh(x=\"Word\", y=\"Count\", legend=False, figsize=(10, 7), color=\"blue\")", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " legend", "=", "False", ",", " fig", "size", "=(", "1", "0", ",", " ", "7", ")", ",", " color", "=", "\"", "blue", "\"", ")", "\n"], "logprobs": [-0.04782098904252052, -0.06548014283180237, -0.0014906733995303512, -0.4755094051361084, -0.0006193388253450394, -1.1098606586456299, -0.0021305023692548275, -0.08823966979980469, -0.00026341783814132214, -6.747018051100895e-05, -0.004941512830555439, -3.933898824470816e-06, -0.0007764662150293589, -0.036189544945955276, -0.0004968600696884096, -6.9141146923357155e-06, -0.0001501924270996824, -1.847726889536716e-05, -0.25072428584098816, -0.4122365713119507, -2.074220174108632e-05, -0.003208491252735257, -1.0983631610870361, -0.08043330907821655, -1.2159273865108844e-05, -0.005222369451075792, -0.03756102919578552, -0.07439888268709183, -0.00035398892941884696, -0.00016115797916427255, -1.9384145736694336, -0.326421320438385, -7.223821739898995e-05, -0.21380962431430817, -0.001772262854501605, -0.001493292162194848, -1.5602350234985352, -0.0007407305529341102, -0.004299444612115622, -0.0006860524881631136], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03399934247136116, -5.483612312673358e-06, -1.6927575416048057e-05, -0.41048163175582886, -0.004650491289794445], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.00726001150906086, -0.00024232311989180744, -6.282132380874828e-05, -0.0014197280397638679, -5.602820692729438e-06, -2.3841830625315197e-06, -0.1763646900653839, -0.007911890745162964], "finish_reason": "stop"}, {"text": "close()  # Close the plot to free memory", "tokens": ["close", "()", "  ", "#", " Close", " the", " plot", " to", " free", " memory", "\n"], "logprobs": [-0.08326862007379532, -0.07011806964874268, -2.3725051879882812, -8.583032467868179e-06, -0.47039419412612915, -0.020290780812501907, -1.3495514392852783, -1.3072322607040405, -0.7045826315879822, -0.2370944619178772, -0.07989461719989777], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.049496836960315704, -0.010965446010231972], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.005596801172941923], "finish_reason": "stop"}], "metrics": {"energy": 1297.7749266281126, "energy_per_token": 4.953339414611117, "time": 33.026814, "time_per_token": 0.12605654198473282}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -0.30648258328437805], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " dict(zip(vectorizer.get_feature_names_out(), vectorizer.fit_transform(df[\"Text\"]).sum(axis=0).A1))", "tokens": [" dict", "(", "zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "()", ",", " vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "sum", "(", "axis", "=", "0", ")", ".", "A", "1", "))", "\n"], "logprobs": [-2.89172625541687, -0.009605735540390015, -0.4814111888408661, -0.12914307415485382, -0.09738002717494965, -7.152555099310121e-07, -0.00011395759065635502, -0.010413359850645065, -8.344646857949556e-07, -4.95898348162882e-05, 0.0, -3.909988299710676e-05, -0.4382266104221344, -0.0008579387213103473, -0.0074860285967588425, -0.0020974320359528065, -0.1799919158220291, -9.417489309271332e-06, -0.00022754464589525014, -0.02972368709743023, -0.0014540109550580382, -1.9907753085135482e-05, -0.009577162563800812, -0.028808748349547386, -0.11629249900579453, -0.0013852057745680213, -0.00048720886115916073, -0.0001616347290109843, -0.08566948026418686, -0.0019339682767167687, -0.49087709188461304, -1.9550132492440753e-05, -0.002179153962060809, -0.0001560327800689265, -0.00041762212640605867, -0.0008630603551864624, -0.002139661693945527, -1.0678699016571045, -0.004445550963282585, -0.03854932636022568, -0.005329211708158255], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.4593242406845093, -1.6689286894688848e-06, -0.17356830835342407, -0.08921404182910919], "finish_reason": "stop"}, {"text": "\"No valid words found in the input data.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " input", " data", ".", "\"", ")", "\n"], "logprobs": [-0.04757412523031235, -1.4227912425994873, -0.3046378791332245, -0.45835140347480774, -0.23235581815242767, -0.13551779091358185, -0.05096009746193886, -0.13703587651252747, -1.4786832332611084, -0.14818450808525085, -0.09944751113653183, -0.0003718638326972723, -0.0005229535745456815], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05922769010066986, -0.015079667791724205], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.00796700268983841, -6.794906312279636e-06, -1.4543427823809907e-05, -7.855583680793643e-05, -0.0003123987407889217, -3.135155202471651e-05, -1.6569954823353328e-05, -0.0001525762490928173, -0.0007110689766705036, -0.0005154472892172635, -0.06514076888561249], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.00543414568528533, -0.013065796345472336, -0.016159160062670708, -0.0688130259513855, -0.00021491125517059118, -0.0002795067266561091, -2.884823152271565e-05, -0.017424948513507843, -0.00042429505265317857, -0.022428425028920174, -0.018940424546599388, -0.04329904913902283, -0.0028033265843987465, -5.972207145532593e-05, -0.0014188947388902307, -0.0007331067463383079, -0.001091599464416504, 0.0, -1.490105023549404e-05, -1.4066597032069694e-05, -0.00037722624256275594, -3.3378546504536644e-06, -7.271502545336261e-05, -5.245195097813848e-06, -5.94836674281396e-05, -0.0037216939963400364, -0.0016906267264857888], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.008976107463240623, -0.007815022952854633, -0.05850207060575485, -0.00011503035057103261, -0.0009845414897426963, -0.00014530557382386178, -0.042018406093120575, -0.0004357342259027064, -0.02206479012966156, -0.013842885382473469, -0.9154359102249146], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03141867741942406, -0.0009756095823831856, -0.0025367720518261194, -5.519237674889155e-05, -3.576272320060525e-06, -0.0007926659309305251, -0.0068947263062000275, -0.00010907054820563644, -0.00100419158115983, -0.0025880197063088417, -8.940297266235575e-05, -0.006001428235322237, -5.447716102935374e-05, -0.0003405229654163122, -0.005904612597078085, -1.1444026313256472e-05, -7.426462980220094e-05, -1.8000440832111053e-05, -0.00554167665541172, -0.016482265666127205, -0.12278091907501221], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.6496015191078186, -8.344646857949556e-07, -2.3841855067985307e-07, -0.014895644970238209, -0.005854838062077761, -0.0020905323326587677, -0.002155839465558529, -0.02733669988811016], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0009887097403407097, -2.264974000354414e-06, -8.940656698541716e-06, -0.008234478533267975, -0.09388452768325806, -5.960462772236497e-07, -3.814689989667386e-06, -0.003291904693469405, -0.0008431693422608078, -0.0046300822868943214, -0.000278195773717016, -0.5674112439155579, -0.0013819915475323796, -5.8412379075889476e-06, -5.602679812000133e-05, -0.06401462107896805, -0.0019858418963849545, -0.0013566347770392895, -0.027429142966866493], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.0644741952419281, -0.05978301540017128, -0.0010122895473614335, -0.346736341714859, -0.0005606033373624086, -0.4100383222103119, -0.06363627314567566, -0.00013207517622504383, -4.207999518257566e-05, -0.00013684290752280504, -1.6689286894688848e-06, -0.003540439996868372, -0.005504806991666555, -0.0019437245791777968, -8.940656698541716e-06, -8.546940807718784e-05, -1.1801649634435307e-05, -0.18988515436649323, -0.0930112674832344, -0.000996331567876041, -0.04970863088965416, -0.40614867210388184, -0.0052878279238939285], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03404554724693298, -6.556489552167477e-06, -2.7417760065873154e-05, -0.46478649973869324, -0.00410571089014411], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.006516753230243921, -0.00027497802511788905, -6.985420623095706e-05, -0.0012379371328279376, -5.722029527532868e-06, -2.861018856492592e-06, -0.17545945942401886, -0.010199449956417084], "finish_reason": "stop"}, {"text": "close()  # Close the figure to free memory", "tokens": ["close", "()", "  ", "#", " Close", " the", " figure", " to", " free", " memory", "\n"], "logprobs": [-0.07328478246927261, -0.08798807114362717, -2.364006519317627, -9.417489309271332e-06, -0.48027002811431885, -0.016783345490694046, -0.38616088032722473, -0.7124767899513245, -0.3858203589916229, -0.24838104844093323, -0.06806626915931702], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.047075897455215454, -0.008800175972282887], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.005431655794382095], "finish_reason": "stop"}], "metrics": {"energy": 1262.7206657619474, "energy_per_token": 5.112229416040273, "time": 32.8272, "time_per_token": 0.13290364372469635}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].fillna(\"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "fill", "na", "(", "\"", "\"", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -1.9498767852783203, -2.3841855067985307e-07, -0.014480502344667912, -0.004825493786484003, -0.03064403124153614, -6.90197994117625e-05, -0.35571086406707764], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray().sum(axis=0)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.40920716524124146, -2.7417760065873154e-05, -5.0424259825376794e-05, -0.008076036348938942, -0.004196765832602978, -0.00030155404238030314, -0.01675954833626747, -0.020926518365740776, -0.09813385456800461, -0.0014137759571895003, -0.0006649667047895491, -0.00011586471373448148, -0.15590016543865204, -0.8676221966743469, -0.7734647989273071, -2.396077979938127e-05, -0.0014028241857886314, -0.13641078770160675, -0.036403946578502655, -0.0006859333370812237, -0.002000356325879693, -0.00036042393185198307, -0.00026639728457666934, -0.0012388896429911256, -0.008032754994928837], "finish_reason": "stop"}, {"text": " ValueError as e:", "tokens": [" Value", "Error", " as", " e", ":", "\n"], "logprobs": [-0.41866397857666016, -1.6689286894688848e-06, -1.6639999151229858, -0.310068815946579, -0.0007960011716932058, -0.011192395351827145], "finish_reason": "stop"}, {"text": "\"No valid words found in the input. Please check the file.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " input", ".", " Please", " check", " the", " file", ".", "\"", ")", "\n"], "logprobs": [-0.05769367143511772, -1.392688512802124, -0.3147001564502716, -0.5313280820846558, -0.2161111831665039, -0.11298973858356476, -0.05004296079277992, -0.13105784356594086, -0.8187887072563171, -1.907679557800293, -0.4441087543964386, -0.28271880745887756, -2.5869765281677246, -0.6376456022262573, -0.0016501632053405046, -0.00023684080224484205, -0.00025018901214934886], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.08910434693098068, -0.016338493674993515], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.0125802056863904, -7.867782187531702e-06, -1.7762025890988298e-05, -0.0001072826053132303, -0.0004478881892282516, -4.8636207793606445e-05, -2.0265373677830212e-05, -0.00015484087634831667, -0.0007148809381760657, -0.0007987407734617591, -0.10545068234205246], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.004271787591278553, -0.007240721024572849, -0.010798354633152485, -0.05530673265457153, -0.00017188502533826977, -0.0002908283786382526, -5.006664650863968e-05, -0.03479476645588875, -0.0002213471452705562, -0.020085398107767105, -0.023310784250497818, -0.04366382956504822, -0.003298320807516575, -5.936446541454643e-05, -0.0019349202048033476, -0.0007107115816324949, -0.0009704885887913406, 0.0, -1.2516897186287679e-05, -1.0967194612021558e-05, -0.0003632839070633054, -3.2186455882765586e-06, -8.451581379631534e-05, -4.529942543740617e-06, -6.103329360485077e-05, -0.00451640086248517, -0.0016089839627966285], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.010322515852749348, -0.006641339045017958, -0.04995950311422348, -0.00011967896716669202, -0.001111485529690981, -0.00013636612857226282, -0.04189346358180046, -0.00045789722935296595, -0.024241702631115913, -0.03176656737923622, -0.5803139209747314, -0.7391542792320251, -0.00010787858627736568, -0.0012190061388537288, -0.0008001701789908111, -0.013582080602645874, -1.1920928244535389e-07, -1.3232143828645349e-05, -2.3841574147809297e-05, -0.0003875934926327318, -1.4305104514278355e-06, -1.490105023549404e-05, -5.722029527532868e-06, -1.0251946150674485e-05, -0.010648337192833424], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.029424510896205902, -0.0010448002722114325, -0.0014065144350752234, -4.541770613286644e-05, -3.576272320060525e-06, -0.000582525331992656, -0.010904248803853989, -0.00010573305189609528, -0.0011248220689594746, -0.0035898545756936073, -7.295342220459133e-05, -0.007789355702698231, -7.772143726469949e-05, -0.00029202012228779495, -0.006979608442634344, -1.4543427823809907e-05, -9.822363062994555e-05, -1.7762025890988298e-05, -0.0059419418685138226, -0.016914160922169685, -0.08699840307235718], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.895256519317627, -1.0728830375228426e-06, -4.768370445162873e-07, -0.01779097504913807, -0.007696261629462242, -0.002186290919780731, -0.001969899283722043, -0.03420879691839218], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0008592489175498486, -3.099436753473128e-06, -1.0847986231965479e-05, -0.0073078228160738945, -0.10895682871341705, -7.152555099310121e-07, -4.0531076592742465e-06, -0.004782191012054682, -0.0011704268399626017, -0.004362115170806646, -0.00034028460504487157, -0.6737402677536011, -0.0010799296433106065, -8.4638240878121e-06, -4.792098479811102e-05, -0.12402312457561493, -0.002124911407008767, -0.0016381428577005863, -0.02781691774725914], "finish_reason": "stop"}, {"text": " df.plot(", "tokens": [" df", ".", "plot", "(", "\n"], "logprobs": [-0.04589368402957916, -0.06245982646942139, -0.0012246021069586277, -1.032654047012329, -0.7771171927452087], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.02708391472697258, -7.033323527139146e-06, -2.3245540432981215e-05, -0.4425675868988037, -0.004588194657117128], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0056069958955049515, -0.0002719986077863723, -6.69933797325939e-05, -0.0010729038622230291, -6.198863957251888e-06, -2.264974000354414e-06, -0.17333467304706573, -0.007180124055594206], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.07108671218156815, -0.06025116145610809, -0.1158856526017189], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.03497124835848808, -0.010653644800186157], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.005847608670592308, -0.6350470781326294], "finish_reason": "stop"}], "metrics": {"energy": 1108.8621911849975, "energy_per_token": 4.88485546777532, "time": 28.752794, "time_per_token": 0.12666429074889868}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].fillna(\"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "fill", "na", "(", "\"", "\"", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -1.9498767852783203, -2.3841855067985307e-07, -0.014480502344667912, -0.004825493786484003, -0.03064403124153614, -6.90197994117625e-05, -0.35571086406707764], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.40920716524124146, -2.7417760065873154e-05, -5.0424259825376794e-05, -0.008076036348938942, -0.004196765832602978, -0.00030155404238030314, -0.01675954833626747, -0.020926518365740776, -0.09813385456800461, -0.0014137759571895003, -0.0006649667047895491, -0.00011586471373448148, -0.15590016543865204, -0.5511701703071594], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.41866397857666016, -1.6689286894688848e-06, -0.21533997356891632, -0.07270398736000061], "finish_reason": "stop"}, {"text": "\"Input file is empty or contains only stop words.\")", "tokens": ["\"", "Input", " file", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.058882370591163635, -2.1248884201049805, -0.48413246870040894, -0.08086033165454865, -0.25056329369544983, -0.01809646189212799, -0.14576177299022675, -0.007037849631160498, -0.004465489182621241, -0.03932371735572815, -0.30386120080947876, -0.0908811017870903, -0.0003511289251036942, -0.0003077510336879641], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07874982804059982, -0.01581764593720436], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.01081686932593584, -8.940656698541716e-06, -1.966933996300213e-05, -9.738924563862383e-05, -0.00046433156239800155, -3.9457496313843876e-05, -2.1576648578047752e-05, -0.0001248043408850208, -0.0008880723617039621, -0.0004886387032456696, -0.06302265077829361], "finish_reason": "stop"}, {"text": " [(word, int(sum_words[0, idx])) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " int", "(", "sum", "_", "words", "[", "0", ",", " idx", "]))", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.010862741619348526, -0.013342027552425861, -0.017664842307567596, -2.7458295822143555, -0.002033072989434004, -0.008552944287657738, -0.00034314466756768525, -0.00039295581518672407, -6.067568756407127e-05, -0.0064897495321929455, -0.0007009433466009796, -0.05640001967549324, -0.04105355963110924, -0.08394350111484528, -0.019784726202487946, -4.875540980719961e-05, -0.0010093123419210315, -0.001479603466577828, -0.0011941214324906468, 0.0, -2.539125671319198e-05, -2.169585604860913e-05, -0.0004085659747943282, -2.264974000354414e-06, -7.533743337262422e-05, -5.602820692729438e-06, -7.247662142617628e-05, -0.006894844584167004, -0.0026246407069265842], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.009421411901712418, -0.00798734463751316, -0.04936329275369644, -0.00010513706365600228, -0.001279846066609025, -0.0001793938863556832, -0.053009163588285446, -0.0002488780301064253, -0.02407367341220379, -0.0197683647274971, -0.5492863655090332, -0.6425609588623047, -3.838465272565372e-05, -0.008101341314613819, -0.0004326361231505871, -0.015740200877189636, -3.421248038648628e-05, -0.0025234543718397617, -1.1920928244535389e-07, -9.65590606938349e-06, -0.000753476400859654, -1.1920928244535389e-07, -6.3774932641536e-05, 0.0, -5.960446742392378e-06, -0.2840566039085388, -0.0014023480471223593], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.04509507864713669, -0.001210671616718173, -0.0023670769296586514, -3.2782016205601394e-05, -3.2186455882765586e-06, -0.0010101459920406342, -0.006982212886214256, -0.00013016807497479022, -0.001105412608012557, -0.0031414704862982035, -0.00011145447206217796, -0.006253082305192947, -6.317892984952778e-05, -0.0004650464979931712, -0.007629777304828167, -1.3232143828645349e-05, -8.415821503149346e-05, -2.074220174108632e-05, -0.006294663064181805, -0.01541016437113285, -0.0860418826341629], "finish_reason": "stop"}, {"text": " [word for word, count in words_freq[:10]]", "tokens": [" [", "word", " for", " word", ",", " count", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.7050791382789612, -0.5578190684318542, -0.4963679015636444, -0.0666007399559021, -0.002104212762787938, -1.3580169677734375, -8.856858039507642e-05, -0.0010754045797511935, -1.311301275563892e-06, -3.576278118089249e-07, -0.2041928917169571, -0.007844356819987297, -0.004103455226868391, -0.014226444996893406, -0.005282728932797909], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Text\", \"Count\"]).sort_values(by=\"Count\")", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Text", "\"", ",", " \"", "Count", "\"", "])", ".", "sort", "_", "values", "(", "by", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.0009763241396285594, -3.099436753473128e-06, -1.585470999998506e-05, -0.005819876212626696, -0.14673277735710144, -5.960462772236497e-07, -2.9802276912960224e-06, -0.003905290039256215, -0.0014861501986160874, -0.0035583768039941788, -0.0002833203470800072, -2.6179962158203125, -0.004101437050849199, -2.753696753643453e-05, -0.00022432672267314047, -0.33224472403526306, -0.15822897851467133, -0.002654958749189973, -2.9134957790374756, -0.43834203481674194, -9.60780744208023e-05, -0.0001262346631847322, -0.0020146328024566174, -1.6593971252441406, -0.03864991292357445, -3.099393507000059e-05, -0.00878091435879469, -4.291525328881107e-06, -0.13432835042476654, -0.002855036174878478], "finish_reason": "stop"}, {"text": " df.plot(", "tokens": [" df", ".", "plot", "(", "\n"], "logprobs": [-0.05178731307387352, -0.07008939236402512, -0.001416275859810412, -1.0438640117645264, -0.7934136390686035], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03371066227555275, -6.794906312279636e-06, -2.169585604860913e-05, -0.42948848009109497, -0.005233516450971365], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.005630822852253914, -0.0003668589051812887, -6.854299135738984e-05, -0.0013916341122239828, -5.960446742392378e-06, -1.9073468138230965e-06, -0.17442156374454498, -0.007848851382732391], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.07431809604167938, -0.06845796853303909, -0.10293462127447128], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05049392580986023, -0.012512161396443844], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.006504672579467297, -0.6568671464920044], "finish_reason": "stop"}], "metrics": {"energy": 1161.2729915943141, "energy_per_token": 4.984004255769588, "time": 30.03724, "time_per_token": 0.12891519313304722}}
{"outputs": [{"text": " pd.read_csv(file_path, dtype=str, engine='c')", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " dtype", "=", "str", ",", " engine", "='", "c", "')", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -2.093414783477783, -2.254937171936035, -0.10474582761526108, -0.2925608456134796, -1.5808390378952026, -2.4058122634887695, -0.7457588315010071, -0.2678743600845337, -0.2353919893503189, -0.19477570056915283], "finish_reason": "stop"}, {"text": " df[\"Text\"].fillna(\"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "fill", "na", "(", "\"", "\"", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -1.9498767852783203, -2.3841855067985307e-07, -0.014480502344667912, -0.004825493786484003, -0.03064403124153614, -6.90197994117625e-05, -0.35571086406707764], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " pd.Series(np.sum(vectorizer.fit_transform(df[\"Text\"]).toarray(), axis=0)).rename(\"Count\")", "tokens": [" pd", ".", "Series", "(", "np", ".", "sum", "(", "vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", ",", " axis", "=", "0", "))", ".", "rename", "(", "\"", "Count", "\"", ")", "\n"], "logprobs": [-1.5684478282928467, -6.305972783593461e-05, -0.3143887221813202, -0.005436279810965061, -2.7751784324645996, -6.437280717364047e-06, -0.12106580287218094, -0.002387413289397955, -0.2033386081457138, -0.00031680811662226915, -7.92710343375802e-05, -0.006124064326286316, -2.8609820219571702e-05, -3.0278701160568744e-05, -0.0013061572099104524, -0.05198900029063225, -0.09989012032747269, -0.0014644861221313477, -0.0006965356878936291, -5.6265202147187665e-05, -0.013540801592171192, -0.007302615791559219, -0.02967473678290844, -4.768370445162873e-07, -0.00036006642039865255, -0.0008814025786705315, -0.003985201939940453, -0.00010096516780322418, -0.0006069486844353378, -0.4088970422744751, -0.24311883747577667, -2.002255439758301, -0.055944498628377914, -0.0736854150891304, -0.7305227518081665, -0.04815458878874779, -0.0029955299105495214, -0.10520423203706741], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.4593242406845093, -1.6689286894688848e-06, -0.17356830835342407, -0.08921404182910919], "finish_reason": "stop"}, {"text": "\"No valid text was found in the input data.\")", "tokens": ["\"", "No", " valid", " text", " was", " found", " in", " the", " input", " data", ".", "\"", ")", "\n"], "logprobs": [-0.04757412523031235, -1.4227912425994873, -0.3046378791332245, -2.4396727085113525, -3.3942222595214844, -0.20017889142036438, -0.1898467093706131, -0.011935249902307987, -0.3124217092990875, -1.719056248664856, -0.09623494744300842, -0.09410437941551208, -0.000442407006630674, -0.00047469791024923325], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07454926520586014, -0.016153531149029732], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.009185673668980598, -6.079655122448457e-06, -1.645074735279195e-05, -9.190614218823612e-05, -0.0006164796068333089, -4.088794958079234e-05, -2.253030106658116e-05, -0.00016258825780823827, -0.0007608617888763547, -0.0007526425761170685, -0.12434215098619461], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.005531599745154381, -0.012443523854017258, -0.014762569218873978, -0.059612542390823364, -0.0001433984871255234, -0.0002903516869992018, -4.255681051290594e-05, -0.014768324792385101, -0.00032944019767455757, -0.02959442138671875, -0.02667623944580555, -0.03979260101914406, -0.004655119031667709, -5.185469490243122e-05, -0.0011635207338258624, -0.0006565084331668913, -0.0009561972692608833, 0.0, -1.3112935448589269e-05, -1.2040065485052764e-05, -0.00025876989820972085, -3.099436753473128e-06, -6.4490144723095e-05, -4.887569048150908e-06, -6.067568756407127e-05, -0.0038435410242527723, -0.001644093543291092], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.008734355680644512, -0.009099324233829975, -0.0517069436609745, -8.475421054754406e-05, -0.0013444918440654874, -0.00013743886665906757, -0.045281488448381424, -0.00035375059815123677, -0.017862290143966675, -0.02157760225236416, -0.8321373462677002], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.032038722187280655, -0.001378896413370967, -0.002630228642374277, -3.3378044463461265e-05, -1.7881377516459906e-06, -0.0006636562757194042, -0.008432970382273197, -0.0001851148990681395, -0.0011475651990622282, -0.0032515060156583786, -7.70062324590981e-05, -0.0050934553146362305, -7.033100700937212e-05, -0.0002896366349887103, -0.005577952601015568, -1.0967194612021558e-05, -8.320462075062096e-05, -2.109982233378105e-05, -0.005871667061001062, -0.01979372650384903, -0.11091690510511398], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.8911447525024414, -9.536738616588991e-07, -3.576278118089249e-07, -0.01901809126138687, -0.005960072390735149, -0.002055557444691658, -0.0021908108610659838, -0.03390485420823097], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0010001424234360456, -1.7881377516459906e-06, -1.2636104656849056e-05, -0.006256280932575464, -0.08458594977855682, -3.576278118089249e-07, -2.145764938177308e-06, -0.004630201030522585, -0.0012878228444606066, -0.003217878518626094, -0.00028236693469807506, -0.6748533248901367, -0.0012057899730280042, -7.033323527139146e-06, -5.543078441405669e-05, -0.07052519172430038, -0.0016074366867542267, -0.0015210260171443224, -0.02326466329395771], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.04420972988009453, -0.06373918056488037, -0.00105408881790936, -0.27795466780662537, -0.0007551440503448248, -0.49270346760749817, -0.08799276500940323, -9.941560711013153e-05, -4.494089080253616e-05, -0.0001431601122021675, -2.145764938177308e-06, -0.0034397027920931578, -0.005891339387744665, -0.002360298065468669, -1.0371154530730564e-05, -9.142934868577868e-05, -1.1801649634435307e-05, -0.13175129890441895, -0.09682193398475647, -0.001122797722928226, -0.06547266244888306, -0.361225962638855, -0.0055879103019833565], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03340345621109009, -6.079655122448457e-06, -1.7523612768854946e-05, -0.46247661113739014, -0.005505992565304041], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.007233264856040478, -0.00020108585886191577, -6.639736966462806e-05, -0.0007514513563364744, -5.8412379075889476e-06, -1.9073468138230965e-06, -0.15898561477661133, -0.007465440779924393], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.07983320951461792, -0.07001225650310516, -0.1007130965590477], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.033081620931625366, -0.010689499787986279], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.0067727770656347275, -0.6581477522850037], "finish_reason": "stop"}], "metrics": {"energy": 1182.6758353843686, "energy_per_token": 4.788161276859792, "time": 30.903418, "time_per_token": 0.12511505263157893}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -0.30648258328437805], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray().sum(axis=0)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.40920716524124146, -2.7417760065873154e-05, -5.0424259825376794e-05, -0.008076036348938942, -0.004196765832602978, -0.00030155404238030314, -0.01675954833626747, -0.020926518365740776, -0.09813385456800461, -0.0014137759571895003, -0.0006649667047895491, -0.00011586471373448148, -0.15590016543865204, -0.8676221966743469, -0.7734647989273071, -2.396077979938127e-05, -0.0014028241857886314, -0.13641078770160675, -0.036403946578502655, -0.0006859333370812237, -0.002000356325879693, -0.00036042393185198307, -0.00026639728457666934, -0.0012388896429911256, -0.008032754994928837], "finish_reason": "stop"}, {"text": " Exception as e:", "tokens": [" Exception", " as", " e", ":", "\n"], "logprobs": [-1.7983407974243164, -0.27075955271720886, -0.07820725440979004, -0.0011868583969771862, -0.008565708994865417], "finish_reason": "stop"}, {"text": "\"The input data is either empty or contains only stop words.\")", "tokens": ["\"", "The", " input", " data", " is", " either", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.055234089493751526, -1.3493260145187378, -0.40181925892829895, -1.7312288284301758, -0.23736132681369781, -1.2468785047531128, -0.009201855398714542, -0.02804413251578808, -0.2857348918914795, -0.004512840881943703, -0.00807875581085682, -0.09224355965852737, -0.07213594019412994, -0.04775802791118622, -0.0004757702990900725, -0.00016616393986623734], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.087624691426754, -0.013213921338319778], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.009835472330451012, -6.794906312279636e-06, -1.8000440832111053e-05, -8.284702198579907e-05, -0.00023552982020191848, -3.8980677345534787e-05, -2.3483953555114567e-05, -0.00016723664884921163, -0.0007388246012851596, -0.000498289882671088, -0.09226182103157043], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.005687601864337921, -0.010870523750782013, -0.013912132009863853, -0.05457080900669098, -0.00031895318534225225, -0.0010056205792352557, -3.1470757676288486e-05, -0.023791426792740822, -0.0002269487304147333, -0.02030549943447113, -0.01761155016720295, -0.034850142896175385, -0.005700047593563795, -6.05564855504781e-05, -0.001958001870661974, -0.0006785474251955748, -0.0009115354623645544, 0.0, -1.156323378381785e-05, -1.0847986231965479e-05, -0.0003868784988299012, -3.814689989667386e-06, -6.69933797325939e-05, -6.794906312279636e-06, -6.007967749610543e-05, -0.004013223107904196, -0.0020536540541797876], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.00690289493650198, -0.006589943543076515, -0.053192634135484695, -0.0001431601122021675, -0.0010205067228525877, -0.00014709345123264939, -0.04693474993109703, -0.0003698380314745009, -0.026445114985108376, -0.022683320567011833, -0.5513773560523987, -0.7881279587745667, -0.00012039413559250534, -0.001090885023586452, -0.0007671750499866903, -0.032313570380210876, -1.1920928244535389e-07, -2.2172682292875834e-05, -2.455681169521995e-05, -0.00017355366435367614, -1.6689286894688848e-06, -1.4781842764932662e-05, -5.722029527532868e-06, -8.821448318485636e-06, -0.013775750994682312], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03243625536561012, -0.0009074860718101263, -0.00136461085639894, -2.7179348762729205e-05, -5.483612312673358e-06, -0.0006537684239447117, -0.006888569798320532, -0.0001387499796692282, -0.0008422164828516543, -0.0022019920870661736, -8.67805938469246e-05, -0.005047198850661516, -7.331102824537084e-05, -0.00037508129025809467, -0.007142723072320223, -1.07287787614041e-05, -9.703165414975956e-05, -1.9192511899746023e-05, -0.00509511586278677, -0.023807723075151443, -0.0928390845656395], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.7960458397865295, -9.536738616588991e-07, -3.576278118089249e-07, -0.01825917698442936, -0.006320605520159006, -0.001811888418160379, -0.0020277192816138268, -0.02760043740272522], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0009934734553098679, -2.9802276912960224e-06, -1.537788011773955e-05, -0.009488839656114578, -0.09175285696983337, -5.960462772236497e-07, -3.576272320060525e-06, -0.0043854969553649426, -0.0008916454971767962, -0.0034361386206001043, -0.00019488819816615433, -0.5923395752906799, -0.0010874316794797778, -6.9141146923357155e-06, -6.3774932641536e-05, -0.11119597405195236, -0.0017613149248063564, -0.002413457492366433, -0.024578724056482315], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.06024004891514778, -0.06994278728961945, -0.000969535845797509, -0.2674155533313751, -0.0006152882124297321, -0.4760560393333435, -0.056935518980026245, -0.00014423283573705703, -4.494089080253616e-05, -0.00015436411194968969, -2.50339189733495e-06, -0.0032550706528127193, -0.005628214683383703, -0.002074234653264284, -1.1801649634435307e-05, -8.356221951544285e-05, -8.583032467868179e-06, -0.18259136378765106, -0.09058878570795059, -0.0014854359906166792, -0.08098115772008896, -0.3903512954711914, -0.0038497161585837603], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03725385293364525, -7.390948667307384e-06, -2.253030106658116e-05, -0.49624648690223694, -0.004541796166449785], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.006883005611598492, -0.0002790300059132278, -5.304672595229931e-05, -0.0010875507723540068, -7.629365427419543e-06, -2.3841830625315197e-06, -0.1909489929676056, -0.007922771386802197], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.08082679659128189, -0.10521861165761948, -0.08690201491117477], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.054551396518945694, -0.009066837839782238], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.005737858358770609, -1.7107329368591309, -1.966933996300213e-05, -1.7881377516459906e-06, -1.168244216387393e-05, -2.825220326485578e-05, -0.17222744226455688], "finish_reason": "stop"}], "metrics": {"energy": 1230.064490379334, "energy_per_token": 4.980018179673417, "time": 31.894808, "time_per_token": 0.12912877732793523}}
{"outputs": [{"text": " pd.read_csv(file_path, encoding='utf-8')", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " encoding", "='", "utf", "-", "8", "')", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -2.093414783477783, -1.5585204362869263, -0.5601299405097961, -0.7897725701332092, -0.018194327130913734, -0.005366087891161442, -0.29769057035446167, -0.07400596141815186], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -0.30648258328437805], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS, token_pattern=r\"\\b\\w+\\b\")", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " token", "_", "pattern", "=", "r", "\"", "\\", "b", "\\", "w", "+\\", "b", "\"", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -1.6434025764465332, -2.4684510231018066, -0.021755965426564217, -4.100715523236431e-05, -0.15290255844593048, -0.053460147231817245, -0.2445015013217926, -0.3482688069343567, -0.4898926317691803, -0.2427987903356552, -0.1376694291830063, -0.03224131837487221, -0.0005199749139137566, -0.004271313082426786, -0.14353187382221222, -0.012574908323585987], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", "\n"], "logprobs": [-0.38236111402511597, -2.729855441430118e-05, -6.151010165922344e-05, -0.00906081311404705, -0.0031112858559936285, -0.00023862851958256215, -0.015496439300477505, -0.019526753574609756, -0.08569333702325821, -0.0016655157087370753, -0.0005972985527478158, -8.987976616481319e-05, -0.16271989047527313, -0.9951920509338379, -0.8150907158851624, -2.8132995794294402e-05, -0.0014294892316684127, -1.9680484533309937], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.337374746799469, -1.9073468138230965e-06, -0.20268119871616364, -0.06832905113697052], "finish_reason": "stop"}, {"text": "\"The input text file is empty or contains only stop words.\")", "tokens": ["\"", "The", " input", " text", " file", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.055268943309783936, -1.3764382600784302, -0.42118343710899353, -3.019160270690918, -2.0034523010253906, -0.201724573969841, -0.2958683967590332, -0.02583100087940693, -0.18886183202266693, -0.007213026750832796, -0.007137515116482973, -0.0527518168091774, -0.06910131126642227, -0.06588926911354065, -0.0004724340105894953, -0.0002611534437164664], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07354564964771271, -0.017394136637449265], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.013345674611628056, -7.390948667307384e-06, -1.680836794548668e-05, -0.00010430268594063818, -0.00034683887497521937, -4.136476854910143e-05, -2.002696055569686e-05, -0.00017653337272349745, -0.0009635811438784003, -0.0007146426942199469, -0.060710784047842026], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.006924204993993044, -0.012770773842930794, -0.012452000752091408, -0.06969387829303741, -0.0002076410164590925, -0.0003951007383875549, -2.098061486321967e-05, -0.020268818363547325, -0.000302030734019354, -0.029726115986704826, -0.02503259852528572, -0.041482750326395035, -0.004141445271670818, -5.304672595229931e-05, -0.0023098711390048265, -0.000730128725990653, -0.0011372057488188148, 0.0, -1.4662635294371285e-05, -1.4781842764932662e-05, -0.00037019551382400095, -3.099436753473128e-06, -6.961580220377073e-05, -5.602820692729438e-06, -5.709961988031864e-05, -0.004566360730677843, -0.0014522254932671785], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.009092945605516434, -0.0075617507100105286, -0.06734870374202728, -0.00013851160474587232, -0.0012413900112733245, -0.00010072677832795307, -0.03721468895673752, -0.00033158526639454067, -0.016560593619942665, -0.022797051817178726, -0.4672345221042633, -0.6049326062202454, -0.00010799778101500124, -0.000766102981287986, -0.0007863528444431722, -0.013155685737729073, -1.1920928244535389e-07, -1.2874520507466514e-05, -2.90866428258596e-05, -0.0001995364436879754, -1.1920922133867862e-06, -7.867782187531702e-06, -4.6491513785440475e-06, -1.07287787614041e-05, -0.013716724701225758], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.032655268907547, -0.0011305375955998898, -0.0013183006085455418, -3.2305197237292305e-05, -2.264974000354414e-06, -0.0006976078147999942, -0.005293282680213451, -0.00012838016846217215, -0.0006587718962691724, -0.002209723461419344, -7.295342220459133e-05, -0.005820824299007654, -5.364274329622276e-05, -0.0003631647559814155, -0.006226426921784878, -1.2636104656849056e-05, -0.00011538793478393927, -1.6927575416048057e-05, -0.005421933718025684, -0.019142065197229385, -0.09214191883802414], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.8220891952514648, -8.344646857949556e-07, -2.3841855067985307e-07, -0.01456239353865385, -0.006372963078320026, -0.0017552459612488747, -0.0023097521625459194, -0.02227279730141163], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.000979539705440402, -2.3841830625315197e-06, -7.748573807475623e-06, -0.0076235076412558556, -0.1442146748304367, -8.344646857949556e-07, -6.556489552167477e-06, -0.0028713210485875607, -0.0008131535141728818, -0.0036668223328888416, -0.00023588736075907946, -0.5705665946006775, -0.0010284854797646403, -7.867782187531702e-06, -5.2927523938706145e-05, -0.06719233840703964, -0.0016662298003211617, -0.0015844660811126232, -0.02164584770798683], "finish_reason": "stop"}, {"text": " df.plot(", "tokens": [" df", ".", "plot", "(", "\n"], "logprobs": [-0.05681096389889717, -0.0586589090526104, -0.0007951673469506204, -1.3956384658813477, -0.8340913653373718], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03327178210020065, -7.390948667307384e-06, -3.158996332786046e-05, -0.4772639572620392, -0.004761191550642252], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.006762002129107714, -0.00029202012228779495, -6.758938252460212e-05, -0.0010193157941102982, -5.602820692729438e-06, -2.264974000354414e-06, -0.16366615891456604, -0.007107332348823547], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.07317864149808884, -0.06861692667007446, -0.10445324331521988], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.044374410063028336, -0.009407948702573776], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.004845780320465565, -0.6128494143486023], "finish_reason": "stop"}], "metrics": {"energy": 1126.0820609941475, "energy_per_token": 4.771534156754862, "time": 29.403538, "time_per_token": 0.12459126271186441}}
{"outputs": [{"text": " pd.read_csv(file_path)  # Read the CSV file", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "  ", "#", " Read", " the", " CS", "V", " file", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -2.8299341201782227, -0.00016282663273159415, -0.5233489274978638, -0.626197338104248, -0.16687114536762238, -1.0609570381348021e-05, -0.052112236618995667, -0.993761420249939], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -0.30648258328437805], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS, token_pattern=r\"\\b\\w+\\b\")", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " token", "_", "pattern", "=", "r", "\"", "\\", "b", "\\", "w", "+\\", "b", "\"", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -1.6434025764465332, -2.4684510231018066, -0.021755965426564217, -4.100715523236431e-05, -0.15290255844593048, -0.053460147231817245, -0.2445015013217926, -0.3482688069343567, -0.4898926317691803, -0.2427987903356552, -0.1376694291830063, -0.03224131837487221, -0.0005199749139137566, -0.004271313082426786, -0.14353187382221222, -0.012574908323585987], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"].astype(str))", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", "))", "\n"], "logprobs": [-0.38236111402511597, -2.729855441430118e-05, -6.151010165922344e-05, -0.00906081311404705, -0.0031112858559936285, -0.00023862851958256215, -0.015496439300477505, -0.019526753574609756, -0.08569333702325821, -0.0016655157087370753, -0.0005972985527478158, -8.987976616481319e-05, -1.9034768342971802, -0.008183755911886692, -2.0780887603759766, -1.1920928244535389e-07, -0.010773942805826664, -0.10702022165060043, -0.10945818573236465, -0.36929580569267273], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.337374746799469, -1.9073468138230965e-06, -0.20268119871616364, -0.06832905113697052], "finish_reason": "stop"}, {"text": "\"No valid words found in the input.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " input", ".", "\"", ")", "\n"], "logprobs": [-0.055268943309783936, -1.3056813478469849, -0.3267199993133545, -0.5007254481315613, -0.21861854195594788, -0.10693742334842682, -0.056289710104465485, -0.14402425289154053, -0.8031479120254517, -0.22299610078334808, -0.0004439560289029032, -0.0004940004437230527], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.08214394003152847, -0.015833135694265366], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.01189013384282589, -9.417489309271332e-06, -1.4424220353248529e-05, -0.0001137191939051263, -0.0006460248259827495, -4.255681051290594e-05, -2.658331868587993e-05, -0.00021479207498487085, -0.0009576263837516308, -0.000636255950666964, -0.06547702103853226], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.008588992990553379, -0.011448285542428493, -0.011735214851796627, -0.06144130602478981, -0.00025507534155622125, -0.0002851079625543207, -2.4676019165781327e-05, -0.02073761448264122, -0.00022957073815632612, -0.03347378224134445, -0.02526172436773777, -0.03400084003806114, -0.0032785970252007246, -5.6622808187967166e-05, -0.0014837696217000484, -0.0006360176485031843, -0.0011263700434938073, 0.0, -1.2159273865108844e-05, -1.2993727978027891e-05, -0.0005984899471513927, -2.9802276912960224e-06, -7.617183291586116e-05, -6.437280717364047e-06, -5.8053239627042785e-05, -0.0036353471223264933, -0.0017641709418967366], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.007630960550159216, -0.011254046112298965, -0.08896464854478836, -0.00014304091746453196, -0.0013872295385226607, -0.00013767725613433868, -0.03248206898570061, -0.0002847504511009902, -0.01956883817911148, -0.02561257779598236, -0.561236560344696, -0.5207449793815613, -3.0278701160568744e-05, -0.007470883894711733, -0.00038354191929101944, -0.012501565739512444, -3.0636318115284666e-05, -0.003125665243715048, -1.1920928244535389e-07, -9.059865078597795e-06, -0.0011601868318393826, -1.1920928244535389e-07, -4.732496745418757e-05, 0.0, -9.894321920000948e-06, -0.367827832698822, -0.0015836328966543078], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03638118878006935, -0.0010409895330667496, -0.0016899126349017024, -3.790783375734463e-05, -4.529942543740617e-06, -0.0007557396893389523, -0.0066576809622347355, -0.00015436411194968969, -0.0012566297082230449, -0.003792001400142908, -6.937739817658439e-05, -0.007597480434924364, -6.246371776796877e-05, -0.0003182381624355912, -0.004619165789335966, -1.168244216387393e-05, -0.00010251473577227443, -2.3483953555114567e-05, -0.005840142257511616, -0.023632286116480827, -0.10949569195508957], "finish_reason": "stop"}, {"text": " [word[0] for word in words_freq[:10]]", "tokens": [" [", "word", "[", "0", "]", " for", " word", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.7571285367012024, -0.6773184537887573, -1.5998088121414185, -0.006576560903340578, -0.00027640812913887203, -0.0011926926672458649, -0.0016832482069730759, -0.00021598390594590455, -0.001620528637431562, -1.7881377516459906e-06, -5.960462772236497e-07, -0.29377296566963196, -0.007824721746146679, -0.0032092041801661253, -0.02518070489168167, -0.009587080217897892], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0011020784731954336, -2.861018856492592e-06, -1.1324817933200393e-05, -0.008021757937967777, -0.14064979553222656, -8.344646857949556e-07, -3.576272320060525e-06, -0.0043098898604512215, -0.0011551857460290194, -0.003967985510826111, -0.0002650863316375762, -0.6207213997840881, -0.0010715939570218325, -7.986990567587782e-06, -5.936446541454643e-05, -0.14070498943328857, -0.001883281976915896, -0.0017736908048391342, -0.02030608244240284], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\")", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.047198712825775146, -0.05604616925120354, -0.0011124381562694907, -0.4587048888206482, -0.0007061848300509155, -0.42323121428489685, -0.07408832013607025, -0.00011836781777674332, -4.792098479811102e-05, -0.00013541258522309363, -2.264974000354414e-06, -0.00394352525472641, -0.005182285793125629, -0.0030929844360798597, -9.536697689327411e-06, -9.858122211880982e-05, -1.2516897186287679e-05, -1.84896981716156, -0.0043071601539850235], "finish_reason": "stop"}, {"text": " save_path is None:", "tokens": [" save", "_", "path", " is", " None", ":", "\n"], "logprobs": [-0.04632706195116043, -7.748573807475623e-06, -1.3947389561508317e-05, -0.9141225218772888, -1.0001938343048096, -0.13266520202159882, -0.023653358221054077], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0064146569930016994, -0.00037222131504677236, -5.936446541454643e-05, -0.0009765623253770173, -5.960446742392378e-06, -1.9073468138230965e-06, -0.17993247509002686, -0.009022064507007599], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.06967197358608246, -0.08202105760574341, -0.0725359246134758], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05006597936153412, -0.009014267474412918], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.004795359913259745, -1.8670711517333984, -1.4662635294371285e-05, -2.145764938177308e-06, -1.1324817933200393e-05, -2.4318398573086597e-05, -0.15276546776294708], "finish_reason": "stop"}], "metrics": {"energy": 1252.2785306701664, "energy_per_token": 4.707814025075813, "time": 32.367027, "time_per_token": 0.12168055263157895}}
{"outputs": [{"text": " pd.read_csv(file_path, dtype=str)  # Read the CSV file", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " dtype", "=", "str", ")", "  ", "#", " Read", " the", " CS", "V", " file", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -2.093414783477783, -2.254937171936035, -0.10474582761526108, -0.2925608456134796, -0.23197542130947113, -2.2898812294006348, -0.0001820160250645131, -1.1097177267074585, -0.8035070300102234, -0.18044258654117584, -1.6093124941107817e-05, -0.07622778415679932, -0.9557675719261169], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -0.30648258328437805], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS, max_features=10)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " max", "_", "features", "=", "1", "0", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -1.6434025764465332, -1.063494086265564, -9.775113539944869e-06, -0.07346811890602112, -0.0011444692499935627, -0.23876436054706573, -0.02234472706913948, -0.37287887930870056, -0.008888795971870422], "finish_reason": "stop"}, {"text": " pd.DataFrame(dict(vectorizer.fit_transform(df[\"Text\"]).todense())).sum().sort_values(ascending=False)", "tokens": [" pd", ".", "DataFrame", "(", "dict", "(", "vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "t", "od", "ense", "()))", ".", "sum", "()", ".", "sort", "_", "values", "(", "asc", "ending", "=", "False", ")", "\n"], "logprobs": [-1.7013099193572998, -6.103329360485077e-05, -1.4533753395080566, -0.06377854943275452, -3.175945997238159, -0.00836889911442995, -0.416668176651001, -1.3708974620385561e-05, -0.0001787979417713359, -0.04764971509575844, -0.0007596705690957606, -0.00014184899919200689, -0.012103093788027763, -0.033304180949926376, -0.09324357658624649, -0.0019207615405321121, -0.0005762108485214412, -9.452849917579442e-05, -0.1040927916765213, -0.004449348896741867, -1.0506229400634766, -3.5523738915799186e-05, -0.008765553124248981, -0.47185325622558594, -0.18292580544948578, -0.2497672289609909, -0.3360966444015503, -1.2662115097045898, -0.38885873556137085, -3.099436753473128e-06, -0.0012661544606089592, -0.07972655445337296, -0.01758800819516182, -5.960462772236497e-07, -0.0010796914575621486, -0.006075367331504822, -0.09612320363521576, -0.7394518256187439], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.34379932284355164, -1.5497195136049413e-06, -0.20667898654937744, -0.06658735871315002], "finish_reason": "stop"}, {"text": "\"No text found or only stop words found in the input CSV file.\")", "tokens": ["\"", "No", " text", " found", " or", " only", " stop", " words", " found", " in", " the", " input", " CS", "V", " file", ".", "\"", ")", "\n"], "logprobs": [-0.062477633357048035, -1.384777545928955, -2.68910551071167, -1.7354557514190674, -2.1032767295837402, -0.9918976426124573, -0.029024504125118256, -0.08389264345169067, -1.4989644289016724, -0.22887422144412994, -0.021576901897788048, -0.4970089793205261, -2.827406406402588, -2.586808113846928e-05, -0.3118433356285095, -0.04290558397769928, -0.014122194610536098, -0.00028939827461726964, -0.00016950124700088054], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.06901074945926666, -0.019117504358291626], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.015281148254871368, -7.510157047363464e-06, -1.6927575416048057e-05, -8.439661905867979e-05, -0.00044860312482342124, -4.2437604861333966e-05, -2.3245540432981215e-05, -0.0002019201492657885, -0.0009197533945553005, -0.0004951919545419514, -0.06628874689340591], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.008739436976611614, -0.01733134128153324, -0.011779867112636566, -0.058405935764312744, -0.0002090712368953973, -0.0003233625029679388, -4.053033626405522e-05, -0.029571736231446266, -0.00044490929576568305, -0.022443227469921112, -0.016446031630039215, -0.03953378275036812, -0.0033949140924960375, -5.8887653722194955e-05, -0.0018491327064111829, -0.0006292270263656974, -0.0006046851049177349, 0.0, -1.156323378381785e-05, -9.298280929215252e-06, -0.00032550760079175234, -2.7418097943154862e-06, -7.521823135903105e-05, -5.960446742392378e-06, -6.4490144723095e-05, -0.0035717994906008244, -0.0017244244227185845], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.00858320202678442, -0.007735298480838537, -0.043985702097415924, -0.0001225396408699453, -0.0008450751192867756, -0.00019214690837543458, -0.044129546731710434, -0.0003983181086368859, -0.017187807708978653, -0.015550783835351467, -0.5510339140892029, -0.4619024693965912, -0.00011312322021694854, -0.0008538890979252756, -0.0005683475756086409, -0.019795011729002, -1.1920928244535389e-07, -1.6927575416048057e-05, -1.645074735279195e-05, -0.00018523407925385982, -1.9073468138230965e-06, -1.3112935448589269e-05, -4.768360213347478e-06, -1.156323378381785e-05, -0.015966303646564484], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.027149351313710213, -0.00144091690890491, -0.0011759040644392371, -3.957670196541585e-05, -2.7418097943154862e-06, -0.0005816913326270878, -0.006440241355448961, -0.00011920218821614981, -0.0012752028414979577, -0.0025222653057426214, -0.00011646069469861686, -0.005703248083591461, -6.568216485902667e-05, -0.0003293210465926677, -0.0056062848307192326, -1.4543427823809907e-05, -0.00010716341057559475, -1.9192511899746023e-05, -0.006104396190494299, -0.01732267066836357, -0.10958576202392578], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.8278483152389526, -9.536738616588991e-07, -3.576278118089249e-07, -0.015911513939499855, -0.007303680758923292, -0.0020889858715236187, -0.0017579830018803477, -0.025190236046910286], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0010110987350344658, -2.145764938177308e-06, -1.0013530300057027e-05, -0.008657895028591156, -0.08948059380054474, -3.576278118089249e-07, -3.2186455882765586e-06, -0.004361877683550119, -0.0008474572678096592, -0.004009186290204525, -0.0002460177056491375, -1.1168193817138672, -0.00045694399159401655, -1.537788011773955e-05, -7.188061863416806e-05, -0.3392932415008545, -0.02616063505411148, -0.0008958140970207751, -0.0030637490563094616, -0.01938973180949688], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", legend=False, title=\"Top 10 Words\")", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " legend", "=", "False", ",", " title", "=", "\"", "Top", " ", "1", "0", " Words", "\"", ")", "\n"], "logprobs": [-0.04948832839727402, -0.07318252325057983, -0.0013342535821720958, -0.4277900457382202, -0.0006204110686667264, -0.5862553715705872, -0.087544746696949, -0.00013362467871047556, -5.352353764465079e-05, -0.0001399419124936685, -1.7881377516459906e-06, -0.0034468306694179773, -0.00534320343285799, -0.002261820714920759, -1.0490362910786644e-05, -0.00010513706365600228, -1.1324817933200393e-05, -0.097500279545784, -2.270566940307617, -0.00011729506513802335, -0.0024374795611947775, -1.460334062576294, -1.6840641498565674, -0.003011812688782811, -0.015186401084065437, -0.6506725549697876, -0.23435339331626892, -0.0013815154088661075, -0.00013672371278516948, -0.15793800354003906, -0.16059766709804535, -0.024981092661619186, -0.0030805058777332306], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03095971792936325, -6.9141146923357155e-06, -1.823885577323381e-05, -0.47000178694725037, -0.004313688259571791], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.007769247051328421, -0.00047219570842571557, -5.7338023907504976e-05, -0.0010879080509766936, -5.960446742392378e-06, -2.9802276912960224e-06, -0.17367392778396606, -0.007577722892165184], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.0964253768324852, -0.08322913199663162, -0.08251973986625671], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.055272892117500305, -0.00792726594954729], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.004888368770480156, -0.5870841145515442], "finish_reason": "stop"}], "metrics": {"energy": 1310.0323073768611, "energy_per_token": 4.580532543275738, "time": 34.140697, "time_per_token": 0.11937306643356645}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8732607364654541, -0.00027926836628466845, -0.3627833425998688], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -0.41199398040771484, -3.2186455882765586e-06, -4.6491513785440475e-06, -0.001842707279138267, -0.017574068158864975, 0.0, -8.4638240878121e-06, -2.145764938177308e-06, 0.0, 0.0, -0.19700554013252258, -0.016263552010059357], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).sum(axis=0).A1", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "sum", "(", "axis", "=", "0", ")", ".", "A", "1", "\n"], "logprobs": [-0.3623025119304657, -2.5987286790041253e-05, -7.664863369427621e-05, -0.006578811444342136, -0.003461324144154787, -0.00025328766787424684, -0.014238080009818077, -0.018174076452851295, -0.08900271356105804, -0.0017191881779581308, -0.0005447572330012918, -0.00019298121333122253, -0.14113116264343262, -0.8871932625770569, -0.7439634203910828, -2.8609820219571702e-05, -0.0035499429795891047, -0.0002762889489531517, -0.0005152089870534837, -9.011816291604191e-05, -0.5393536686897278, -0.1144876480102539, -0.7244664430618286, -0.015235595405101776], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.43807297945022583, -1.7881377516459906e-06, -0.19968245923519135, -0.0924256220459938], "finish_reason": "stop"}, {"text": "\"No valid data to plot.\")", "tokens": ["\"", "No", " valid", " data", " to", " plot", ".", "\"", ")", "\n"], "logprobs": [-0.06172657385468483, -1.2838648557662964, -0.3282153308391571, -1.5730758905410767, -2.0484092235565186, -1.7028238773345947, -0.5874411463737488, -0.6372501850128174, -0.0005467826849780977, -0.0009921634336933494], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07014596462249756, -0.016533976420760155], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.012680144049227238, -8.106198947643861e-06, -1.680836794548668e-05, -0.00011932138295378536, -0.0003305127320345491, -3.790783375734463e-05, -2.6225699912174605e-05, -0.00017474555352237076, -0.0008423355757258832, -0.000602421467192471, -0.05761287361383438], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.006671062204986811, -0.006818598601967096, -0.013791271485388279, -0.05422861874103546, -0.0002488780301064253, -0.0005703729693777859, -2.8609820219571702e-05, -0.016089601442217827, -0.0008366183610633016, -0.029310718178749084, -0.017594683915376663, -0.03596072643995285, -0.0018713833997026086, -6.353653589030728e-05, -0.0014492495683953166, -0.0006530536338686943, -0.0009347598534077406, 0.0, -1.3828182090946939e-05, -1.2516897186287679e-05, -0.00018749863374978304, -2.50339189733495e-06, -6.818538531661034e-05, -5.125986263010418e-06, -8.105902816168964e-05, -0.004595907870680094, -0.001717998180538416], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.009039785712957382, -0.009931908920407295, -0.07240243256092072, -0.00015424491721205413, -0.001177451922558248, -0.00013839241000823677, -0.03550833463668823, -0.0003507714136503637, -0.023539956659078598, -0.02553797885775566, -1.1011466979980469], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.036095380783081055, -0.0011731653939932585, -0.002063646912574768, -5.221230458118953e-05, -3.6954811548639555e-06, -0.000743708573281765, -0.009240951389074326, -0.0001461399078834802, -0.0009647720726206899, -0.002463165670633316, -0.0001284993631998077, -0.006157593801617622, -7.223821739898995e-05, -0.0002703301142901182, -0.006972860544919968, -1.3947389561508317e-05, -0.00010394509445177391, -1.9550132492440753e-05, -0.005718656815588474, -0.015615100041031837, -0.09624187648296356], "finish_reason": "stop"}, {"text": " [word for word, count in words_freq[:10]]", "tokens": [" [", "word", " for", " word", ",", " count", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.6310889720916748, -0.6416735649108887, -0.3533773124217987, -0.06661467999219894, -0.002707746112719178, -1.1495716571807861, -0.00010644822759786621, -0.000996212475001812, -8.344646857949556e-07, -3.576278118089249e-07, -0.14123685657978058, -0.008100868202745914, -0.004304904956370592, -0.012957658618688583, -0.00558186462149024], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.00101383775472641, -2.9802276912960224e-06, -1.6689160474925302e-05, -0.007460944354534149, -0.09420495480298996, -7.152555099310121e-07, -2.861018856492592e-06, -0.0030403363052755594, -0.0011755467858165503, -0.0030614910647273064, -0.0004545609117485583, -0.5690827369689941, -0.001002047909423709, -8.702239938429557e-06, -7.199982064776123e-05, -0.06647113710641861, -0.001529596047475934, -0.0014717471785843372, -0.022947238758206367], "finish_reason": "stop"}, {"text": " df[\"Count\"].plot(", "tokens": [" df", "[", "\"", "Count", "\"", "]", ".", "plot", "(", "\n"], "logprobs": [-0.04740235581994057, -2.688028573989868, -0.0009631047723814845, -0.012116520665585995, -3.6954811548639555e-06, -0.001505790394730866, -0.00039426659350283444, -0.021479714661836624, -0.13643844425678253, -0.1865590363740921], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.0222959965467453, -6.9141146923357155e-06, -2.47952248173533e-05, -0.46061819791793823, -0.003927495330572128], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.005766186397522688, -0.0003819928097072989, -5.9602869441732764e-05, -0.0011954312212765217, -4.6491513785440475e-06, -1.4305104514278355e-06, -0.1974261999130249, -0.006493065971881151], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.09255266934633255, -0.06677069514989853, -0.0852985829114914], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04492607340216637, -0.009911253117024899], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.006344889290630817, -0.6538130640983582], "finish_reason": "stop"}], "metrics": {"energy": 1143.767406593322, "energy_per_token": 5.369800030954563, "time": 30.03377, "time_per_token": 0.1410036150234742}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8732607364654541, -0.00027926836628466845, -0.3627833425998688], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -1.6521062850952148], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.3539600968360901, -2.4914430468925275e-05, -6.12716976320371e-05, -0.012970485724508762, -0.0033378859516233206, -0.00029869386344216764, -0.017540685832500458, -0.02443099208176136, -0.10681615769863129, -0.001796776195988059, -0.0005147324409335852, -0.00021300431399140507, -0.17036493122577667, -0.6040729284286499], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.36196017265319824, -1.7881377516459906e-06, -0.18933852016925812, -0.07577545940876007], "finish_reason": "stop"}, {"text": "\"Input file is either empty or contains only stopwords\")", "tokens": ["\"", "Input", " file", " is", " either", " empty", " or", " contains", " only", " stop", "words", "\"", ")", "\n"], "logprobs": [-0.061218976974487305, -2.2386772632598877, -0.48579660058021545, -0.08035168796777725, -1.6067619323730469, -0.006774907931685448, -0.02151716686785221, -0.2910943925380707, -0.017408430576324463, -0.008491599932312965, -2.7462291717529297, -2.0175087451934814, -0.0007413261337205768, -0.0006330393371172249], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07890671491622925, -0.017573131248354912], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.011157972738146782, -9.059865078597795e-06, -1.5735502529423684e-05, -0.00010716341057559475, -0.0005479741375893354, -3.290122185717337e-05, -2.109982233378105e-05, -0.00016699827392585576, -0.0010012142593041062, -0.0007239343249239028, -0.07931523025035858], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.009258786216378212, -0.016209715977311134, -0.015329399146139622, -0.07873153686523438, -0.000350175570929423, -0.00029905137489549816, -3.1709168979432434e-05, -0.021665329113602638, -0.00028427375946193933, -0.028275351971387863, -0.013022142462432384, -0.03830590099096298, -0.004191898740828037, -6.401333666872233e-05, -0.0019212374463677406, -0.000964533886872232, -0.001110890181735158, 0.0, -1.3112935448589269e-05, -1.3589766240329482e-05, -0.0002809368306770921, -3.814689989667386e-06, -7.843663479434326e-05, -7.629365427419543e-06, -5.721882189391181e-05, -0.0033715092577040195, -0.0018312842585146427], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.007786871865391731, -0.00841500237584114, -0.052524153143167496, -0.00016044282529037446, -0.001517931348644197, -0.00022873646230436862, -0.026125330477952957, -0.00041940953815355897, -0.015856485813856125, -0.02426113374531269, -0.6050354838371277, -1.0392072200775146, -0.00012575789878610522, -0.0011338717304170132, -0.0006512666586786509, -0.01702585443854332, -1.1920928244535389e-07, -1.7523612768854946e-05, -3.111314072157256e-05, -0.00018630675913300365, -2.264974000354414e-06, -1.5258672647178173e-05, -6.079655122448457e-06, -1.0847986231965479e-05, -0.01406436413526535], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.033760108053684235, -0.0011726891389116645, -0.0014543681172654033, -3.611976353568025e-05, -2.145764938177308e-06, -0.0008299481705762446, -0.007144971750676632, -0.0001245659514097497, -0.0008949803304858506, -0.002801305614411831, -9.798523387871683e-05, -0.006138637196272612, -7.533743337262422e-05, -0.0004450284468475729, -0.005893472582101822, -1.2397689715726301e-05, -8.821098163025454e-05, -1.9550132492440753e-05, -0.006989315617829561, -0.022102337330579758, -0.14088952541351318], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.8640446662902832, -1.0728830375228426e-06, -3.576278118089249e-07, -0.01778675988316536, -0.0066685751080513, -0.001965378178283572, -0.002144895726814866, -0.030025087296962738], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0009289240115322173, -2.3841830625315197e-06, -2.658331868587993e-05, -0.007302142214030027, -0.0831926092505455, -5.960462772236497e-07, -3.6954811548639555e-06, -0.004080779384821653, -0.0015312624163925648, -0.0036934271920472383, -0.00021836756786797196, -0.5895268321037292, -0.001334610627964139, -9.179073458653875e-06, -5.61460001335945e-05, -0.15262793004512787, -0.0014643670292571187, -0.0016611122991889715, -0.02332545816898346], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0, title=\"Top 10 Words in Text Dataset\")", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ",", " title", "=", "\"", "Top", " ", "1", "0", " Words", " in", " Text", " Dataset", "\"", ")", "\n"], "logprobs": [-0.037183791399002075, -0.06728172302246094, -0.00183271209243685, -0.3494178056716919, -0.0008641323074698448, -0.42315563559532166, -0.09020859748125076, -0.00015615197480656207, -6.139089964563027e-05, -0.00016902448260225356, -2.145764938177308e-06, -0.0029863782692700624, -0.007144853472709656, -0.0017606009496375918, -6.9141146923357155e-06, -7.915183232398704e-05, -1.0609570381348021e-05, -0.13670414686203003, -0.08634821325540543, -0.0011126763420179486, -0.08767470717430115, -1.1681592464447021, -0.5620396733283997, -0.0066461944952607155, -0.010407933034002781, -0.7920589447021484, -0.30118319392204285, -0.0013949673157185316, -0.00011908298620255664, -0.1127532348036766, -2.558823585510254, -0.8893818259239197, -1.8503761291503906, -0.0043938048183918, -0.008406136184930801, -0.002061981474980712], "finish_reason": "stop"}, {"text": " save_path is None:", "tokens": [" save", "_", "path", " is", " None", ":", "\n"], "logprobs": [-0.03375261649489403, -6.556489552167477e-06, -2.5629668016335927e-05, -0.9027663469314575, -1.0028765201568604, -0.1579328030347824, -0.02185487560927868], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.007448875345289707, -0.0003634030872490257, -6.961580220377073e-05, -0.0012416280806064606, -5.483612312673358e-06, -2.3841830625315197e-06, -0.17918695509433746, -0.009561103768646717], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.10797262191772461, -0.08041075617074966, -0.08235353976488113], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.043462395668029785, -0.009298470802605152], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.00536869652569294, -0.557662844657898], "finish_reason": "stop"}], "metrics": {"energy": 1109.1956625223163, "energy_per_token": 4.801712824771932, "time": 29.021824, "time_per_token": 0.12563560173160174}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8732607364654541, -0.00027926836628466845, -0.3627833425998688], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -0.41199398040771484, -3.2186455882765586e-06, -4.6491513785440475e-06, -0.001842707279138267, -0.017574068158864975, 0.0, -8.4638240878121e-06, -2.145764938177308e-06, 0.0, 0.0, -0.19700554013252258, -0.016263552010059357], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.Text).toarray().sum(axis=0)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "Text", ")", ".", "to", "array", "()", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.3623025119304657, -2.5987286790041253e-05, -7.664863369427621e-05, -0.006578811444342136, -0.003461324144154787, -0.00025328766787424684, -0.014238080009818077, -0.018174076452851295, -2.882680892944336, -0.3217850923538208, -0.1350305825471878, -0.7328172326087952, -0.7602896094322205, -1.8596476365928538e-05, -0.002549851778894663, -0.10256888717412949, -0.03356001153588295, -0.0005335576133802533, -0.0040984689258039, -0.0006521005416288972, -0.00044204952428117394, -0.002452106447890401, -0.00787345226854086], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.3932550549507141, -1.4305104514278355e-06, -0.16442182660102844, -0.09705877304077148], "finish_reason": "stop"}, {"text": "\"The input data is empty or contains only stop words.\")", "tokens": ["\"", "The", " input", " data", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.05430144816637039, -1.2647895812988281, -0.4665965735912323, -1.7038600444793701, -0.2326182872056961, -0.3757435381412506, -0.01889515481889248, -0.25135716795921326, -0.0040525225922465324, -0.005871904082596302, -0.051891785115003586, -0.10450661182403564, -0.057980798184871674, -0.00045813556062057614, -0.00025674383505247533], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07093781232833862, -0.017048826441168785], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.013858992606401443, -6.556489552167477e-06, -1.7762025890988298e-05, -0.00010334911348763853, -0.00047636605449952185, -4.184158387943171e-05, -2.8132995794294402e-05, -0.00014304091746453196, -0.000871159543748945, -0.0004372832481749356, -0.07808841019868851], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.006544111296534538, -0.011054699309170246, -0.011292001232504845, -0.059369247406721115, -0.00018046658078674227, -0.00023982033599168062, -5.245071224635467e-05, -0.01854720152914524, -0.00034981805947609246, -0.03320639953017235, -0.026254581287503242, -0.03618977591395378, -0.004101437050849199, -5.245071224635467e-05, -0.0018443731823936105, -0.0006255338666960597, -0.0010486110113561153, 0.0, -1.2755313036905136e-05, -1.4185804502631072e-05, -0.00024184639914892614, -3.3378546504536644e-06, -8.272782724816352e-05, -5.006777428206988e-06, -6.19869097135961e-05, -0.0038653912488371134, -0.001630882965400815], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.007313502952456474, -0.008253277279436588, -0.06624077260494232, -0.00013076403411105275, -0.001255558105185628, -0.00022539935889653862, -0.05558191239833832, -0.0002699726028367877, -0.018491849303245544, -0.025226272642612457, -1.0904663801193237], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.036089859902858734, -0.0011570908827707171, -0.002160121686756611, -3.9219088648678735e-05, -2.7418097943154862e-06, -0.0007061848300509155, -0.005087169352918863, -0.00014375607133843005, -0.0009662011871114373, -0.003086210461333394, -0.0001110968878492713, -0.00528059434145689, -7.354942499659956e-05, -0.00036161558819003403, -0.006417381577193737, -1.1920858014491387e-05, -9.142934868577868e-05, -1.764281842042692e-05, -0.007264627143740654, -0.023253949359059334, -0.11734965443611145], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.8036978244781494, -1.0728830375228426e-06, -2.3841855067985307e-07, -0.010697755962610245, -0.006434674374759197, -0.0023053516633808613, -0.001966329989954829, -0.030978208407759666], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0012012654915452003, -3.099436753473128e-06, -1.9550132492440753e-05, -0.008992057293653488, -0.10714296251535416, -5.960462772236497e-07, -4.529942543740617e-06, -0.0048683201894164085, -0.0010119322687387466, -0.0037813130766153336, -0.0003022690652869642, -0.5665526390075684, -0.0010968389688059688, -5.245195097813848e-06, -5.6622808187967166e-05, -0.07592076808214188, -0.001740013831295073, -0.0017407278064638376, -0.021176312118768692], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.05181051790714264, -0.07118074595928192, -0.0015043619787320495, -0.2885553240776062, -0.0005053196800872684, -0.38293585181236267, -0.04157057777047157, -0.00010179955279454589, -4.851700214203447e-05, -0.00012611546844709665, -1.7881377516459906e-06, -0.0032952316105365753, -0.005518440622836351, -0.002659833524376154, -9.65590606938349e-06, -9.858122211880982e-05, -1.5258672647178173e-05, -0.13568463921546936, -0.08745070546865463, -0.0011323237558826804, -0.07907447963953018, -0.3427298963069916, -0.004347753711044788], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03906397148966789, -6.9141146923357155e-06, -2.6702524337451905e-05, -0.440183162689209, -0.004674696829169989], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.006256280932575464, -0.0004587313160300255, -6.878139538457617e-05, -0.0011245838832110167, -6.079655122448457e-06, -2.0265558760002023e-06, -0.16476845741271973, -0.009358230978250504], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.09098690003156662, -0.08168197423219681, -0.09094205498695374], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.057277239859104156, -0.008594311773777008], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.006035909987986088, -0.5503747463226318], "finish_reason": "stop"}], "metrics": {"energy": 1099.9304518318174, "energy_per_token": 4.932423550815325, "time": 28.352633, "time_per_token": 0.12714185201793723}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.replace(\"[^\\w\\s]\", \"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "replace", "(", "\"", "[", "^\\", "w", "\\", "s", "]\"", ",", " \"", "\"", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8530066013336182, -0.03686662018299103, -0.9176292419433594, -0.6719261407852173, -0.4418778419494629, -0.002056509256362915, -0.014608211815357208, -0.0033687767572700977, -0.022391121834516525, -7.998623186722398e-05, -0.00933602824807167, -0.046057745814323425, -0.10099869966506958, -0.8507019281387329], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -0.41199398040771484, -3.2186455882765586e-06, -4.6491513785440475e-06, -0.001842707279138267, -0.017574068158864975, 0.0, -8.4638240878121e-06, -2.145764938177308e-06, 0.0, 0.0, -0.19700554013252258, -0.016263552010059357], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df.Text)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", ".", "Text", ")", "\n"], "logprobs": [-0.3623025119304657, -2.5987286790041253e-05, -7.664863369427621e-05, -0.006578811444342136, -0.003461324144154787, -0.00025328766787424684, -0.014238080009818077, -0.018174076452851295, -2.882680892944336, -0.3217850923538208, -0.1350305825471878, -0.6622148156166077], "finish_reason": "stop"}, {"text": " Exception as e:", "tokens": [" Exception", " as", " e", ":", "\n"], "logprobs": [-1.9379727840423584, -0.25975501537323, -0.09624328464269638, -0.0012180536286905408, -0.011400554329156876], "finish_reason": "stop"}, {"text": "\"No valid words found in the input.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " input", ".", "\"", ")", "\n"], "logprobs": [-0.052613504230976105, -1.3355460166931152, -0.27699723839759827, -0.4237065613269806, -0.23610873520374298, -0.13460148870944977, -0.07026997953653336, -0.12238144874572754, -0.8117414712905884, -0.23159193992614746, -0.0005023409612476826, -0.0005830018781125546], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07298295944929123, -0.014721103943884373], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.011350227519869804, -9.894321920000948e-06, -2.312633478140924e-05, -9.786603914108127e-05, -0.00039176418795250356, -3.8980677345534787e-05, -2.8609820219571702e-05, -0.00014780859055463225, -0.0010724276071414351, -0.000671042304020375, -0.1404583752155304], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.009171144105494022, -0.016213586553931236, -0.011968821287155151, -0.05950617790222168, -0.00028308198670856655, -0.0005317704635672271, -2.706014311115723e-05, -0.024640601128339767, -0.00013457823661156, -0.02423076517879963, -0.015567684546113014, -0.04991300776600838, -0.0048323748633265495, -5.6503606174374e-05, -0.0020352143328636885, -0.000597179401665926, -0.0012049565557390451, 0.0, -8.821448318485636e-06, -1.2278481335670222e-05, -0.00026127262390218675, -2.264974000354414e-06, -8.940297266235575e-05, -7.033323527139146e-06, -6.0437283536884934e-05, -0.00410511763766408, -0.0017519139219075441], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.008942908607423306, -0.007984624244272709, -0.052772510796785355, -0.00021252757869660854, -0.0014887689612805843, -0.00021336186910048127, -0.03326082602143288, -0.0004363300104159862, -0.017532017081975937, -0.021731702610850334, -0.7931233048439026], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.0314033105969429, -0.0009279712685383856, -0.0020236745476722717, -3.182837463100441e-05, -3.6954811548639555e-06, -0.000801837770268321, -0.006354839541018009, -0.0001472126314183697, -0.0010486110113561153, -0.002541647292673588, -0.00013326710904948413, -0.006870337296277285, -6.663577369181439e-05, -0.0003131137927994132, -0.006370593793690205, -9.894321920000948e-06, -8.248942322097719e-05, -1.764281842042692e-05, -0.006178919691592455, -0.025598634034395218, -0.11594722419977188], "finish_reason": "stop"}, {"text": " [word for word, freq in words_freq[:10]]", "tokens": [" [", "word", " for", " word", ",", " fre", "q", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.6985524892807007, -0.5801299810409546, -0.4098661541938782, -0.07729359716176987, -0.0022250674664974213, -1.0447193384170532, -2.3483953555114567e-05, -0.0002536452084314078, -0.0012681784573942423, -9.536738616588991e-07, -4.768370445162873e-07, -0.14584338665008545, -0.008366062305867672, -0.004480917472392321, -0.0180372204631567, -0.0062958477064967155], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0009843033039942384, -4.0531076592742465e-06, -1.7165990357170813e-05, -0.006445689592510462, -0.12985952198505402, -5.960462772236497e-07, -3.099436753473128e-06, -0.00395266804844141, -0.0010422994382679462, -0.003181635867804289, -0.0003292018664069474, -0.5745425820350647, -0.0012994902208447456, -5.8412379075889476e-06, -5.817244164063595e-05, -0.09241573512554169, -0.0015281677478924394, -0.0016990762669593096, -0.02378898113965988], "finish_reason": "stop"}, {"text": " df.plot.bar(", "tokens": [" df", ".", "plot", ".", "bar", "(", "\n"], "logprobs": [-0.059969402849674225, -0.07151652127504349, -0.0017227584030479193, -0.31846603751182556, -0.0008454324561171234, -0.4506770968437195, -2.5304112434387207], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.0381012037396431, -7.033323527139146e-06, -2.1934269170742482e-05, -0.9594351649284363, -0.4428611993789673, -6.437280717364047e-06, -0.010337027721107006, -0.005849386565387249], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.00656116520985961, -0.0003630455757956952, -5.8410845667822286e-05, -0.0013750869547948241, -6.794906312279636e-06, -3.576272320060525e-06, -0.20011457800865173, -0.008318656124174595], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.060409627854824066, -0.06566357612609863, -0.10890220105648041], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04354582726955414, -0.008041506633162498], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.005463548935949802, -0.6054698824882507], "finish_reason": "stop"}], "metrics": {"energy": 1104.1491792774207, "energy_per_token": 5.111801755913985, "time": 28.576077, "time_per_token": 0.13229665277777777}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.replace(\"[^a-zA-Z0-9\\s]\", \"\", regex=True)  # Remove non-alphabetic characters", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "replace", "(", "\"", "[", "^", "a", "-", "z", "A", "-", "Z", "0", "-", "9", "\\", "s", "]\"", ",", " \"", "\"", ",", " regex", "=", "True", ")", "  ", "#", " Remove", " non", "-", "alph", "abetic", " characters", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8530066013336182, -0.03686662018299103, -0.9176292419433594, -0.6719261407852173, -1.0810647010803223, -0.031654197722673416, -5.1020273531321436e-05, -0.00012981049076188356, -0.006551217287778854, -2.002696055569686e-05, -0.0013404440833255649, -1.4619388580322266, -0.0024756519123911858, -6.735097849741578e-05, -0.853931725025177, -0.06740687787532806, -0.39043372869491577, -3.6954195820726454e-05, -0.00550362141802907, -0.06092728301882744, -1.175634503364563, -0.01205127127468586, -0.00023529145983047783, -0.0016142207896336913, -0.004694630391895771, -2.116276264190674, -0.001026341924443841, -0.48072677850723267, -0.8801857829093933, -0.028814075514674187, -2.1916401386260986, -0.06818653643131256, -0.3593583405017853, -0.08287455141544342], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -0.41199398040771484, -3.2186455882765586e-06, -4.6491513785440475e-06, -0.001842707279138267, -0.017574068158864975, 0.0, -8.4638240878121e-06, -2.145764938177308e-06, 0.0, 0.0, -0.19700554013252258, -0.016263552010059357], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"].values)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "]", ".", "values", ")", "\n"], "logprobs": [-0.3623025119304657, -2.5987286790041253e-05, -7.664863369427621e-05, -0.006578811444342136, -0.003461324144154787, -0.00025328766787424684, -0.014238080009818077, -0.018174076452851295, -0.08900271356105804, -0.0017191881779581308, -0.0005447572330012918, -0.00019298121333122253, -2.0354373455047607, -0.011352820321917534, -0.6191137433052063, -0.2168676108121872, -0.5554349422454834], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-2.4669501781463623, -0.009200792759656906], "finish_reason": "stop"}, {"text": "\"No valid words found in the input dataset.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " input", " dataset", ".", "\"", ")", "\n"], "logprobs": [-0.05920566990971565, -1.3059569597244263, -0.31097477674484253, -0.4660279452800751, -0.20546840131282806, -0.1299435943365097, -0.06054608151316643, -0.16484983265399933, -3.253293514251709, -0.11789333820343018, -0.06530538946390152, -0.00048744716332294047, -0.00037126801908016205], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.08735906332731247, -0.015620146878063679], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.009726513177156448, -6.794906312279636e-06, -1.6093124941107817e-05, -0.00010990492592100054, -0.0003499372396618128, -5.1020273531321436e-05, -1.883488948806189e-05, -0.0001722425949992612, -0.0010801678290590644, -0.000550357042811811, -0.07704993337392807], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.005098199471831322, -0.013706612400710583, -0.016846762970089912, -0.05686141923069954, -0.00024387246230617166, -0.0003393312799744308, -3.6954195820726454e-05, -0.019317707046866417, -0.0003887851198669523, -0.03085465356707573, -0.019976498559117317, -0.04394736886024475, -0.0029091201722621918, -5.090107151772827e-05, -0.0013228246243670583, -0.0007003477076068521, -0.000982397934421897, 0.0, -1.2516897186287679e-05, -1.3708974620385561e-05, -0.00027295202016830444, -2.7418097943154862e-06, -9.417090768693015e-05, -7.152531907195225e-06, -5.328513361746445e-05, -0.004094313830137253, -0.0015929164364933968], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.00948954839259386, -0.010225173085927963, -0.05065599083900452, -0.00011121608258690685, -0.000777776469476521, -0.0001938155182870105, -0.02839876338839531, -0.00026353701832704246, -0.017506247386336327, -0.028394127264618874, -0.8290709257125854], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.027713147923350334, -0.0010811204556375742, -0.002050560899078846, -4.5298504119273275e-05, -2.264974000354414e-06, -0.000854961050208658, -0.006203325465321541, -0.00013517419574782252, -0.0011373248416930437, -0.0020920787937939167, -7.795983401592821e-05, -0.005604032427072525, -7.128461584215984e-05, -0.00042655906872823834, -0.007079871371388435, -1.156323378381785e-05, -9.142934868577868e-05, -2.038458114839159e-05, -0.00604799622669816, -0.021615633741021156, -0.10705835372209549], "finish_reason": "stop"}, {"text": " [word for word, count in words_freq[:10]]", "tokens": [" [", "word", " for", " word", ",", " count", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.7423156499862671, -0.6175760626792908, -0.3464961051940918, -0.06238367035984993, -0.0018496087286621332, -1.360093593597412, -8.880697714630514e-05, -0.0011261318577453494, -9.536738616588991e-07, -4.768370445162873e-07, -0.15207834541797638, -0.008041860535740852, -0.003497794270515442, -0.014592587016522884, -0.005638883449137211], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Text\", \"Count\"]).sort_values(by=\"Count\")", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Text", "\"", ",", " \"", "Count", "\"", "])", ".", "sort", "_", "values", "(", "by", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.0014537728857249022, -2.264974000354414e-06, -2.1815061700181104e-05, -0.008702803403139114, -0.12765228748321533, -5.960462772236497e-07, -3.3378546504536644e-06, -0.004557341802865267, -0.0013017522869631648, -0.0038666974287480116, -0.0002026352594839409, -2.796128273010254, -0.0037667055148631334, -2.1934269170742482e-05, -0.0002057340752799064, -0.286441445350647, -0.15667079389095306, -0.003285607323050499, -2.891991376876831, -0.40655502676963806, -7.807903602952138e-05, -9.345571743324399e-05, -0.002046754118055105, -1.5511391162872314, -0.04509040713310242, -2.658331868587993e-05, -0.01058358233422041, -4.6491513785440475e-06, -0.09232226014137268, -0.002183911856263876], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0, title=\"Top 10 Words in Text\")", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ",", " title", "=", "\"", "Top", " ", "1", "0", " Words", " in", " Text", "\"", ")", "\n"], "logprobs": [-0.05042184144258499, -0.08578841388225555, -0.0011232740944251418, -0.3229644000530243, -0.0008780676289461553, -0.4704028367996216, -0.06618576496839523, -0.0001333863037871197, -4.327203714638017e-05, -0.00011228884250158444, -1.9073468138230965e-06, -0.0027948864735662937, -0.005447306204587221, -0.0024133387487381697, -8.106198947643861e-06, -8.487341256113723e-05, -1.0847986231965479e-05, -0.1725112795829773, -0.07682982832193375, -0.0010275328531861305, -0.05417678877711296, -1.1390281915664673, -0.564931333065033, -0.005421933718025684, -0.010083685629069805, -0.8328751921653748, -0.2950809895992279, -0.001293061301112175, -0.00012599628826137632, -0.13093747198581696, -2.3117566108703613, -0.8777192234992981, -1.125046968460083, -0.01692752167582512, -0.004138952121138573], "finish_reason": "stop"}, {"text": " save_path is None:", "tokens": [" save", "_", "path", " is", " None", ":", "\n"], "logprobs": [-0.03399069979786873, -6.794906312279636e-06, -1.6212332411669195e-05, -0.9406048059463501, -1.0540480613708496, -0.12014281749725342, -0.023003974929451942], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.006352707277983427, -0.00030417583184316754, -6.246371776796877e-05, -0.0012287693098187447, -5.364403477869928e-06, -2.9802276912960224e-06, -0.17410731315612793, -0.006590535398572683], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.08874530345201492, -0.0743340328335762, -0.08896781504154205], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.047398149967193604, -0.008915852755308151], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.005234820768237114, -0.5796248316764832], "finish_reason": "stop"}], "metrics": {"energy": 1305.3780796813955, "energy_per_token": 4.729630723483317, "time": 33.59059, "time_per_token": 0.12170503623188406}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -0.30648258328437805], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " pd.Series(vectorizer.fit_transform(df[\"Text\"].values).toarray().sum(axis=0), index=vectorizer.get_feature_names_out()).sort_values(ascending=False)", "tokens": [" pd", ".", "Series", "(", "vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "]", ".", "values", ")", ".", "to", "array", "()", ".", "sum", "(", "axis", "=", "0", ")", ",", " index", "=", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "())", ".", "sort", "_", "values", "(", "asc", "ending", "=", "False", ")", "\n"], "logprobs": [-1.5684478282928467, -6.305972783593461e-05, -0.3143887221813202, -0.005436279810965061, -0.39022597670555115, -5.1020273531321436e-05, -8.153582894010469e-05, -0.006843462120741606, -0.00020525732543319464, -2.109982233378105e-05, -0.0038347532972693443, -0.043425071984529495, -0.1488974243402481, -0.0013777059502899647, -0.0007348936051130295, -0.00017033556650858372, -2.2412352561950684, -0.011185086332261562, -0.5933356881141663, -0.19172793626785278, -0.019763806834816933, -0.9104704856872559, -0.00014208737411536276, -0.00472061475738883, -0.0019001776818186045, -0.005240157246589661, -0.0008057684754021466, -0.007363796699792147, -0.00035958975786343217, -0.0003923600015696138, -1.2832382917404175, -0.0054565537720918655, -0.5851223468780518, -0.00481991795822978, -0.0033519056160002947, -2.3841855067985307e-07, -0.0003197873884346336, -0.0014973392244428396, -5.602820692729438e-06, -1.0609570381348021e-05, 0.0, -6.627816765103489e-05, -0.13625235855579376, -5.674201020156033e-05, -0.008567837066948414, -0.9703353643417358, -0.21947874128818512, -7.033323527139146e-06, -4.3987260141875595e-05, -0.01632019877433777, -0.004750750958919525, -1.1920928244535389e-07, -0.0004768426588270813, -0.0025175088085234165, -0.03393193334341049, -0.35913345217704773], "finish_reason": "stop"}, {"text": " ValueError as e:", "tokens": [" Value", "Error", " as", " e", ":", "\n"], "logprobs": [-0.4593242406845093, -1.6689286894688848e-06, -1.866711139678955, -0.32966816425323486, -0.000856628583278507, -0.013471764512360096], "finish_reason": "stop"}, {"text": "\"The input data is either empty or contains only stop words.\")", "tokens": ["\"", "The", " input", " data", " is", " either", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.054576341062784195, -1.3673819303512573, -0.384626179933548, -1.7621909379959106, -0.2352052628993988, -1.0729337930679321, -0.010166526772081852, -0.02036472037434578, -0.3358837366104126, -0.005640780087560415, -0.007779065053910017, -0.08092431724071503, -0.08155925571918488, -0.043099138885736465, -0.0004702892620116472, -0.00020561488054227084], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07049763947725296, -0.015913741663098335], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.010235085152089596, -7.152531907195225e-06, -1.7404405298293568e-05, -8.451581379631534e-05, -0.00038378025055862963, -3.218599158572033e-05, -2.95634672511369e-05, -0.00018070495571009815, -0.0009747759322635829, -0.0006519814487546682, -0.07927712798118591], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.010184699669480324, -0.012671316042542458, -0.02388373203575611, -0.06727336347103119, -0.00031585473334416747, -0.0006306566647253931, -5.23315102327615e-05, -0.01118048932403326, -0.0003673355677165091, -0.021658213809132576, -0.022686466574668884, -0.044241663068532944, -0.004282708279788494, -5.829164365422912e-05, -0.0019477697787806392, -0.000750736624468118, -0.0012012654915452003, 0.0, -1.2516897186287679e-05, -1.2874520507466514e-05, -0.00045944625162519515, -2.7418097943154862e-06, -7.009260298218578e-05, -4.291525328881107e-06, -6.806619057897478e-05, -0.0045465431176126, -0.0015880366554483771], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.011330308392643929, -0.006951315328478813, -0.055669188499450684, -0.00011646069469861686, -0.0010439666220918298, -0.00014768941036891192, -0.028226561844348907, -0.0003097769513260573, -0.016194231808185577, -0.02678232453763485, -0.5163480639457703, -0.6013867855072021, -0.00010024998482549563, -0.0007350126979872584, -0.0009314250783063471, -0.03164010867476463, -1.1920928244535389e-07, -1.3232143828645349e-05, -4.2676016164477915e-05, -0.00025876989820972085, -1.9073468138230965e-06, -2.0503786799963564e-05, -5.245195097813848e-06, -1.0609570381348021e-05, -0.013567733578383923], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.039504677057266235, -0.0012453190283849835, -0.0019487215904518962, -4.7801782784517854e-05, -2.264974000354414e-06, -0.0008515069494023919, -0.006926099304109812, -0.00011264643399044871, -0.001158400671556592, -0.002887843642383814, -8.34430247778073e-05, -0.006668930407613516, -8.570780482841656e-05, -0.0003393312799744308, -0.006705994252115488, -1.490105023549404e-05, -9.464769391342998e-05, -1.8358061424805783e-05, -0.006537124048918486, -0.020560352131724358, -1.9321619272232056, -0.006126552354544401, -0.0019169541774317622, -0.0005488081369549036, -0.005830068606883287], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.8103496432304382, -8.344646857949556e-07, -3.576278118089249e-07, -0.01512405276298523, -0.005975833162665367, -0.00199083867482841, -0.0019629988819360733, -0.030785415321588516], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0011088658357039094, -2.861018856492592e-06, -2.1815061700181104e-05, -0.006724940147250891, -0.15310275554656982, -7.152555099310121e-07, -3.4570634852570947e-06, -0.003719793865457177, -0.0008450751192867756, -0.004320691339671612, -0.0002833203470800072, -0.9959178566932678, -0.0004797023138962686, -1.2516897186287679e-05, -7.164221460698172e-05, -0.44566985964775085, -0.024607103317975998, -0.0009042703313753009, -0.0029042467940598726, -0.018218090757727623], "finish_reason": "stop"}, {"text": " df.plot.barh(x=\"Word\", y=\"Count\", legend=False)", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.04900478571653366, -0.0830598697066307, -0.002019510604441166, -0.3111628293991089, -0.000707971747033298, -0.8560856580734253, -0.0016794399125501513, -0.10015913099050522, -0.0002299282787134871, -7.056941103655845e-05, -0.004586177412420511, -4.291525328881107e-06, -0.0009641766082495451, -0.05314323678612709, -0.0004447901446837932, -7.986990567587782e-06, -0.00012766500003635883, -1.0371154530730564e-05, -0.10377670079469681, -0.342678040266037, -1.2874520507466514e-05, -0.0029572586063295603, -0.27408143877983093, -0.0025969373527914286], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.032471682876348495, -6.437280717364047e-06, -2.5629668016335927e-05, -0.5111901164054871, -0.003960504662245512], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0074792844243347645, -0.00029762129997834563, -6.186770770000294e-05, -0.001073261140845716, -4.768360213347478e-06, -1.6689286894688848e-06, -0.1951320767402649, -0.00965567771345377], "finish_reason": "stop"}, {"text": "close(ax.figure)", "tokens": ["close", "(", "ax", ".", "figure", ")", "\n"], "logprobs": [-0.06007549166679382, -2.7600979804992676, -0.3308050036430359, -0.19520768523216248, -0.012371821328997612, -0.00067473528906703, -0.18203286826610565], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.03940451517701149, -0.009161340072751045], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.006443202495574951], "finish_reason": "stop"}], "metrics": {"energy": 1311.0597739028929, "energy_per_token": 4.63272004912683, "time": 33.739094, "time_per_token": 0.11921941342756184}}
{"outputs": [{"text": " pd.read_csv(file_path, encoding='utf-8')", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " encoding", "='", "utf", "-", "8", "')", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -2.093414783477783, -1.5585204362869263, -0.5601299405097961, -0.7897725701332092, -0.018194327130913734, -0.005366087891161442, -0.29769057035446167, -0.07400596141815186], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.replace(r\"[^\\w\\s]\", \"\")  # Remove punctuation", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "replace", "(", "r", "\"", "[", "^\\", "w", "\\", "s", "]\"", ",", " \"", "\"", ")", "  ", "#", " Remove", " punct", "uation", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8530066013336182, -0.03686662018299103, -0.5351066589355469, -0.0717620924115181, -0.43940263986587524, -0.28418439626693726, -0.004647287540137768, -0.006018491927534342, -0.004493022337555885, -0.07108870893716812, -8.05822346592322e-05, -0.01076545100659132, -0.05595419183373451, -0.5941842198371887, -1.846498966217041, -0.0005860995152033865, -0.27796831727027893, -0.6771735548973083, -0.022518524900078773, -0.1412520706653595], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -1.6521062850952148], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.3539600968360901, -2.4914430468925275e-05, -6.12716976320371e-05, -0.012970485724508762, -0.0033378859516233206, -0.00029869386344216764, -0.017540685832500458, -0.02443099208176136, -0.10681615769863129, -0.001796776195988059, -0.0005147324409335852, -0.00021300431399140507, -0.17036493122577667, -0.6040729284286499], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.36196017265319824, -1.7881377516459906e-06, -0.18933852016925812, -0.07577545940876007], "finish_reason": "stop"}, {"text": "\"The input file is empty or contains only stop words.\")", "tokens": ["\"", "The", " input", " file", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.061218976974487305, -1.2113831043243408, -0.4156884253025055, -0.5708483457565308, -0.13178002834320068, -0.35972535610198975, -0.018624547868967056, -0.1088901236653328, -0.005254624877125025, -0.004045636393129826, -0.026381375268101692, -0.11935313791036606, -0.07144205272197723, -0.0004166688595432788, -0.00025376438861712813], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07890671491622925, -0.017573131248354912], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.011157972738146782, -9.059865078597795e-06, -1.5735502529423684e-05, -0.00010716341057559475, -0.0005479741375893354, -3.290122185717337e-05, -2.109982233378105e-05, -0.00016699827392585576, -0.0010012142593041062, -0.0007239343249239028, -0.07931523025035858], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.009258786216378212, -0.016209715977311134, -0.015329399146139622, -0.07873153686523438, -0.000350175570929423, -0.00029905137489549816, -3.1709168979432434e-05, -0.021665329113602638, -0.00028427375946193933, -0.028275351971387863, -0.013022142462432384, -0.03830590099096298, -0.004191898740828037, -6.401333666872233e-05, -0.0019212374463677406, -0.000964533886872232, -0.001110890181735158, 0.0, -1.3112935448589269e-05, -1.3589766240329482e-05, -0.0002809368306770921, -3.814689989667386e-06, -7.843663479434326e-05, -7.629365427419543e-06, -5.721882189391181e-05, -0.0033715092577040195, -0.0018312842585146427], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.007786871865391731, -0.00841500237584114, -0.052524153143167496, -0.00016044282529037446, -0.001517931348644197, -0.00022873646230436862, -0.026125330477952957, -0.00041940953815355897, -0.015856485813856125, -0.02426113374531269, -0.6050354838371277, -0.44991278648376465, -3.671578815556131e-05, -0.008056760765612125, -0.00034505134681239724, -0.01139961089938879, -4.005352093372494e-05, -0.0021569100208580494, -1.1920928244535389e-07, -8.821448318485636e-06, -0.0014423454413190484, -1.1920928244535389e-07, -4.9828242481453344e-05, 0.0, -8.22540732769994e-06, -0.38943156599998474, -0.001531976624391973], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.025441408157348633, -0.001141373417340219, -0.002504071919247508, -4.672895011026412e-05, -2.622600959512056e-06, -0.0010116941994056106, -0.006297269370406866, -0.00015341058315243572, -0.0010156240314245224, -0.003131844801828265, -7.688703772146255e-05, -0.004657254554331303, -9.572047565598041e-05, -0.0003687655262183398, -0.006727663800120354, -1.2755313036905136e-05, -8.928377064876258e-05, -1.9788545614574105e-05, -0.0063604069873690605, -0.023649517446756363, -0.07968130707740784], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.7465273141860962, -1.0728830375228426e-06, -3.576278118089249e-07, -0.014823648147284985, -0.006209248676896095, -0.002356849145144224, -0.0019296851241961122, -0.030137987807393074], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=top_words, columns=[\"word\", \"word_freq\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "word", "_", "freq", "\"", "])", "\n"], "logprobs": [-0.0011322046630084515, -2.145764938177308e-06, -1.5258672647178173e-05, -0.010363219305872917, -2.7556257247924805, -0.03257934749126434, -0.07736829668283463, -5.960462772236497e-07, -3.6954811548639555e-06, -0.020986169576644897, -0.0020353333093225956, -0.05140374228358269, -0.0006317288498394191, -0.5614237785339355, -0.0016514724120497704, -6.9141146923357155e-06, -0.00012194366718176752, -2.5609099864959717, -0.09742046892642975, -1.3057081699371338, -0.028897928074002266, -0.010105517692863941, -0.023409422487020493], "finish_reason": "stop"}, {"text": " df.plot.barh(x=\"Word\", y=\"Count\", legend=False)", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.048504315316677094, -0.06690450012683868, -0.002305113710463047, -0.41219452023506165, -0.0009133219718933105, -1.0026962757110596, -0.0016212427290156484, -0.09578529745340347, -0.0002699726028367877, -7.116541382856667e-05, -0.00696102250367403, -3.933898824470816e-06, -0.0006872437661513686, -0.015938498079776764, -0.0005828827270306647, -7.390948667307384e-06, -0.00014232576359063387, -1.2636104656849056e-05, -0.10952411592006683, -0.4900742769241333, -1.585470999998506e-05, -0.0029091201722621918, -0.33275339007377625, -0.002312249969691038], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.02591429091989994, -7.867782187531702e-06, -2.3245540432981215e-05, -0.4394093453884125, -0.004365913104265928], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.005864081904292107, -0.0004107108688913286, -6.139089964563027e-05, -0.0014366315444931388, -6.318072337307967e-06, -2.145764938177308e-06, -0.19875483214855194, -0.008212133310735226], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.07388472557067871, -0.06971244513988495, -0.10958950221538544], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.03596808388829231, -0.009529695846140385], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.005832083523273468, -0.651024580001831], "finish_reason": "stop"}], "metrics": {"energy": 1212.8286088562015, "energy_per_token": 4.910237282818629, "time": 31.31398, "time_per_token": 0.12677724696356277}}
{"outputs": [{"text": " pd.read_csv(file_path, encoding='utf-8')", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " encoding", "='", "utf", "-", "8", "')", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -2.093414783477783, -1.5585204362869263, -0.5601299405097961, -0.7897725701332092, -0.018194327130913734, -0.005366087891161442, -0.29769057035446167, -0.07400596141815186], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)  # ensure 'Text' column is treated as strings", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "  ", "#", " ensure", " '", "Text", "'", " column", " is", " treated", " as", " strings", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -1.6845190525054932, -0.000120751719805412, -3.318777084350586, -2.1790802478790283, -0.009819419123232365, -0.0001573438785271719, -0.050211671739816666, -0.03140169382095337, -2.7228965759277344, -0.0004345426568761468, -1.2665579319000244, -0.006162688601762056], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " pd.Series(vectorizer.fit_transform(df[\"Text\"]).sum(axis=0).A1).sort_values(ascending=False)", "tokens": [" pd", ".", "Series", "(", "vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "sum", "(", "axis", "=", "0", ")", ".", "A", "1", ")", ".", "sort", "_", "values", "(", "asc", "ending", "=", "False", ")", "\n"], "logprobs": [-1.5684478282928467, -6.305972783593461e-05, -0.3143887221813202, -0.005436279810965061, -0.39022597670555115, -5.1020273531321436e-05, -8.153582894010469e-05, -0.006843462120741606, -0.00020525732543319464, -2.109982233378105e-05, -0.0038347532972693443, -0.043425071984529495, -0.1488974243402481, -0.0013777059502899647, -0.0007348936051130295, -0.00017033556650858372, -0.11527013033628464, -0.005947629921138287, -0.7577686309814453, -3.075552376685664e-05, -0.0011294659925624728, -0.0002727136597968638, -0.0005956306122243404, -0.02379061095416546, -0.026646526530385017, -0.04431271180510521, -0.13470400869846344, -0.002902345033362508, -0.18435561656951904, -0.014452890492975712, -9.846202738117427e-05, -0.005847964435815811, -0.011997914873063564, -0.02880840003490448, -7.152555099310121e-07, -0.0005265279905870557, -0.0018072477541863918, -0.022316748276352882, -0.2532274127006531], "finish_reason": "stop"}, {"text": " IndexError:", "tokens": [" Index", "Error", ":", "\n"], "logprobs": [-3.329214572906494, -0.00010477947944309562, -0.12529271841049194, -0.05073440447449684], "finish_reason": "stop"}, {"text": "\"Input file is empty or contains only stop words.\")", "tokens": ["\"", "Input", " file", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.05747298523783684, -2.2236480712890625, -0.5636186599731445, -0.08925525844097137, -0.21978795528411865, -0.016579939052462578, -0.1441713273525238, -0.009412200190126896, -0.0048632193356752396, -0.03916817530989647, -0.30543747544288635, -0.09090863913297653, -0.00033802041434682906, -0.0003289635351393372], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.06141440197825432, -0.01444172766059637], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.010343752801418304, -7.271740287251305e-06, -1.3947389561508317e-05, -7.760223525110632e-05, -0.00026294111739844084, -5.566918844124302e-05, -1.9907753085135482e-05, -0.000192504478036426, -0.000990615226328373, -0.0009353553177788854, -0.060937490314245224], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.008751136250793934, -0.013290269300341606, -0.01417449675500393, -0.04577992856502533, -0.000270091783022508, -0.0003232433518860489, -3.576214658096433e-05, -0.013022612780332565, -0.0001113352773245424, -0.02858564630150795, -0.015756746754050255, -0.029056116938591003, -0.003575956914573908, -5.8887653722194955e-05, -0.0017728578532114625, -0.0005697772721759975, -0.0009557208395563066, 0.0, -1.3947389561508317e-05, -1.2159273865108844e-05, -0.0004530118894763291, -2.145764938177308e-06, -9.011816291604191e-05, -5.602820692729438e-06, -4.994744449504651e-05, -0.0035961500834673643, -0.0017200212460011244], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.012656837701797485, -0.007729028817266226, -0.0578390471637249, -0.00014673586701974273, -0.0009427393670193851, -0.00013302871957421303, -0.0286844614893198, -0.0002522150462027639, -0.02489493414759636, -0.015747828409075737, -0.6805392503738403], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.035593122243881226, -0.001444011926651001, -0.001774523756466806, -2.8371408916427754e-05, -2.264974000354414e-06, -0.0006790239713154733, -0.007088393904268742, -0.00012206286191940308, -0.0009072478278540075, -0.003034631721675396, -8.237022848334163e-05, -0.006264336407184601, -5.781483559985645e-05, -0.0003493413969408721, -0.006707296706736088, -1.1324817933200393e-05, -9.214453893946484e-05, -1.9311717551317997e-05, -0.006180104333907366, -0.01872236281633377, -0.08988133072853088], "finish_reason": "stop"}, {"text": " [x[0] for x in words_freq if x[0] not in STOP_WORDS][:10]", "tokens": [" [", "x", "[", "0", "]", " for", " x", " in", " words", "_", "freq", " if", " x", "[", "0", "]", " not", " in", " ST", "OP", "_", "W", "OR", "DS", "][", ":", "1", "0", "]", "\n"], "logprobs": [-0.6823388338088989, -1.9831472635269165, -0.04300345107913017, -0.00671901972964406, -0.0003474347176961601, -0.0004814896092284471, -0.0012163867941126227, -0.00011562632425921038, -0.0010318199638277292, -2.0265558760002023e-06, -4.768370445162873e-07, -3.0990617275238037, -0.16269131004810333, -0.00792052410542965, -0.43508222699165344, -0.02171117253601551, -0.06476083397865295, -2.9802276912960224e-06, -0.008926840499043465, 0.0, -1.0728830375228426e-06, -1.9073468138230965e-06, 0.0, -7.152555099310121e-07, -0.23300421237945557, -0.10962304472923279, -0.0057448516599833965, -0.002003211760893464, -0.0004203628050163388, -0.002292744582518935], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0008605591137893498, -1.7881377516459906e-06, -6.9141146923357155e-06, -0.008534269407391548, -0.08172471076250076, -5.960462772236497e-07, -3.576272320060525e-06, -0.004259442910552025, -0.0010140759404748678, -0.003829528111964464, -0.00031871485407464206, -0.5782405734062195, -0.0010420613689348102, -7.152531907195225e-06, -5.4834770708112046e-05, -0.10940508544445038, -0.0021496538538485765, -0.0012880609137937427, -0.021458595991134644], "finish_reason": "stop"}, {"text": " df[\"Count\"].plot(", "tokens": [" df", "[", "\"", "Count", "\"", "]", ".", "plot", "(", "\n"], "logprobs": [-0.05621454492211342, -2.728759527206421, -0.0008630603551864624, -0.00995598640292883, -3.933898824470816e-06, -0.0017872564494609833, -0.00033563701435923576, -0.0205958541482687, -0.13064461946487427, -0.137489914894104], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.02306816354393959, -7.271740287251305e-06, -1.9788545614574105e-05, -0.9669167995452881, -0.4187772274017334, -6.198863957251888e-06, -0.01485559530556202, -0.005197702441364527], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.005932461470365524, -0.00028951745480298996, -5.400034933700226e-05, -0.0011021974496543407, -5.602820692729438e-06, -2.145764938177308e-06, -0.1823132485151291, -0.006412169896066189], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.09566819667816162, -0.09837449342012405, -0.09532515704631805], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.042730383574962616, -0.009310754016041756], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.005901412572711706], "finish_reason": "stop"}], "metrics": {"energy": 1323.7948887500772, "energy_per_token": 4.958033291198791, "time": 34.59867, "time_per_token": 0.12958303370786517}}
{"outputs": [{"text": " pd.read_csv(file_path, header=None, names=[\"Text\"])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " header", "=", "None", ",", " names", "=[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -2.093414783477783, -2.4337053298950195, -0.01698295958340168, -0.7022395133972168, -0.5499776601791382, -0.3049733340740204, -0.23452001810073853, -0.0075080362148582935, -0.07339578121900558, -0.0012721073580905795, -0.1480492353439331, -0.14240218698978424], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -0.30648258328437805], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.40920716524124146, -2.7417760065873154e-05, -5.0424259825376794e-05, -0.008076036348938942, -0.004196765832602978, -0.00030155404238030314, -0.01675954833626747, -0.020926518365740776, -0.09813385456800461, -0.0014137759571895003, -0.0006649667047895491, -0.00011586471373448148, -0.15590016543865204, -0.5511701703071594], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.41866397857666016, -1.6689286894688848e-06, -0.21533997356891632, -0.07270398736000061], "finish_reason": "stop"}, {"text": "\"No text data found.\")", "tokens": ["\"", "No", " text", " data", " found", ".", "\"", ")", "\n"], "logprobs": [-0.058882370591163635, -1.3703662157058716, -2.751763105392456, -0.5516294240951538, -0.29140666127204895, -2.363957643508911, -0.7230915427207947, -0.0020342625211924314, -0.0019903627689927816], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.09007177501916885, -0.01740526594221592], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0).tolist()[0]", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", ".", "tol", "ist", "()[", "0", "]", "\n"], "logprobs": [-0.010264347307384014, -9.65590606938349e-06, -1.3947389561508317e-05, -0.00012146688823122531, -0.00038044367101974785, -3.1470757676288486e-05, -2.276871418871451e-05, -0.00017510310863144696, -0.0009753713966347277, -0.0006107610533945262, -2.8758106231689453, -0.8955101370811462, -1.1920928244535389e-07, -0.02541351690888405, -0.000510204816237092, -0.00033241944038309157, -0.012566432356834412], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.005467105656862259, -0.013422482647001743, -0.014073062688112259, -0.0572163350880146, -0.00023505308490712196, -0.00043501926120370626, -2.7656173188006505e-05, -0.022194450721144676, -0.00035851728171110153, -0.023194432258605957, -0.02172505483031273, -0.04142945259809494, -0.004370423499494791, -6.341733387671411e-05, -0.0014969820622354746, -0.0006461439770646393, -0.000979539705440402, 0.0, -1.3232143828645349e-05, -9.775113539944869e-06, -0.0002857038634829223, -2.9802276912960224e-06, -7.235741941258311e-05, -7.390948667307384e-06, -5.495397272170521e-05, -0.0035691861994564533, -0.0015981532633304596], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for idx, word in enumerate(vectorizer.get_feature_names())", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " idx", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-0.009716714732348919, -0.007576303090900183, -0.07731091976165771, -0.00014172980445437133, -0.0011559001868590713, -0.00012611546844709665, -0.030090099200606346, -0.0004024887748528272, -0.015378821641206741, -0.02115611918270588, -0.5818183422088623, -0.8845322728157043, -3.8265450712060556e-05, -0.008618894964456558, -0.0004472924047149718, -0.01808030530810356, -3.123234637314454e-05, -0.002406560117378831, -1.1920928244535389e-07, -8.4638240878121e-06, -0.0007074952009133995, -1.1920928244535389e-07, -4.565611743601039e-05, 0.0, -8.22540732769994e-06, -0.37302401661872864, -0.001538642100058496], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.04420311376452446, -0.0011570908827707171, -0.0019555033650249243, -4.732496745418757e-05, -1.9073468138230965e-06, -0.0006796196103096008, -0.006437517236918211, -0.00013028726971242577, -0.0011152960360050201, -0.002950840163975954, -8.546940807718784e-05, -0.006331859156489372, -5.507317473529838e-05, -0.0003383779258001596, -0.006666680797934532, -1.6689160474925302e-05, -9.7508447652217e-05, -2.372236667724792e-05, -0.005257589276880026, -0.02270790934562683, -0.10476858913898468], "finish_reason": "stop"}, {"text": " [words for words, count in words_freq[:10]]", "tokens": [" [", "words", " for", " words", ",", " count", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.7597986459732056, -3.576195240020752, -1.9458242654800415, -0.04248397424817085, -0.0006660388899035752, -0.40287795662879944, -0.0007164295529946685, -0.002137520583346486, -3.6954811548639555e-06, -7.152555099310121e-07, -0.4111391603946686, -0.00760528864338994, -0.003946018870919943, -0.024227390065789223, -0.006712033413350582], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0010180057724937797, -2.145764938177308e-06, -9.179073458653875e-06, -0.008835624903440475, -0.12420181185007095, -7.152555099310121e-07, -3.4570634852570947e-06, -0.004553188569843769, -0.0010215784423053265, -0.0032075406052172184, -0.0003327769518364221, -1.1659142971038818, -0.00045122456504032016, -1.1444026313256472e-05, -6.508615479106084e-05, -0.36237841844558716, -0.025292526930570602, -0.0009274948388338089, -0.0033962209708988667, -0.01741182804107666], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.043147094547748566, -0.055791519582271576, -0.0011093420907855034, -0.20599612593650818, -0.0007219092221930623, -0.39614439010620117, -0.06931678205728531, -0.00011979816190432757, -4.51792984677013e-05, -0.00017355366435367614, -1.9073468138230965e-06, -0.003500170074403286, -0.005962086841464043, -0.0018083186587318778, -7.986990567587782e-06, -7.509902934543788e-05, -1.0132738680113107e-05, -0.09048066288232803, -0.10783179104328156, -0.0014492495683953166, -0.0808127224445343, -0.37484049797058105, -0.005636157002300024], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.03356393054127693, -6.794906312279636e-06, -1.680836794548668e-05, -0.9932820796966553, -0.44159257411956787, -8.702239938429557e-06, -0.010371478274464607, -0.004293509759008884], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.005219048820436001, -0.0002828436263371259, -6.937739817658439e-05, -0.0014969820622354746, -5.602820692729438e-06, -1.4305104514278355e-06, -0.15621447563171387, -0.008477769792079926], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.09827541559934616, -0.07483989000320435, -0.0956781655550003], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.049095701426267624, -0.010987140238285065], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.004248877987265587, -0.5757927894592285], "finish_reason": "stop"}], "metrics": {"energy": 1190.4647319507594, "energy_per_token": 4.724066396629998, "time": 30.70592, "time_per_token": 0.12184888888888888}}
{"outputs": [{"text": " pd.read_csv(file_path)  # Read CSV file into a DataFrame", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "  ", "#", " Read", " CS", "V", " file", " into", " a", " Data", "Frame", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -2.8299341201782227, -0.00016282663273159415, -0.5233489274978638, -1.0065875053405762, -8.940656698541716e-06, -0.22303730249404907, -0.8178713917732239, -0.5556440353393555, -0.7012583613395691, -0.004170055501163006, -0.13787342607975006], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str).str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -2.628652811050415, -0.28045332431793213, -0.00027092601521871984, -0.5358991622924805, -0.00037520044133998454, -0.31676197052001953], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -1.618343710899353], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.40831175446510315, -3.433168603805825e-05, -5.781483559985645e-05, -0.010303991846740246, -0.0041359844617545605, -0.0002450642641633749, -0.01690947264432907, -0.016153765842318535, -0.09501907229423523, -0.0017995130037888885, -0.0006517431465908885, -0.00013410145766101778, -0.13625818490982056, -0.5005525350570679], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.40799906849861145, -1.5497195136049413e-06, -0.22453895211219788, -0.07221867889165878], "finish_reason": "stop"}, {"text": "\"Error: The input file is empty or contains only stop words.\")", "tokens": ["\"", "Error", ":", " The", " input", " file", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.05433215573430061, -3.03383469581604, -0.045461028814315796, -0.4733673632144928, -0.29483699798583984, -0.6411214470863342, -0.15994791686534882, -0.2758272886276245, -0.014875679276883602, -0.09875720739364624, -0.003607196733355522, -0.003976297099143267, -0.029020914807915688, -0.09521017223596573, -0.026211731135845184, -0.0005695389700122178, -0.0002361257211305201], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.09004911780357361, -0.020028963685035706], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.01121986098587513, -6.794906312279636e-06, -1.2397689715726301e-05, -0.00010501786891836673, -0.00040284625720232725, -2.0146166207268834e-05, -3.0517112463712692e-05, -0.00013398226292338222, -0.0007932615117169917, -0.0007581220706924796, -0.08496181666851044], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.0067327553406357765, -0.010548785328865051, -0.010200276039540768, -0.06402681022882462, -0.00018737945356406271, -0.0003667397249955684, -2.658331868587993e-05, -0.014955423772335052, -0.0003177614707965404, -0.02515385113656521, -0.024890748783946037, -0.03126317262649536, -0.0026088273152709007, -5.352353764465079e-05, -0.0020427091512829065, -0.0006158839096315205, -0.0013131813611835241, 0.0, -1.0013530300057027e-05, -1.156323378381785e-05, -0.0003084660565946251, -2.50339189733495e-06, -9.083335316972807e-05, -5.364403477869928e-06, -6.55629628454335e-05, -0.004821460228413343, -0.0015217402251437306], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.01281985267996788, -0.00711573613807559, -0.052222106605768204, -0.00011777184408856556, -0.0009220162755809724, -0.00026008085114881396, -0.04186202585697174, -0.0003797286772169173, -0.018497467041015625, -0.029020335525274277, -0.8800060153007507], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.03540397807955742, -0.0010896942112594843, -0.0027759848162531853, -4.970903682988137e-05, -3.099436753473128e-06, -0.0006301801186054945, -0.006658628117293119, -0.00011526874004630372, -0.0013735393295064569, -0.002975800074636936, -9.023735765367746e-05, -0.007735298480838537, -6.115249561844394e-05, -0.00035363141796551645, -0.0061747729778289795, -1.2636104656849056e-05, -0.00011085849109804258, -1.490105023549404e-05, -0.007074426393955946, -0.022372471168637276, -0.08797627687454224], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.7128334045410156, -8.344646857949556e-07, -3.576278118089249e-07, -0.014839974232017994, -0.006306272000074387, -0.002198542468249798, -0.002138948068022728, -0.04176199063658714], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0009709649602882564, -3.099436753473128e-06, -1.3470558769768104e-05, -0.009632772766053677, -0.09897889941930771, -7.152555099310121e-07, -3.3378546504536644e-06, -0.0033283811062574387, -0.00110041128937155, -0.0036682477220892906, -0.00021228920377325267, -0.6690462231636047, -0.0011378012131899595, -8.821448318485636e-06, -4.8040190449682996e-05, -0.10008762776851654, -0.001816648175008595, -0.0016210046596825123, -0.019658729434013367], "finish_reason": "stop"}, {"text": " df.plot.barh(x=\"Word\", y=\"Count\", figsize=(10, 5), legend=False)", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " fig", "size", "=(", "1", "0", ",", " ", "5", ")", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.043689850717782974, -0.04784326255321503, -0.0010008569806814194, -0.39678847789764404, -0.0006074252305552363, -0.9180454611778259, -0.0018830440239980817, -0.10029005259275436, -0.0003060825983993709, -5.98412734689191e-05, -0.010355315171182156, -4.172316494077677e-06, -0.0006431656656786799, -0.029066886752843857, -0.0004724340105894953, -6.6756979322235566e-06, -0.00013386306818574667, -1.0132738680113107e-05, -0.14511540532112122, -1.2002993822097778, -3.3378546504536644e-06, -0.0025531810242682695, -0.13371917605400085, -0.041508596390485764, -0.0005035324720665812, -0.00019167017308063805, -0.29622703790664673, -0.04495490714907646, -1.8954096958623268e-05, -2.1412081718444824, -8.153582894010469e-05, -0.009383149445056915, -0.11305670440196991, -0.0017417988274246454], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03258442506194115, -7.033323527139146e-06, -1.7881233361549675e-05, -0.4798003137111664, -0.004381817765533924], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=300, bbox_inches=\"tight\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "3", "0", "0", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ")", "\n"], "logprobs": [-0.00772311445325613, -0.00024720950750634074, -6.425174069590867e-05, -0.001091480371542275, -5.483612312673358e-06, -2.145764938177308e-06, -1.9523850679397583, -1.402109980583191, -4.768360213347478e-06, -0.0009261847590096295, -0.2530333399772644, -0.0045468988828361034, -0.00013171759201213717, -1.4602439403533936, -0.14032267034053802, -1.311301275563892e-06, -6.615896563744172e-05, -1.966933996300213e-05, -2.634490556374658e-05, -0.003026431193575263, -0.00035684893373399973, -1.1920922133867862e-06, -3.2186455882765586e-06, -0.040747109800577164, -0.007777054328471422], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.08470457047224045, -0.07805411517620087, -0.10635881125926971], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04361053928732872, -0.009313470683991909], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.0055965641513466835], "finish_reason": "stop"}], "metrics": {"energy": 1233.386042400361, "energy_per_token": 4.973330816130488, "time": 31.318167, "time_per_token": 0.1262829314516129}}
{"outputs": [{"text": " pd.read_csv(file_path, dtype=str)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " dtype", "=", "str", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -2.093414783477783, -2.254937171936035, -0.10474582761526108, -0.2925608456134796, -0.23197542130947113, -0.16254287958145142], "finish_reason": "stop"}, {"text": " df[\"Text\"].fillna(\"\")  # Fill NaN values with empty strings", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "fill", "na", "(", "\"", "\"", ")", "  ", "#", " Fill", " Na", "N", " values", " with", " empty", " strings", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -1.9498767852783203, -2.3841855067985307e-07, -0.014480502344667912, -0.004825493786484003, -0.03064403124153614, -6.90197994117625e-05, -1.7950419187545776, -0.00011681827891152352, -1.7017900943756104, -1.6927156448364258, -0.044395845383405685, -0.0514267273247242, -0.05381356179714203, -0.18843895196914673, -0.5941640734672546, -0.02027255669236183], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).sum(axis=0).A1", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "sum", "(", "axis", "=", "0", ")", ".", "A", "1", "\n"], "logprobs": [-0.40920716524124146, -2.7417760065873154e-05, -5.0424259825376794e-05, -0.008076036348938942, -0.004196765832602978, -0.00030155404238030314, -0.01675954833626747, -0.020926518365740776, -0.09813385456800461, -0.0014137759571895003, -0.0006649667047895491, -0.00011586471373448148, -0.15590016543865204, -0.8676221966743469, -0.6699320077896118, -4.470248313737102e-05, -0.003995888400822878, -0.0002544794406276196, -0.0006578188622370362, -0.00011145447206217796, -0.37987980246543884, -0.11060061305761337, -0.49595680832862854, -0.01232448685914278], "finish_reason": "stop"}, {"text": " Exception as e:", "tokens": [" Exception", " as", " e", ":", "\n"], "logprobs": [-1.7983407974243164, -0.27075955271720886, -0.07820725440979004, -0.0011868583969771862, -0.008565708994865417], "finish_reason": "stop"}, {"text": "\"No valid words found in the input. Please provide a different file.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " input", ".", " Please", " provide", " a", " different", " file", ".", "\"", ")", "\n"], "logprobs": [-0.055234089493751526, -1.352028727531433, -0.30598366260528564, -0.4086001515388489, -0.22762416303157806, -0.1178937628865242, -0.04498932510614395, -0.13438640534877777, -0.9874183535575867, -1.773236870765686, -1.6413347721099854, -1.2548165321350098, -2.806878089904785, -0.535248875617981, -0.19670702517032623, -0.001540784491226077, -0.00042215018766000867, -0.00016485285595990717], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07481466978788376, -0.016628244891762733], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.015189923346042633, -6.6756979322235566e-06, -1.537788011773955e-05, -0.00011467275908216834, -0.0002824861148837954, -4.303362584323622e-05, -2.288792165927589e-05, -0.0001463782973587513, -0.0009097489528357983, -0.0006422125734388828, -0.08713006973266602], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.007206872571259737, -0.014825526624917984, -0.012114047072827816, -0.0592101626098156, -0.00020954797219019383, -0.00035506143467500806, -3.290122185717337e-05, -0.020117761567234993, -0.0003094194398727268, -0.026206621900200844, -0.036239903420209885, -0.03801444545388222, -0.002092435723170638, -6.508615479106084e-05, -0.002052226336672902, -0.000706780469045043, -0.0011102947173640132, 0.0, -1.0967194612021558e-05, -1.0251946150674485e-05, -0.0004119024670217186, -2.7418097943154862e-06, -8.475421054754406e-05, -6.556489552167477e-06, -5.149708886165172e-05, -0.00439261831343174, -0.002023079665377736], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.012571258470416069, -0.007739911787211895, -0.05281197652220726, -0.00013815402053296566, -0.0013144910335540771, -0.00012385078298393637, -0.03592588007450104, -0.00021395778458099812, -0.01996200904250145, -0.025550296530127525, -0.4716976583003998, -0.4941692352294922, -0.00012158608296886086, -0.001211862312629819, -0.0007531190640293062, -0.03242044523358345, -1.1920928244535389e-07, -1.7165990357170813e-05, -2.8967437174287625e-05, -0.00015984688070602715, -1.5497195136049413e-06, -1.4066597032069694e-05, -4.768360213347478e-06, -1.2874520507466514e-05, -0.01435818150639534], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.04492185637354851, -0.0008039817912504077, -0.002188432030379772, -2.8609820219571702e-05, -1.7881377516459906e-06, -0.0008329259580932558, -0.007274686358869076, -0.00012516192509792745, -0.0008756855386309326, -0.0029157765675336123, -9.846202738117427e-05, -0.007977173663675785, -5.125868119648658e-05, -0.000300600629998371, -0.005043047480285168, -1.2040065485052764e-05, -0.0001248043408850208, -1.7523612768854946e-05, -0.006943975109606981, -0.015671784058213234, -2.2455077171325684, -0.005850689951330423, -0.0017137139802798629, -0.000557267339900136, -0.004467862658202648], "finish_reason": "stop"}, {"text": " [word for word, _ in words_freq[:10]]", "tokens": [" [", "word", " for", " word", ",", " _", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-0.7913841605186462, -0.6968382596969604, -0.3494187295436859, -0.06616802513599396, -0.001901724492199719, -1.768385887145996, -0.0044339201413095, -0.0012736550997942686, -1.6689286894688848e-06, -4.768370445162873e-07, -0.08590307086706161, -0.007040690630674362, -0.004080898128449917, -0.01119899656623602, -0.010719102807343006], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.0009397619287483394, -3.099436753473128e-06, -1.6927575416048057e-05, -0.00630473205819726, -0.10018026828765869, -4.768370445162873e-07, -3.2186455882765586e-06, -0.005391580518335104, -0.0010888606775552034, -0.0036634968128055334, -0.0002057340752799064, -0.9990940690040588, -0.0004781533498317003, -1.811964830267243e-05, -6.711257447022945e-05, -1.1225814819335938, -0.015237356536090374, -0.0025522299110889435, -0.02726072072982788], "finish_reason": "stop"}, {"text": " df.plot(x=\"Word\", y=\"Count\", kind=\"bar\", legend=False, rot=45, figsize=(10, 8))", "tokens": [" df", ".", "plot", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " kind", "=", "\"", "bar", "\"", ",", " legend", "=", "False", ",", " rot", "=", "4", "5", ",", " fig", "size", "=(", "1", "0", ",", " ", "8", "))", "\n"], "logprobs": [-0.0630088821053505, -0.07741816341876984, -0.0010699268896132708, -1.6718461513519287, -0.7297330498695374, -0.00014828535495325923, -0.00014709345123264939, -0.0006264869589358568, -2.145764938177308e-06, -0.000125281119835563, -0.00524608651176095, -0.001513408264145255, -1.883488948806189e-05, -0.00016783259343355894, -3.135155202471651e-05, -0.00040546778473071754, -0.005230907816439867, -5.757642793469131e-05, -3.015949550899677e-05, -7.629103492945433e-05, -0.043198127299547195, -0.02674936316907406, -1.1872496604919434, -6.496695277746767e-05, -0.023130251094698906, -1.2347168922424316, -0.6262577772140503, -0.017616119235754013, -2.0638837814331055, -0.03591955453157425, -2.227023124694824, -1.8674967288970947, -0.0002790300059132278, -0.002619171282276511, -0.3551502525806427, -0.5376940369606018, -0.0015103134792298079, -0.013641114346683025, -3.3409953117370605, -0.01811753399670124, -0.007853582501411438], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03504733368754387, -8.22540732769994e-06, -1.847726889536716e-05, -0.4634236693382263, -0.005433552898466587], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.004427274223417044, -0.00032872517476789653, -6.663577369181439e-05, -0.0012400802224874496, -6.437280717364047e-06, -1.9073468138230965e-06, -0.1466999351978302, -0.005980336107313633], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.10206977277994156, -0.08940766751766205, -0.08632908016443253], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.053708840161561966, -0.008157152682542801], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.004226442892104387], "finish_reason": "stop"}], "metrics": {"energy": 1325.2571981678007, "energy_per_token": 4.650025256729125, "time": 34.015392, "time_per_token": 0.11935225263157895}}
{"outputs": [{"text": " pd.read_csv(file_path, usecols=[\"Text\"])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " use", "cols", "=[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -2.093414783477783, -1.774027943611145, -0.0003014348621945828, -0.3088325262069702, -0.009692040272057056, -0.05044235661625862, -0.00038699767901562154, -0.1395024210214615, -0.1456359624862671], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8732607364654541, -0.00027926836628466845, -0.3627833425998688], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -1.6521062850952148], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"].values)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "]", ".", "values", ")", "\n"], "logprobs": [-0.3539600968360901, -2.4914430468925275e-05, -6.12716976320371e-05, -0.012970485724508762, -0.0033378859516233206, -0.00029869386344216764, -0.017540685832500458, -0.02443099208176136, -0.10681615769863129, -0.001796776195988059, -0.0005147324409335852, -0.00021300431399140507, -1.8619123697280884, -0.009108892641961575, -0.6704585552215576, -0.1829475462436676, -0.6235015988349915], "finish_reason": "stop"}, {"text": " ValueError as e:", "tokens": [" Value", "Error", " as", " e", ":", "\n"], "logprobs": [-0.36196017265319824, -1.7881377516459906e-06, -1.7822550535202026, -0.33132532238960266, -0.0007765853079035878, -0.013197568245232105], "finish_reason": "stop"}, {"text": "\"Invalid input. Please provide valid text data.\")", "tokens": ["\"", "Invalid", " input", ".", " Please", " provide", " valid", " text", " data", ".", "\"", ")", "\n"], "logprobs": [-0.05638154223561287, -2.8628177642822266, -0.4416499733924866, -0.9072916507720947, -0.5149999260902405, -1.097981572151184, -1.0019762516021729, -0.3962918221950531, -0.03783034905791283, -0.3616270124912262, -0.006501711905002594, -0.0002553137019276619, -0.0003398079425096512], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.08151025325059891, -0.014601279981434345], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.012846097350120544, -8.34461570775602e-06, -1.3589766240329482e-05, -0.00011073929636040702, -0.0002932118659373373, -2.7894584491150454e-05, -2.9682672902708873e-05, -0.00017712931730784476, -0.0008262557676061988, -0.00038115866482257843, -0.05762694403529167], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.005597038194537163, -0.011199821718037128, -0.010912974365055561, -0.06286414712667465, -0.00024279984063468874, -0.0003134713042527437, -3.5523738915799186e-05, -0.029038747772574425, -0.00017796363681554794, -0.03443226218223572, -0.019542183727025986, -0.03734056279063225, -0.003000046359375119, -6.961580220377073e-05, -0.0017254954436793923, -0.000754786713514477, -0.0008806879632174969, 0.0, -8.106198947643861e-06, -1.168244216387393e-05, -0.0004637358069885522, -2.861018856492592e-06, -9.548207890475169e-05, -4.529942543740617e-06, -7.116541382856667e-05, -0.003734045661985874, -0.0014363934751600027], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.012118051759898663, -0.00862350407987833, -0.04786121845245361, -0.00016437610611319542, -0.0004412154376041144, -0.00022933237778488547, -0.03054957278072834, -0.0005427317810244858, -0.025330882519483566, -0.020128393545746803, -0.9748991131782532], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.040335122495889664, -0.0011593532981351018, -0.0022395786363631487, -2.9801878554280847e-05, -3.099436753473128e-06, -0.0007117837085388601, -0.005903308745473623, -0.00011872540198964998, -0.0014423454413190484, -0.0030706420075148344, -9.870042413240299e-05, -0.007870731875300407, -6.437094270950183e-05, -0.0002951186615973711, -0.006216238718479872, -1.1801649634435307e-05, -0.00010990492592100054, -1.6689160474925302e-05, -0.005470662377774715, -0.023360978811979294, -0.14389801025390625], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.9307426810264587, -9.536738616588991e-07, -3.576278118089249e-07, -0.018862517550587654, -0.0070089660584926605, -0.0023225971963256598, -0.0024240415077656507, -0.034283559769392014], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.001303299912251532, -2.145764938177308e-06, -1.7881233361549675e-05, -0.006258413195610046, -0.10334817320108414, -7.152555099310121e-07, -3.814689989667386e-06, -0.005215965677052736, -0.0009676303598098457, -0.003255545860156417, -0.00028618055512197316, -0.6165164113044739, -0.0013263961300253868, -8.940656698541716e-06, -5.745722592109814e-05, -0.12787888944149017, -0.0015920833684504032, -0.002138353418558836, -0.02587258815765381], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.04689494147896767, -0.05941979959607124, -0.0009435730753466487, -0.279146671295166, -0.0008204193436540663, -0.5414697527885437, -0.1053161472082138, -0.00011193125828867778, -4.172238186583854e-05, -0.00016830935783218592, -2.0265558760002023e-06, -0.004114377778023481, -0.006983159575611353, -0.0020976699888706207, -1.0371154530730564e-05, -9.810443589231e-05, -1.156323378381785e-05, -0.19056542217731476, -0.09129814058542252, -0.001581966644152999, -0.07565134763717651, -0.2980937659740448, -0.005393121857196093], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.04147874563932419, -6.9141146923357155e-06, -2.5152843591058627e-05, -0.48721417784690857, -0.004897621460258961], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.004972235299646854, -0.00044764988706447184, -6.05564855504781e-05, -0.0013153243344277143, -6.318072337307967e-06, -2.0265558760002023e-06, -0.20110881328582764, -0.007485555484890938], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.08533491939306259, -0.063162662088871, -0.08420062065124512], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05040019005537033, -0.00942306499928236], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.0047418526373803616, -1.7477205991744995, -1.2516897186287679e-05, -1.6689286894688848e-06, -1.2755313036905136e-05, -3.099393507000059e-05, -0.16307905316352844], "finish_reason": "stop"}], "metrics": {"energy": 1070.8070566539773, "energy_per_token": 4.91195897547696, "time": 28.157875, "time_per_token": 0.1291645642201835}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9791278839111328, -5.245195097813848e-06, -0.005629874300211668, -0.03960654139518738, -0.003621806390583515, -0.30648258328437805], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010901705827564, -6.198863957251888e-06, -7.152555099310121e-07, -0.01337061170488596, -0.4073680341243744, -3.576272320060525e-06, -5.006777428206988e-06, -0.001777260797098279, -0.01795339025557041, 0.0, -1.07287787614041e-05, -2.7418097943154862e-06, 0.0, 0.0, -0.2203412652015686, -0.015561229549348354], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.40920716524124146, -2.7417760065873154e-05, -5.0424259825376794e-05, -0.008076036348938942, -0.004196765832602978, -0.00030155404238030314, -0.01675954833626747, -0.020926518365740776, -0.09813385456800461, -0.0014137759571895003, -0.0006649667047895491, -0.00011586471373448148, -0.15590016543865204, -0.5511701703071594], "finish_reason": "stop"}, {"text": " IndexError:", "tokens": [" Index", "Error", ":", "\n"], "logprobs": [-3.305779457092285, -0.00011872540198964998, -0.14914155006408691, -0.05471178516745567], "finish_reason": "stop"}, {"text": "\"No valid data found. Please check the input file.\")", "tokens": ["\"", "No", " valid", " data", " found", ".", " Please", " check", " the", " input", " file", ".", "\"", ")", "\n"], "logprobs": [-0.055234089493751526, -1.352028727531433, -0.30598366260528564, -1.7699549198150635, -0.575868546962738, -1.3340280055999756, -0.6377384066581726, -0.36671027541160583, -0.30195751786231995, -0.07999388873577118, -0.543015718460083, -0.2214072197675705, -0.0028267446905374527, -0.00026913834153674543, -0.0004702892620116472], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.07788915932178497, -0.017769543454051018], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.012712513096630573, -7.271740287251305e-06, -1.4543427823809907e-05, -8.95221673999913e-05, -0.0004917366313748062, -2.9682672902708873e-05, -1.966933996300213e-05, -0.00022373080719262362, -0.0007873057620599866, -0.00038115866482257843, -0.06896001845598221], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.004648949019610882, -0.009198784828186035, -0.012805964797735214, -0.043009161949157715, -0.0002203936892328784, -0.0003022690652869642, -1.9311717551317997e-05, -0.030036885291337967, -0.00030882356804795563, -0.025046665221452713, -0.019021600484848022, -0.04155125096440315, -0.0026857517659664154, -5.602679812000133e-05, -0.0017406088300049305, -0.0004664763400796801, -0.001206861576065421, 0.0, -1.1444026313256472e-05, -1.7165990357170813e-05, -0.00025900822947733104, -2.9802276912960224e-06, -8.165503095369786e-05, -6.198863957251888e-06, -5.07818695041351e-05, -0.003649600315839052, -0.0019527667900547385], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.007863162085413933, -0.008617948740720749, -0.05765169858932495, -0.00016318420239258558, -0.0012171011185273528, -0.00013982271775603294, -0.031034953892230988, -0.0002991705550812185, -0.016901619732379913, -0.024416916072368622, -0.7837270498275757], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.033821649849414825, -0.001013718661852181, -0.0016454027500003576, -4.434487345861271e-05, -2.861018856492592e-06, -0.0008263748604804277, -0.006210789084434509, -0.00010096516780322418, -0.0008721124031580985, -0.002815451705828309, -0.00013755806139670312, -0.005479198880493641, -5.9960475482512265e-05, -0.0003146630187984556, -0.0074896966107189655, -1.3470558769768104e-05, -9.965400386136025e-05, -2.2172682292875834e-05, -0.0060546319000422955, -0.01974627561867237, -0.12349741160869598], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.7617164254188538, -9.536738616588991e-07, -2.3841855067985307e-07, -0.013753293082118034, -0.006768750958144665, -0.0021152759436517954, -0.002161787124350667, -0.02658616565167904], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.0010457528987899423, -3.576272320060525e-06, -1.2755313036905136e-05, -0.006305561400949955, -0.128773495554924, -5.960462772236497e-07, -3.6954811548639555e-06, -0.004012985620647669, -0.0010712367948144674, -0.0034508698154240847, -0.0002743821241892874, -0.6246753334999084, -0.0012768696760758758, -6.9141146923357155e-06, -5.781483559985645e-05, -0.12516629695892334, -0.0016696811653673649, -0.0018615075387060642, -0.021927418187260628], "finish_reason": "stop"}, {"text": " df.plot.barh(x=\"Word\", y=\"Count\", legend=False)", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.05915926769375801, -0.08611878007650375, -0.0011829291470348835, -0.28019481897354126, -0.0008149401983246207, -1.1173006296157837, -0.0020801827777177095, -0.07158078253269196, -0.0001951265730895102, -6.687417771900073e-05, -0.005583642516285181, -3.576272320060525e-06, -0.0006828360492363572, -0.04939880967140198, -0.0003738896339200437, -7.748573807475623e-06, -0.00013326710904948413, -1.1801649634435307e-05, -0.1764744073152542, -0.48753130435943604, -1.680836794548668e-05, -0.0038592161145061255, -0.25795578956604004, -0.0025720868725329638], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.03859119117259979, -6.437280717364047e-06, -1.5139465176616795e-05, -0.4572661221027374, -0.005173272453248501], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.005739992018789053, -0.00024375328212045133, -6.985420623095706e-05, -0.0011841197265312076, -5.006777428206988e-06, -2.264974000354414e-06, -0.18806131184101105, -0.007126980926841497], "finish_reason": "stop"}, {"text": "clf()", "tokens": ["cl", "f", "()", "\n"], "logprobs": [-2.5417044162750244, -0.021008115261793137, -0.0002475670480635017, -0.6103603839874268], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.06241682171821594, -0.009438416920602322], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|", ">\""], "logprobs": [-0.0037943765055388212, -1.8482575416564941, -1.6689160474925302e-05, -1.6689286894688848e-06, -1.0847986231965479e-05, -4.184158387943171e-05, -1.9088131189346313, -0.0020590072963386774], "finish_reason": "stop"}], "metrics": {"energy": 1187.9483857574462, "energy_per_token": 5.3271228060872025, "time": 30.477665, "time_per_token": 0.13667114349775783}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.028327034786343575, -4.2676016164477915e-05, -0.0006591292913071811, -0.00041559641249477863, -0.00021407696476671845, -0.0007447806419804692, -0.006485012359917164, -0.0002540027489885688, -1.5497195136049413e-06, -0.13720935583114624, -0.08588501811027527], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.replace(r\"\\W+\", \" \").str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "replace", "(", "r", "\"", "\\", "W", "+", "\"", ",", " \"", " \"", ")", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.02159685082733631, -0.02501399628818035, -0.0004172646440565586, -0.0006177900941111147, -1.6331539882230572e-05, -0.00032693761750124395, -0.006361709907650948, -0.9762210845947266, -0.00010168035078095272, -0.8530066013336182, -0.03686662018299103, -0.5351066589355469, -0.0717620924115181, -1.4834890365600586, -1.875661849975586, -1.2491421699523926, -0.0107523612678051, -0.00014661667228210717, -0.04520856961607933, -0.026420271024107933, -0.701377809047699, -1.1574256420135498, -0.03480662405490875, -0.00012432756193447858, -0.4057242274284363, -0.0005830018781125546, -0.3879663646221161], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0010480155469849706, -4.0531076592742465e-06, -7.152555099310121e-07, -0.01476115919649601, -0.41199398040771484, -3.2186455882765586e-06, -4.6491513785440475e-06, -0.001842707279138267, -0.017574068158864975, 0.0, -8.4638240878121e-06, -2.145764938177308e-06, 0.0, 0.0, -0.19700554013252258, -0.016263552010059357], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"].dropna())", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "]", ".", "drop", "na", "())", "\n"], "logprobs": [-0.3623025119304657, -2.5987286790041253e-05, -7.664863369427621e-05, -0.006578811444342136, -0.003461324144154787, -0.00025328766787424684, -0.014238080009818077, -0.018174076452851295, -0.08900271356105804, -0.0017191881779581308, -0.0005447572330012918, -0.00019298121333122253, -2.0354373455047607, -0.011352820321917534, -2.6674954891204834, -0.05225853621959686, -0.17509791254997253, -0.3945474326610565], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-0.43807297945022583, -1.7881377516459906e-06, -0.19968245923519135, -0.0924256220459938], "finish_reason": "stop"}, {"text": "\"No data available or all words are stop words.\")", "tokens": ["\"", "No", " data", " available", " or", " all", " words", " are", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.06172657385468483, -1.2838648557662964, -2.53769850730896, -2.567209243774414, -1.0609771013259888, -0.6635894775390625, -0.5036982893943787, -0.45837539434432983, -0.07882959395647049, -0.2482050061225891, -0.16448411345481873, -0.03222908079624176, -0.00034540885826572776, -0.0003843760641757399], "finish_reason": "stop"}, {"text": "", "tokens": ["\n"], "logprobs": [-2.933000087738037], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.013046146370470524, -6.079655122448457e-06, -2.2053474822314456e-05, -0.00010597144137136638, -0.0003033416287507862, -5.328513361746445e-05, -2.5152843591058627e-05, -0.00018046658078674227, -0.0007058274932205677, -0.0008403107640333474, -0.0811467096209526], "finish_reason": "stop"}, {"text": " [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", " for", " word", ",", " idx", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "]", "\n"], "logprobs": [-0.006595627870410681, -0.011346926912665367, -0.016029421240091324, -0.06747763603925705, -0.0001932195882545784, -0.0003328961320221424, -3.182837463100441e-05, -0.027566922828555107, -0.0002944036095868796, -0.024486597627401352, -0.02503073774278164, -0.04826716333627701, -0.0020350953564047813, -5.674201020156033e-05, -0.0014632957754656672, -0.0009248746791854501, -0.0010078833438456059, 0.0, -1.0132738680113107e-05, -1.0490362910786644e-05, -0.0002712835557758808, -2.3841830625315197e-06, -8.046303264563903e-05, -5.483612312673358e-06, -4.732496745418757e-05, -0.00413171062245965, -0.001939084380865097], "finish_reason": "stop"}, {"text": "word, sum_words[0, idx])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", ",", " idx", "])", "\n"], "logprobs": [-0.01213925052434206, -0.009220872074365616, -0.05011008679866791, -0.00016604475968051702, -0.001149470335803926, -0.00017510310863144696, -0.04021340236067772, -0.00028165188268758357, -0.017066873610019684, -0.015702413395047188, -0.8203230500221252], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.024977603927254677, -0.0009396428358741105, -0.001733468729071319, -4.386805812828243e-05, -3.4570634852570947e-06, -0.0007200032705441117, -0.007858668453991413, -0.00011705666838679463, -0.0015430459752678871, -0.0021962826140224934, -9.178694017464295e-05, -0.006103329826146364, -4.8397800128441304e-05, -0.00032217081752605736, -0.006439175456762314, -1.0847986231965479e-05, -9.822363062994555e-05, -2.2411095415009186e-05, -0.007303799036890268, -0.017815450206398964, -0.1033148467540741], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.6854259967803955, -1.0728830375228426e-06, -3.576278118089249e-07, -0.017144450917840004, -0.0071356212720274925, -0.002066264161840081, -0.001965973060578108, -0.02926580049097538], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.0007668177131563425, -2.264974000354414e-06, -1.0967194612021558e-05, -0.005825327709317207, -0.09348050504922867, -4.768370445162873e-07, -3.814689989667386e-06, -0.0038430660497397184, -0.0009076051646843553, -0.003324578981846571, -0.00025960413040593266, -1.1172137260437012, -0.0004634975048247725, -1.0490362910786644e-05, -8.451581379631534e-05, -1.2778253555297852, -0.014827759005129337, -0.003026312217116356, -0.025454191491007805], "finish_reason": "stop"}, {"text": " df.plot.barh(x=\"Word\", y=\"Count\", figsize=(10, 5), legend=False)", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " fig", "size", "=(", "1", "0", ",", " ", "5", ")", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.05940946191549301, -0.06633827835321426, -0.0015326907159760594, -0.2806425094604492, -0.0007951673469506204, -1.147484540939331, -0.0021219374611973763, -0.09147265553474426, -0.0002774807217065245, -7.152301259338856e-05, -0.008517131209373474, -4.6491513785440475e-06, -0.0006617502076551318, -0.03591587394475937, -0.0006198153714649379, -6.794906312279636e-06, -0.00013195598148740828, -1.168244216387393e-05, -0.1759195774793625, -1.4670779705047607, -3.3378546504536644e-06, -0.0020649556536227465, -0.1227562427520752, -0.04628643020987511, -0.0004686211177613586, -0.00028010259848088026, -0.3881186246871948, -0.16280579566955566, -2.396077979938127e-05, -2.081770181655884, -9.7508447652217e-05, -0.010891631245613098, -0.10078898072242737, -0.0017514378996565938], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.03805415332317352, -6.079655122448457e-06, -2.372236667724792e-05, -0.9855000972747803, -0.4096166491508484, -7.033323527139146e-06, -0.01023496687412262, -0.005099504254758358], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.006546953693032265, -0.0002426806604489684, -6.19869097135961e-05, -0.0010311054065823555, -5.602820692729438e-06, -1.9073468138230965e-06, -0.16715148091316223, -0.00824606604874134], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.06757179647684097, -0.07158466428518295, -0.07475116848945618], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.039829835295677185, -0.010274848900735378], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.006962324492633343, -0.6390264630317688], "finish_reason": "stop"}], "metrics": {"energy": 1177.6957730016707, "energy_per_token": 4.806921522455799, "time": 30.878447, "time_per_token": 0.1260344775510204}}
