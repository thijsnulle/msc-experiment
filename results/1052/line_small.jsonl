{"outputs": [{"text": " pd.read_csv(file_path, usecols=[\"Text\"])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " use", "cols", "=[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -1.1519023180007935, -1.7836906909942627, -0.0002790300059132278, -0.11544199287891388, -0.4571756422519684, -0.049763187766075134, -0.001157924416474998, -0.09974868595600128, -0.09944028407335281], "finish_reason": "stop"}, {"text": " df[\"Text\"].apply(str.strip)  # Remove leading/trailing spaces", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "apply", "(", "str", ".", "strip", ")", "  ", "#", " Remove", " leading", "/", "tra", "iling", " spaces", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.800536870956421, -0.011861036531627178, -1.9939435720443726, -1.905624508857727, -0.3917306661605835, -0.0014118712861090899, -1.8435511589050293, -0.0009326160652562976, -0.8139739632606506, -0.7475687265396118, -0.5663208961486816, -0.0021759422961622477, -0.00015317220822907984, -1.9255964756011963, -0.2962121367454529], "finish_reason": "stop"}, {"text": " CountVectorizer(lowercase=True, stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "low", "erc", "ase", "=", "True", ",", " stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -2.5649709701538086, -0.00028701478731818497, -5.972207145532593e-05, -0.0007078525959514081, -0.04806233569979668, -0.03318829461932182, -0.1467299908399582, -5.8887653722194955e-05, -5.709961988031864e-05, -0.01859845407307148, -0.05762356519699097, -7.152555099310121e-07, -0.0003238391946069896, -3.099436753473128e-06, -1.9073468138230965e-06, -2.3603161025675945e-05, -0.21733658015727997, -0.01592852547764778], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.07234232872724533, -1.1205610462639015e-05, -5.829164365422912e-05, -0.008356131613254547, -0.0031547797843813896, -0.0002703301142901182, -0.01030812133103609, -0.012856689281761646, -0.057394981384277344, -0.0003927174839191139, -8.368142152903602e-05, -3.957670196541585e-05, -0.07278767228126526, -0.6697254776954651], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-1.0949742794036865, -0.02648923173546791], "finish_reason": "stop"}, {"text": "\"Empty or only stopwords detected. No words found.\")", "tokens": ["\"", "Empty", " or", " only", " stop", "words", " detected", ".", " No", " words", " found", ".", "\"", ")", "\n"], "logprobs": [-0.05673933029174805, -2.088240623474121, -0.7686524987220764, -0.46165990829467773, -0.0551360584795475, -2.491732120513916, -2.3399062156677246, -1.1185095310211182, -2.231370687484741, -2.060643196105957, -0.8396144509315491, -0.5550689697265625, -0.02602916583418846, -0.0009411911014467478, -0.0006677066558040679], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.2605212926864624, -0.009277093224227428], "finish_reason": "stop"}, {"text": " pd.Series(word_count.sum(axis=0).toarray()[0])", "tokens": [" pd", ".", "Series", "(", "word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", ".", "to", "array", "()[", "0", "])", "\n"], "logprobs": [-2.5775604248046875, -1.823885577323381e-05, -0.2044467329978943, -0.010324757546186447, -0.2809809148311615, -5.757642793469131e-05, -8.415821503149346e-05, -0.0006145734223537147, -0.07877428829669952, -0.00041345154750160873, -0.0024641170166432858, -0.0008721124031580985, -0.19158798456192017, -0.17501945793628693, -0.9127387404441833, -0.36000069975852966, -0.0801798403263092, -1.1487293243408203, -0.0012943708570674062, -1.4120049476623535, -0.22954753041267395], "finish_reason": "stop"}, {"text": " word_count.data.sum(axis=0) / word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "data", ".", "sum", "(", "axis", "=", "0", ")", " /", " word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-2.1118597984313965, -0.001745130866765976, -1.3470558769768104e-05, -0.03351793438196182, -2.868499755859375, -0.09966722130775452, -2.948289394378662, -0.06853233277797699, -0.0017874945187941194, -0.0018134353449568152, -0.0415121391415596, -0.01855410635471344, -0.8083041906356812, -3.0751118659973145, -6.282132380874828e-05, -7.033323527139146e-06, -0.0005296258023008704, -0.7596461176872253, -0.4146645665168762, -0.004827036056667566, -0.00033778208307921886, -0.04535007104277611, -0.002646517474204302, -0.3388822078704834], "finish_reason": "stop"}, {"text": "word, int(count)) for word, count in zip(vectorizer.get_feature_names(), sum_words)", "tokens": ["word", ",", " int", "(", "count", "))", " for", " word", ",", " count", " in", " zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ",", " sum", "_", "words", ")", "\n"], "logprobs": [-0.14137470722198486, -0.055707525461912155, -2.6944499015808105, -0.01246871892362833, -0.47595450282096863, -0.809929609298706, -0.6811002492904663, -0.14827002584934235, -0.00023183519078884274, -0.0008562712464481592, -0.002918510464951396, -0.1174091249704361, -0.0030812190379947424, -0.04330327361822128, -4.172316494077677e-06, -4.970903682988137e-05, -0.048961538821458817, -3.6954811548639555e-06, -0.0002933310461230576, -2.3841855067985307e-07, -0.0011689979583024979, -0.5624957084655762, -0.0007034449372440577, -0.11776375025510788, -0.0008249455713666975, -1.07287787614041e-05, -0.19859425723552704, -0.037769511342048645], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: -x[1])", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " -", "x", "[", "1", "])", "\n"], "logprobs": [-0.09250082820653915, -0.0002706876548472792, -0.10086851567029953, -0.0006889115320518613, -5.829164365422912e-05, -0.001704074558801949, -0.016961628571152687, -0.0011144625023007393, -0.001388300908729434, -0.04915664717555046, -0.0002714027068577707, -1.0384477376937866, -0.005705974064767361, -0.009337327443063259, -0.0004435985756572336, -0.0018509175861254334, -0.11146505177021027], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5543574094772339, -0.00010871296399272978, -5.07818695041351e-05, -0.1505982130765915, -0.01686258614063263, -0.006718190852552652, -0.0014261561445891857, -0.010751181282103062], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0074945478700101376, -0.00200654286891222, -0.001734539750032127, -0.05507242679595947, -0.3584243059158325, -0.000120751719805412, -1.0251946150674485e-05, -0.05636351555585861, -0.006326884031295776, -0.01786475069820881, -0.0009966888464987278, -0.21700157225131989, -0.002382894279435277, -9.393251093570143e-05, -0.0007367995567619801, -0.6967384815216064, -0.09240138530731201, -0.003864203579723835, -0.0025148927234113216, -0.04354149103164673], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", title=\"Top 10 Words in File\")", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " title", "=", "\"", "Top", " ", "1", "0", " Words", " in", " File", "\"", ")", "\n"], "logprobs": [-0.08501394093036652, -0.03450194001197815, -0.19625255465507507, -0.28237929940223694, -0.009805489331483841, -0.46764835715293884, -0.4233112335205078, -0.0489264614880085, -0.0010483728256076574, -0.0008836655179038644, -4.351044481154531e-05, -0.005274665541946888, -0.054796088486909866, -0.001994288759306073, -0.00023183519078884274, -3.862306402879767e-05, -2.074220174108632e-05, -0.3056795299053192, -2.1513755321502686, -0.007668462116271257, -0.01637309044599533, -0.8367198705673218, -0.08901241421699524, -0.0019980960059911013, -0.0002079985715681687, -1.098053216934204, -1.1956679821014404, -3.145036220550537, -0.058491840958595276, -0.5171107649803162, -0.005463430192321539], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.013870279304683208, -2.2649508537142538e-05, -3.3378546504536644e-06, -0.48497772216796875, -0.002539031207561493], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0020899376831948757, -0.00010275312524754554, -7.319182623177767e-05, -0.004640643019229174, -3.576272320060525e-06, -8.940656698541716e-06, -0.12233714759349823, -0.0038035206962376833], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.1265847533941269, -0.06544854491949081, -0.17514224350452423], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.057580579072237015, -0.008824990130960941], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.02307736687362194, -1.291677713394165], "finish_reason": "stop"}], "metrics": {"energy": 428.33911754989623, "energy_per_token": 1.6224966573859705, "time": 16.38229, "time_per_token": 0.06205412878787879}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(\"str\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "\"", "str", "\"", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -2.2947709560394287, -0.00012635385792236775, -0.01766648329794407, -1.414400339126587, -0.5224848389625549, -1.5020257706055418e-05, -0.001658732071518898, -0.11866489797830582], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -0.3859751522541046, -0.012796196155250072], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.08213822543621063, -1.0490362910786644e-05, -5.185469490243122e-05, -0.007920405827462673, -0.0035200081765651703, -0.00023398046323563904, -0.009247093461453915, -0.012697681784629822, -0.06290388107299805, -0.0004122599493712187, -9.48860906646587e-05, -2.992108420585282e-05, -0.0737694650888443, -0.5549620985984802], "finish_reason": "stop"}, {"text": " AttributeError:", "tokens": [" Attribute", "Error", ":", "\n"], "logprobs": [-2.082515001296997, -0.00010394509445177391, -0.1640866994857788, -0.04667549580335617], "finish_reason": "stop"}, {"text": "\"Error: DataFrame is empty or contains only stop words\")", "tokens": ["\"", "Error", ":", " Data", "Frame", " is", " empty", " or", " contains", " only", " stop", " words", "\"", ")", "\n"], "logprobs": [-0.05799452215433121, -2.6647391319274902, -0.25934910774230957, -1.7676281929016113, -0.19336897134780884, -0.07119151949882507, -0.02180997096002102, -0.06849459558725357, -0.04019565507769585, -0.02455371618270874, -0.008464531041681767, -0.040450870990753174, -0.6229190826416016, -0.0014491304755210876, -0.0008497203234583139], "finish_reason": "stop"}, {"text": "", "tokens": ["\n"], "logprobs": [-1.503070592880249], "finish_reason": "stop"}, {"text": " word_count.sum(axis=1)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "1", ")", "\n"], "logprobs": [-0.9166378974914551, -9.775113539944869e-06, -5.4596363042946905e-05, -0.0005173536483198404, -0.048206958919763565, -0.26646578311920166, -0.0014323461800813675, -0.001379848807118833, -1.5102325677871704, -0.018618697300553322, -0.29209575057029724], "finish_reason": "stop"}, {"text": " [f for f in zip(vectorizer.get_feature_names_out(), sum_words[0])]", "tokens": [" [", "f", " for", " f", " in", " zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "()", ",", " sum", "_", "words", "[", "0", "]", ")]", "\n"], "logprobs": [-3.2851006984710693, -3.738551616668701, -1.953706979751587, -0.38453221321105957, -0.12721338868141174, -1.3696147203445435, -0.017491955310106277, -0.47622716426849365, -1.156323378381785e-05, -7.009260298218578e-05, -0.09580457955598831, -7.629365427419543e-06, -0.0002602000313345343, -3.576278118089249e-07, -0.0011813811725005507, -0.8655363917350769, -3.766942609217949e-05, -0.00761392479762435, -0.00817914493381977, -0.5763990879058838, -0.002427490195259452, -6.9141146923357155e-06, -3.027961254119873, -0.17193548381328583, -0.08346208930015564, -0.06391118466854095, -0.005280120298266411], "finish_reason": "stop"}, {"text": "word, count)", "tokens": ["word", ",", " count", ")", "\n"], "logprobs": [-0.12461002916097641, -0.05556375905871391, -0.9145636558532715, -0.2354016900062561, -0.9197517037391663], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.08495470136404037, -0.0003297977091278881, -0.05883269011974335, -0.0008385241380892694, -5.531158240046352e-05, -0.0018546062055975199, -0.01593298465013504, -0.0010627818992361426, -0.0014347269898280501, -0.05358082801103592, -0.00027581225731410086, -0.4659452736377716, -0.008844959549605846, -0.0018016549292951822, -0.050756726413965225, -0.00014923889830242842, -0.0009461931767873466, -0.0001530530134914443, -0.0001951265730895102, -0.014042736031115055, -0.2870175838470459], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.6148278713226318, -0.00011073929636040702, -6.496695277746767e-05, -0.14871086180210114, -0.016420116648077965, -0.007167223375290632, -0.0014922209084033966, -0.012351215817034245], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.007648705970495939, -0.001405204995535314, -0.0017437029164284468, -0.05594968423247337, -0.39201784133911133, -8.689979586051777e-05, -9.65590606938349e-06, -0.05582748353481293, -0.004511653911322355, -0.016884740442037582, -0.0009862087899819016, -0.2188880890607834, -0.0021234839223325253, -0.00010787858627736568, -0.0006667536217719316, -0.7318397760391235, -0.009165829047560692, -0.0018862566212192178, -0.031044313684105873], "finish_reason": "stop"}, {"text": " df.plot(kind=\"barh\", figsize=(10, 3))", "tokens": [" df", ".", "plot", "(", "kind", "=", "\"", "bar", "h", "\"", ",", " fig", "size", "=(", "1", "0", ",", " ", "3", "))", "\n"], "logprobs": [-0.08166967332363129, -0.03763175383210182, -0.16222497820854187, -1.4238775968551636, -0.5228365063667297, -0.0904981940984726, -9.238292841473594e-05, -0.013255449011921883, -0.16183872520923615, -5.531158240046352e-05, -0.032467182725667953, -1.3353261947631836, -0.00019524575327523053, -0.014304125681519508, -0.41840606927871704, -0.6526259183883667, -0.0031368357595056295, -0.01463816873729229, -3.093247413635254, -1.1902531385421753, -0.00951552577316761], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.014780540950596333, -2.5629668016335927e-05, -2.7418097943154862e-06, -0.9031772017478943, -0.04042499512434006, -1.1086402082582936e-05, -0.011468201875686646, -0.0010825494537129998], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0018642442300915718, -0.00010144196130568162, -7.045020902296528e-05, -0.0036366537678986788, -2.622600959512056e-06, -1.2993727978027891e-05, -0.1271810680627823, -0.0038541099056601524], "finish_reason": "stop"}, {"text": "close()  # Cleanup to avoid memory leakage", "tokens": ["close", "()", "  ", "#", " Clean", "up", " to", " avoid", " memory", " leakage", "\n"], "logprobs": [-0.14894159138202667, -0.05402026325464249, -1.8460386991500854, -0.0004638549580704421, -1.739196538925171, -1.666662335395813, -2.134108543395996, -0.905466616153717, -0.3059881329536438, -2.5008597373962402, -0.1024639829993248], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04638988524675369, -0.01024417020380497], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.018191050738096237, -1.2014250755310059], "finish_reason": "stop"}], "metrics": {"energy": 359.2653571491241, "energy_per_token": 1.6404810828727128, "time": 14.098765, "time_per_token": 0.06437792237442923}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].fillna(\"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "fill", "na", "(", "\"", "\"", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.75266432762146, -1.0251946150674485e-05, -0.023330699652433395, -0.09762462228536606, -0.26671287417411804, -0.000740968796890229, -0.3735211491584778], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -0.3859751522541046, -0.012796196155250072], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", "\n"], "logprobs": [-0.08213822543621063, -1.0490362910786644e-05, -5.185469490243122e-05, -0.007920405827462673, -0.0035200081765651703, -0.00023398046323563904, -0.009247093461453915, -0.012697681784629822, -0.06290388107299805, -0.0004122599493712187, -9.48860906646587e-05, -2.992108420585282e-05, -0.0737694650888443, -0.8698386549949646, -0.33098796010017395, -0.0031708222813904285, -0.131098210811615, -0.33565887808799744], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-1.0263607501983643, -0.02688584290444851], "finish_reason": "stop"}, {"text": "\"Input file is empty or contains only stop words, please check and try again\")", "tokens": ["\"", "Input", " file", " is", " empty", " or", " contains", " only", " stop", " words", ",", " please", " check", " and", " try", " again", "\"", ")", "\n"], "logprobs": [-0.05799452215433121, -1.2033532857894897, -1.1196528673171997, -0.072927325963974, -0.025309031829237938, -0.03515943884849548, -0.022987082600593567, -0.012391484342515469, -0.008246419951319695, -0.07742434740066528, -2.7651500701904297, -2.2986655235290527, -0.8109861612319946, -3.559000015258789, -0.5628165006637573, -0.009167009964585304, -1.4018044471740723, -0.0024486577603965998, -0.0008596062543801963], "finish_reason": "stop"}, {"text": "", "tokens": ["\n"], "logprobs": [-1.503070592880249], "finish_reason": "stop"}, {"text": " pd.Series(word_count.sum(axis=0), index=vectorizer.get_feature_names())", "tokens": [" pd", ".", "Series", "(", "word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", ",", " index", "=", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-2.6468167304992676, -1.9192511899746023e-05, -0.18202143907546997, -0.010603280737996101, -0.32258111238479614, -4.732496745418757e-05, -7.545663538621739e-05, -0.0005920564290136099, -0.09646575897932053, -0.00034588552080094814, -0.0021490592043846846, -0.0008281615446321666, -0.19435422122478485, -0.18209367990493774, -0.5352261662483215, -0.13933423161506653, -0.006565902382135391, -0.020582307130098343, -9.775113539944869e-06, -3.814624506048858e-05, -0.23035576939582825, -1.311301275563892e-06, -0.00022373080719262362, -1.1920928244535389e-07, -0.0026463984977453947, -0.07705699652433395, -0.18434907495975494], "finish_reason": "stop"}, {"text": " sum_words.to_dict()", "tokens": [" sum", "_", "words", ".", "to", "_", "dict", "()", "\n"], "logprobs": [-2.6363730430603027, -0.0025733946822583675, -2.777537883957848e-05, -0.25652751326560974, -1.0487953424453735, -0.7311311960220337, -0.09595903009176254, -0.04980458691716194, -0.015445144847035408], "finish_reason": "stop"}, {"text": "word, sum_words[i]) for word, i in zip(vectorizer.get_feature_names(), range(len(vectorizer.get_feature_names())))", "tokens": ["word", ",", " sum", "_", "words", "[", "i", "])", " for", " word", ",", " i", " in", " zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ",", " range", "(", "len", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ")))", "\n"], "logprobs": [-0.14942370355129242, -0.058733776211738586, -1.055227518081665, -0.018053026869893074, -0.011641544289886951, -0.097014419734478, -0.394141286611557, -0.08333848416805267, -0.6633839011192322, -0.18328602612018585, -0.0010973153403028846, -0.0034077451564371586, -0.0018629353726282716, -1.4941272735595703, -0.0010578995570540428, -0.060843829065561295, -3.6954811548639555e-06, -4.1483970562694594e-05, -0.05964668467640877, -2.3841830625315197e-06, -0.0003084660565946251, -1.1920928244535389e-07, -0.0014461545506492257, -0.5087810754776001, -0.0007419217727147043, -0.07824010401964188, -0.0003496989083942026, -0.5155935287475586, -0.0002252801787108183, -1.8934440612792969, -2.658331868587993e-05, -0.0031566813122481108, -0.03664530813694, -4.7801782784517854e-05, -0.00048720886115916073, -3.576272320060525e-06, -4.255681051290594e-05, -0.035628437995910645, -0.0006395916570909321, -0.029569538310170174], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.07281139492988586, -0.00027736154152080417, -0.10030525177717209, -0.0007858763565309346, -4.5298504119273275e-05, -0.0015942256432026625, -0.015634816139936447, -0.0010605193674564362, -0.0012479382567107677, -0.040566034615039825, -0.00026890001026913524, -0.45262834429740906, -0.008218635804951191, -0.0018994638230651617, -0.06352496892213821, -0.00012659224739763886, -0.0009201106731779873, -0.00013362467871047556, -0.00019298121333122253, -0.01078207977116108, -0.19284211099147797], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5825458765029907, -0.00012051333033014089, -6.05564855504781e-05, -0.16812489926815033, -0.017644228413701057, -0.006119680590927601, -0.0013906818348914385, -0.01090967282652855], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.006976175121963024, -0.0018532973481342196, -0.0016906267264857888, -0.07075945287942886, -3.0921831130981445, -0.13719427585601807, -0.13948169350624084, -4.184158387943171e-05, -6.079655122448457e-06, -0.04416536167263985, -0.016389625146985054, -0.020105144008994102, -0.0018619834445416927, -0.24749566614627838, -0.0025084717199206352, -0.0001333863037871197, -0.0007721779984422028, -0.6882654428482056, -0.008972208946943283, -0.003305212128907442, -0.03252292051911354], "finish_reason": "stop"}, {"text": " df.plot(", "tokens": [" df", ".", "plot", "(", "\n"], "logprobs": [-0.090529665350914, -0.02895120158791542, -0.15282337367534637, -1.3987195491790771, -2.257241725921631], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.011561652645468712, -2.396077979938127e-05, -2.145764938177308e-06, -0.9846639633178711, -0.039344578981399536, -9.298280929215252e-06, -0.012328136712312698, -0.0012954423436895013], "finish_reason": "stop"}, {"text": "savefig(save_path, bbox_inches=\"tight\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ")", "\n"], "logprobs": [-0.00241452781483531, -8.67805938469246e-05, -8.618460560683161e-05, -0.0041873874142766, -2.622600959512056e-06, -7.510157047363464e-06, -2.13927960395813, -0.20676207542419434, -0.00044145373976789415, -7.73638384998776e-05, -0.00011145447206217796, -0.002801305614411831, -0.10779529809951782, -0.0008843801915645599, -0.0027220123447477818, -3.45700973412022e-05, -0.28481096029281616, -0.015591040253639221], "finish_reason": "stop"}, {"text": "show()", "tokens": ["show", "()", "\n"], "logprobs": [-2.899561643600464, -0.06427277624607086, -0.03280755877494812], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05296427756547928, -0.007380718365311623], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.026943400502204895, -0.9945961833000183, -4.768370445162873e-07, -1.311301275563892e-06, -7.772143726469949e-05, -0.00010799778101500124, -0.08807004988193512], "finish_reason": "stop"}], "metrics": {"energy": 424.0807521286011, "energy_per_token": 1.6963230085144043, "time": 16.554882, "time_per_token": 0.066219528}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -2.2947709560394287, -0.00012635385792236775, -0.01766648329794407, -0.2837679088115692, -0.0007201223634183407, -0.4233097434043884], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -0.3859751522541046, -0.012796196155250072], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.08213822543621063, -1.0490362910786644e-05, -5.185469490243122e-05, -0.007920405827462673, -0.0035200081765651703, -0.00023398046323563904, -0.009247093461453915, -0.012697681784629822, -0.06290388107299805, -0.0004122599493712187, -9.48860906646587e-05, -2.992108420585282e-05, -0.0737694650888443, -0.5549620985984802], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-1.0263607501983643, -0.02688584290444851], "finish_reason": "stop"}, {"text": "\"Empty or all-stopwords DataFrame\")", "tokens": ["\"", "Empty", " or", " all", "-", "stop", "words", " Data", "Frame", "\"", ")", "\n"], "logprobs": [-0.05799452215433121, -2.072932720184326, -0.7457187175750732, -2.377281665802002, -1.6745173931121826, -0.011697983369231224, -1.4969136714935303, -2.136030912399292, -0.010012166574597359, -1.2050727605819702, -0.0024312958121299744, -0.0013790155062451959], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.25942009687423706, -0.00946392398327589], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0).to_dict()", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", ".", "to", "_", "dict", "()", "\n"], "logprobs": [-0.9166378974914551, -9.775113539944869e-06, -5.4596363042946905e-05, -0.0005173536483198404, -0.048206958919763565, -0.26646578311920166, -0.0014323461800813675, -0.001379848807118833, -0.253238320350647, -0.012998255901038647, -1.8165496587753296, -0.5725831389427185, -0.8629847168922424, -0.372111052274704, -0.3688836395740509, -0.044085629284381866], "finish_reason": "stop"}, {"text": " pd.Series(dict(zip(vectorizer.get_feature_names(), sum_words)))", "tokens": [" pd", ".", "Series", "(", "dict", "(", "zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ",", " sum", "_", "words", ")))", "\n"], "logprobs": [-1.2447563409805298, -3.361645576660521e-05, -0.9422887563705444, -0.17933771014213562, -1.8341805934906006, -0.061032485216856, -0.3205784261226654, -0.0077531603164970875, -0.11452113837003708, -2.3841830625315197e-06, -1.7404405298293568e-05, -0.10458897054195404, -2.7418097943154862e-06, -0.00022003613412380219, -1.1920928244535389e-07, -0.001167092937976122, -0.16177363693714142, -0.0008659188752062619, -0.39617910981178284, -0.00011848701251437888, -4.291525328881107e-06, -0.15529398620128632, -0.21914339065551758], "finish_reason": "stop"}, {"text": "word, freq) for word, freq in zip(vectorizer.get_feature_names(), sum_words)", "tokens": ["word", ",", " fre", "q", ")", " for", " word", ",", " fre", "q", " in", " zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ",", " sum", "_", "words", ")", "\n"], "logprobs": [-0.12794990837574005, -0.05608099699020386, -3.191866874694824, -0.0001289761275984347, -0.23231036961078644, -0.5673601031303406, -0.06540510803461075, -0.001128037110902369, -0.002960229991003871, -5.722029527532868e-06, -0.0019445574143901467, -0.06620462238788605, -0.0022332745138555765, -0.03352392837405205, -2.861018856492592e-06, -6.174850568640977e-05, -0.034492842853069305, -3.814689989667386e-06, -0.00033063191222026944, -3.576278118089249e-07, -0.0010639727115631104, -0.5394898653030396, -0.0007894498412497342, -0.14420931041240692, -0.00034290633630007505, -1.0847986231965479e-05, -0.17249533534049988, -0.026844993233680725], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.07731997221708298, -0.0003424296446610242, -0.057719215750694275, -0.0007765853079035878, -5.638440416078083e-05, -0.0020378315821290016, -0.013358143158257008, -0.0009952597320079803, -0.001482579275034368, -0.05189269036054611, -0.0003513672563713044, -0.395082950592041, -0.007509456016123295, -0.001935396110638976, -0.052944380789995193, -0.00013100242358632386, -0.000910106289666146, -0.00013016807497479022, -0.00020823694649152458, -0.011205833405256271, -1.657251238822937, -0.02746741659939289, -0.00690289493650198, -0.0035350944381207228, -0.006746964529156685], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5951852798461914, -0.00010823617776622996, -5.6503606174374e-05, -0.1436089128255844, -0.017747526988387108, -0.007447100710123777, -0.0013241341803222895, -0.012822206132113934], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.007441302761435509, -0.0015751824248582125, -0.0018713833997026086, -0.0683768168091774, -0.37727898359298706, -0.00015758226800244302, -1.07287787614041e-05, -0.04823865368962288, -0.006745543330907822, -0.01682824268937111, -0.0010993395699188113, -0.2146824300289154, -0.0023074925411492586, -8.785339014139026e-05, -0.000686767278239131, -0.697405219078064, -0.007991128601133823, -0.0018442542059347034, -0.03707478567957878], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", title=\"Ten Most Common Words\")", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " title", "=", "\"", "Ten", " Most", " Common", " Words", "\"", ")", "\n"], "logprobs": [-0.08734234422445297, -0.03157889097929001, -0.19505125284194946, -0.30535605549812317, -0.01246907189488411, -0.44553250074386597, -0.47426676750183105, -0.052813898772001266, -0.0007985025877133012, -0.0008509114268235862, -3.814624506048858e-05, -0.004732479341328144, -0.048354968428611755, -0.0017059786478057504, -0.00019870213873218745, -5.1020273531321436e-05, -2.3245540432981215e-05, -0.2787443995475769, -2.2463881969451904, -0.007240602746605873, -0.019466780126094818, -3.3402018547058105, -0.15423136949539185, -0.24950067698955536, -0.014454888179898262, -0.3152051866054535, -0.48886796832084656, -0.005734539590775967], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.012227809987962246, -2.455681169521995e-05, -2.861018856492592e-06, -1.1123021841049194, -0.04318511486053467, -1.0132738680113107e-05, -0.011761841364204884, -0.001177809201180935], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.001989767886698246, -0.00011336160969221964, -8.523101132595912e-05, -0.004120432306081057, -2.3841830625315197e-06, -1.1324817933200393e-05, -0.10952966660261154, -0.004349296446889639], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.11203379184007645, -0.05958423763513565, -0.21253709495067596], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04861639812588692, -0.01133785117417574], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.023235198110342026], "finish_reason": "stop"}], "metrics": {"energy": 371.5010539093021, "energy_per_token": 1.5543977151016823, "time": 14.718471, "time_per_token": 0.06158356066945606}}
{"outputs": [{"text": " pd.read_csv(file_path, keep_default_na=False)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " keep", "_", "default", "_", "na", "=", "False", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -1.1519023180007935, -3.59928822517395, -1.1444026313256472e-05, -0.1050681620836258, -9.298280929215252e-06, -0.21108295023441315, -0.0012369846226647496, -0.021638382226228714, -0.42387136816978455, -0.08554002642631531], "finish_reason": "stop"}, {"text": " df[\"Text\"].fillna(\"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "fill", "na", "(", "\"", "\"", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.75266432762146, -1.0251946150674485e-05, -0.023330699652433395, -0.09762462228536606, -0.26671287417411804, -0.000740968796890229, -0.3735211491584778], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS, ngram_range=(1, 4))", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " n", "gram", "_", "range", "=(", "1", ",", " ", "4", "))", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -1.1483709812164307, -1.9000085592269897, -0.05062856525182724, -4.23184028477408e-05, -0.00011920218821614981, -0.013240626081824303, -0.3394184708595276, -0.0008698494057171047, -0.05099306255578995, -3.218820571899414, -0.3008386790752411, -0.015569914132356644], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", "\n"], "logprobs": [-0.06617683917284012, -1.07287787614041e-05, -5.376194530981593e-05, -0.0070527647621929646, -0.004342530854046345, -0.00016366096679121256, -0.015089061111211777, -0.011216914281249046, -0.07005982846021652, -0.0003856868715956807, -8.809178689261898e-05, -2.7417760065873154e-05, -0.06754259765148163, -0.7628987431526184, -0.31631791591644287, -0.003953974228352308, -0.1712157130241394, -0.39177820086479187], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-1.1256194114685059, -0.029702626168727875], "finish_reason": "stop"}, {"text": "\"Input is empty or contains only stop words, or there is an error while processing.\")", "tokens": ["\"", "Input", " is", " empty", " or", " contains", " only", " stop", " words", ",", " or", " there", " is", " an", " error", " while", " processing", ".", "\"", ")", "\n"], "logprobs": [-0.050182193517684937, -1.1648367643356323, -1.4442365169525146, -0.07078199833631516, -0.05665777996182442, -0.0629941076040268, -0.016365116462111473, -0.008906400762498379, -0.09268371760845184, -2.754145860671997, -3.1233901977539062, -2.478114604949951, -0.6733882427215576, -0.8345588445663452, -0.25950196385383606, -1.7829562425613403, -1.0157346725463867, -2.0480451583862305, -0.06961771100759506, -0.003213600954040885, -0.0006696127820760012], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.21010003983974457, -0.00888135191053152], "finish_reason": "stop"}, {"text": " sum(word_count.sum(axis=0))", "tokens": [" sum", "(", "word", "_", "count", ".", "sum", "(", "axis", "=", "0", "))", "\n"], "logprobs": [-1.2700132131576538, -0.08456173539161682, -0.23890554904937744, -0.009353270754218102, -0.0008992680232040584, -0.01184042077511549, -0.44810497760772705, -0.0005771639989688993, -0.011158091016113758, -0.0005339150666259229, -0.35884571075439453, -0.3865342438220978, -0.031365882605314255], "finish_reason": "stop"}, {"text": " [(word, sum_words[i]) for i, word in enumerate(vectorizer.get_feature_names())]", "tokens": [" [(", "word", ",", " sum", "_", "words", "[", "i", "])", " for", " i", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ")]", "\n"], "logprobs": [-2.985199451446533, -0.18358223140239716, -0.07507690042257309, -1.4550904035568237, -0.020570745691657066, -0.011741812340915203, -0.05268317088484764, -0.4130180776119232, -0.023966602981090546, -0.05413929745554924, -2.297715425491333, -0.0005158047424629331, -0.00870339386165142, -0.0036529258359223604, -0.008840114809572697, -0.00032360086333937943, -0.08875348418951035, -1.1444026313256472e-05, -7.712543447269127e-05, -0.12919199466705322, -3.3378546504536644e-06, -0.00027366707217879593, -3.576278118089249e-07, -0.0013978243805468082, -0.9358228445053101, -0.0009818024700507522, -0.0028904585633426905], "finish_reason": "stop"}, {"text": "word, count)", "tokens": ["word", ",", " count", ")", "\n"], "logprobs": [-0.1372813731431961, -0.054554782807826996, -0.9365121126174927, -0.2739083170890808, -0.8956549167633057], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: -x[1])", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " -", "x", "[", "1", "])", "\n"], "logprobs": [-0.09775900840759277, -0.0003367095487192273, -0.11057265102863312, -0.0009127265075221658, -5.3165931603871286e-05, -0.0021089711226522923, -0.020131314173340797, -0.001158519764430821, -0.001336158369667828, -0.04642573371529579, -0.00025340684805996716, -1.1043516397476196, -0.005109110847115517, -0.012444701045751572, -0.00044955636258237064, -0.0016731324139982462, -0.12934085726737976], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.4987553358078003, -0.00011526874004630372, -5.6980417866725475e-05, -0.15023964643478394, -0.014952488243579865, -0.006815638393163681, -0.0012755600037053227, -0.012251715175807476], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-0.006780710071325302, -0.0019224273273721337, -0.0015422129072248936, -0.06254281103610992, -2.5830564498901367], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\")", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.08768245577812195, -0.03425982967019081, -0.17140424251556396, -0.27091604471206665, -0.011472326703369617, -0.40698179602622986, -0.41249287128448486, -0.05229462683200836, -0.0008398343343287706, -0.0009252319578081369, -3.838465272565372e-05, -0.006478853523731232, -0.050970859825611115, -0.0016793209360912442, -0.0002711643755901605, -4.005352093372494e-05, -2.002696055569686e-05, -1.264012336730957, -0.08376877009868622], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.011760427616536617, -2.253030106658116e-05, -2.622600959512056e-06, -0.9591014385223389, -0.038484860211610794, -9.65590606938349e-06, -0.009318194352090359, -0.0014612721279263496], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0024567442014813423, -8.666139910928905e-05, -8.797258487902582e-05, -0.0027101237792521715, -2.622600959512056e-06, -1.0847986231965479e-05, -0.09912476688623428, -0.00435689277946949], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.1290804147720337, -0.057138312608003616, -0.20400209724903107], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.050444621592760086, -0.009979946538805962], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.021328257396817207, -0.9964197874069214, -3.576278118089249e-07, -1.1920922133867862e-06, -9.667406266089529e-05, -9.30981186684221e-05, -0.09252658486366272], "finish_reason": "stop"}], "metrics": {"energy": 336.27920731258416, "energy_per_token": 1.5012464612168936, "time": 14.28906, "time_per_token": 0.06379044642857143}}
{"outputs": [{"text": " pd.read_csv(file_path, error_on_empty=False, index_col=False, header=0, encoding=\"utf8\",", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " error", "_", "on", "_", "empty", "=", "False", ",", " index", "_", "col", "=", "False", ",", " header", "=", "0", ",", " encoding", "=", "\"", "utf", "8", "\"", ",", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -1.1519023180007935, -3.9185163974761963, -0.0013910388806834817, -2.213120937347412, -0.0019387274514883757, -1.7679346799850464, -1.4473295211791992, -0.09024040400981903, -1.1983907222747803, -2.1812031269073486, -8.344646857949556e-07, -0.0008244690834544599, -0.014168385416269302, -1.1691886186599731, -0.8971417546272278, -0.9284818172454834, -0.036552563309669495, -0.5758668184280396, -0.6576606035232544, -3.2152938842773438, -0.9163624048233032, -0.012565843760967255, -0.2589271068572998, -1.3689939975738525, -0.001405204995535314, -1.4105868339538574, -0.9097474813461304], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.strip().str.lower().str.replace(\"\\n\", \" \")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "strip", "()", ".", "str", ".", "lower", "()", ".", "str", ".", "replace", "(", "\"", "\\", "n", "\"", ",", " \"", " \"", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -1.4731777906417847, -0.015683403238654137, -1.6494832038879395, -0.10918707400560379, -0.00015233787416946143, -0.28285160660743713, -0.0011043408885598183, -1.6965022087097168, -0.12965913116931915, -0.0003668589051812887, -0.4348658621311188, -0.14842337369918823, -0.28796494007110596, -1.5481950044631958, -0.2510453164577484, -0.03691016137599945, -0.0019658540841192007, -0.02925642393529415, -0.09306351095438004, -0.013298857025802135, -0.46147897839546204], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -0.3178824782371521, -0.017030777409672737], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray().sum(axis=0).tolist()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", ".", "sum", "(", "axis", "=", "0", ")", ".", "tol", "ist", "()", "\n"], "logprobs": [-0.06323696672916412, -9.059865078597795e-06, -5.364274329622276e-05, -0.005345338024199009, -0.003966085612773895, -0.0002489972102921456, -0.013667807914316654, -0.012809259817004204, -0.05914623290300369, -0.00033682872890494764, -9.190614218823612e-05, -3.814624506048858e-05, -0.062393974512815475, -0.8757088780403137, -0.34937435388565063, -0.0032137196976691484, -0.13576114177703857, -1.2898914813995361, -0.6198922395706177, -0.043128255754709244, -0.030463429167866707, -0.0022078203037381172, -0.035079218447208405, -0.01306826714426279, -2.920074701309204, -1.1999506950378418, -1.1920922133867862e-06, -0.5329874753952026, -0.04287154972553253], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-0.9912388324737549, -0.02996828407049179], "finish_reason": "stop"}, {"text": "\"No valid words found in the CSV file.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " CS", "V", " file", ".", "\"", ")", "\n"], "logprobs": [-0.04835723713040352, -1.4620225429534912, -0.41416141390800476, -0.3039606213569641, -0.06703125685453415, -0.19624696671962738, -0.12876133620738983, -2.368861675262451, -6.925819616299123e-05, -0.16985680162906647, -0.17342408001422882, -0.08262752741575241, -0.0013061572099104524, -0.0007272697403095663], "finish_reason": "stop"}, {"text": "", "tokens": ["\n"], "logprobs": [-1.4698673486709595], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.9671698212623596, -1.537788011773955e-05, -4.184158387943171e-05, -0.00039962888695299625, -0.052819326519966125, -0.31074389815330505, -0.0011555430246517062, -0.001559709431603551, -0.2733663022518158, -0.013030026108026505, -0.2846435606479645], "finish_reason": "stop"}, {"text": " pd.DataFrame(word_count.toarray(), columns=vectorizer.get_feature_names())", "tokens": [" pd", ".", "DataFrame", "(", "word", "_", "count", ".", "to", "array", "()", ",", " columns", "=", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-1.2262579202651978, -3.516612196108326e-05, -0.3975595235824585, -0.12463865429162979, -0.4697800874710083, -6.401333666872233e-05, -6.198863957251888e-06, -0.011075212620198727, -0.5418069362640381, -0.0029597545508295298, -0.016298385336995125, -0.052203211933374405, -0.6271365880966187, -0.013822780922055244, -0.01549456175416708, -7.867782187531702e-06, -5.435795901576057e-05, -0.022419331595301628, -1.0728830375228426e-06, -0.00011431517486926168, -1.1920928244535389e-07, -0.0012261499650776386, -0.44628655910491943, -0.3878834545612335], "finish_reason": "stop"}, {"text": "word, sum_words[i])", "tokens": ["word", ",", " sum", "_", "words", "[", "i", "])", "\n"], "logprobs": [-0.14743705093860626, -0.06132493540644646, -0.8786777257919312, -0.013692384585738182, -0.01220767293125391, -0.11073572188615799, -0.49554452300071716, -0.08755675703287125, -0.7541755437850952], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1])[::-1]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "])", "[", "::", "-", "1", "]", "\n"], "logprobs": [-0.07410481572151184, -0.00024959311122074723, -0.09474726766347885, -0.001209481037221849, -5.9602869441732764e-05, -0.001901724492199719, -0.02236279658973217, -0.0010031197452917695, -0.001348182326182723, -0.04515102878212929, -0.00029845553217455745, -0.3493441045284271, -0.009496869519352913, -0.0018349728779867291, -2.7894866466522217, -1.2855944633483887, -0.12428214401006699, -0.003423545742407441, -5.328513361746445e-05, -0.44571709632873535, -0.015043851919472218], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.6023601293563843, -9.405170567333698e-05, -7.092700980138034e-05, -0.15945245325565338, -0.016638679429888725, -0.007127335760742426, -0.0015664937673136592, -0.010454177856445312], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.007609665859490633, -0.0020171310752630234, -0.0017273995326831937, -0.06166829541325569, -0.36130842566490173, -0.00011860620725201443, -1.0251946150674485e-05, -0.05634695291519165, -0.00740710599347949, -0.01679682731628418, -0.0010746901389211416, -0.23913289606571198, -0.002538912231102586, -0.00011014331539627165, -0.0006764031131751835, -0.7902476787567139, -0.09461941570043564, -0.00403008284047246, -0.002397878561168909, -0.038423262536525726], "finish_reason": "stop"}, {"text": " df.plot(kind=\"barh\", figsize=(8, 10))", "tokens": [" df", ".", "plot", "(", "kind", "=", "\"", "bar", "h", "\"", ",", " fig", "size", "=(", "8", ",", " ", "1", "0", "))", "\n"], "logprobs": [-0.0715077593922615, -0.025679387152194977, -0.16904392838478088, -1.4668266773223877, -0.53850257396698, -0.087775819003582, -0.00010346830822527409, -0.013340028002858162, -0.22419871389865875, -5.4596363042946905e-05, -0.02549777179956436, -1.3275867700576782, -0.000169382052263245, -0.01585765928030014, -1.6411826610565186, -0.004401044920086861, -0.0154426796361804, -2.901980400085449, -0.4988378584384918, -1.0629818439483643, -0.009466639719903469], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.01015803124755621, -2.5510462364763953e-05, -2.7418097943154862e-06, -0.9979550242424011, -0.04491889104247093, -1.1920858014491387e-05, -0.01217387430369854, -0.0012142435880377889], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0024023978039622307, -0.00012838016846217215, -8.928377064876258e-05, -0.002966410480439663, -2.7418097943154862e-06, -1.0013530300057027e-05, -0.10020960122346878, -0.0044607422314584255], "finish_reason": "stop"}, {"text": "close()  # Close the current plot to free up resources", "tokens": ["close", "()", "  ", "#", " Close", " the", " current", " plot", " to", " free", " up", " resources", "\n"], "logprobs": [-0.14299027621746063, -0.056607767939567566, -1.8384050130844116, -0.0006697318749502301, -0.8342310190200806, -0.02444995380938053, -1.8426135778427124, -0.6841852068901062, -0.6807218194007874, -0.3038859963417053, -0.33176884055137634, -0.5008300542831421, -0.07862553745508194], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.048278067260980606, -0.009753193706274033], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.022219400852918625], "finish_reason": "stop"}], "metrics": {"energy": 420.177729547501, "energy_per_token": 1.5334953633120474, "time": 16.37773, "time_per_token": 0.05977273722627737}}
{"outputs": [{"text": " pd.read_csv(file_path, names=['Text'], dtype=str)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " names", "=['", "Text", "']", ",", " dtype", "=", "str", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -1.1519023180007935, -3.7281391620635986, -1.391223430633545, -0.24040503799915314, -2.3911845684051514, -0.030600329861044884, -1.8327085971832275, -0.8255049586296082, -0.09597484022378922, -0.13851432502269745, -0.13398277759552002], "finish_reason": "stop"}, {"text": " df[\"Text\"].apply(lambda x: x.strip())", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "apply", "(", "lambda", " x", ":", " x", ".", "strip", "())", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.800536870956421, -0.011861036531627178, -0.26765576004981995, -0.13337089121341705, -0.000940595637075603, -0.3939773738384247, -0.022056277841329575, -0.9216697216033936, -0.07201159745454788, -0.16643723845481873], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -2.2581491470336914], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.07234232872724533, -1.1205610462639015e-05, -5.829164365422912e-05, -0.008356131613254547, -0.0031547797843813896, -0.0002703301142901182, -0.01030812133103609, -0.012856689281761646, -0.057394981384277344, -0.0003927174839191139, -8.368142152903602e-05, -3.957670196541585e-05, -0.07278767228126526, -0.6697254776954651], "finish_reason": "stop"}, {"text": " AttributeError:", "tokens": [" Attribute", "Error", ":", "\n"], "logprobs": [-2.0811173915863037, -8.511180931236595e-05, -0.18352657556533813, -0.050150223076343536], "finish_reason": "stop"}, {"text": "\"No valid words found in the input data. Please check the input file.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " input", " data", ".", " Please", " check", " the", " input", " file", ".", "\"", ")", "\n"], "logprobs": [-0.05673933029174805, -1.471602439880371, -0.38650619983673096, -0.2799481749534607, -0.07004649192094803, -0.21211041510105133, -0.12315598875284195, -0.6568654775619507, -1.1962693929672241, -0.1821276694536209, -3.042865037918091, -0.43965068459510803, -0.3989390730857849, -1.4392623901367188, -0.5218489170074463, -0.5934581160545349, -0.005003668367862701, -0.001073975581675768, -0.0005683475756086409], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.27275654673576355, -0.00986144132912159], "finish_reason": "stop"}, {"text": " sum(count for count in word_count.toarray())", "tokens": [" sum", "(", "count", " for", " count", " in", " word", "_", "count", ".", "to", "array", "())", "\n"], "logprobs": [-1.2530484199523926, -0.07270941883325577, -2.914384365081787, -0.08336196094751358, -0.7001041769981384, -0.003415467217564583, -0.04617055878043175, -8.34461570775602e-06, -1.490105023549404e-05, -0.013780218549072742, -1.324582815170288, -0.003102135146036744, -2.1745967864990234, -0.0298287533223629], "finish_reason": "stop"}, {"text": " (word_count.T / sum_words).T", "tokens": [" (", "word", "_", "count", ".", "T", " /", " sum", "_", "words", ")", ".", "T", "\n"], "logprobs": [-1.7293754816055298, -0.44709259271621704, -0.0009748950251378119, -3.7431014789035544e-05, -0.07037097960710526, -0.5663314461708069, -0.039208296686410904, -0.019308820366859436, -0.0011968599865213037, -3.480850500636734e-05, -0.032852549105882645, -0.08401934802532196, -0.45346540212631226, -0.30486243963241577], "finish_reason": "stop"}, {"text": "word, count)", "tokens": ["word", ",", " count", ")", "\n"], "logprobs": [-0.14955449104309082, -0.05408226698637009, -0.7976403832435608, -0.32735496759414673, -0.791966438293457], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.09029541164636612, -0.00026723151677288115, -0.09712670743465424, -0.0009033175301738083, -4.8874615458771586e-05, -0.0018322361866012216, -0.013979493640363216, -0.0012429377529770136, -0.0016357626300305128, -0.04779587313532829, -0.00029940891545265913, -0.3715309500694275, -0.009734304621815681, -0.001817957148887217, -0.07392425090074539, -0.000161038784426637, -0.000840906286612153, -0.00011514954530866817, -0.00019536493346095085, -0.011942553333938122, -0.2763179838657379], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5726166367530823, -0.00011550712952157483, -5.4596363042946905e-05, -0.14412766695022583, -0.017659805715084076, -0.007371843326836824, -0.0013681822456419468, -0.012795725837349892], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.007980367168784142, -0.0018638873007148504, -0.001408895361237228, -0.06638278812170029, -0.37582269310951233, -0.00013743886665906757, -1.0013530300057027e-05, -0.050858814269304276, -0.007313384674489498, -0.018861697986721992, -0.0012192443246021867, -0.21546068787574768, -0.0022613448090851307, -7.128461584215984e-05, -0.0007111880695447326, -0.773192822933197, -0.10943243652582169, -0.003640929702669382, -0.0029150634072721004, -0.040265731513500214], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=90, figsize=(15, 5), color=\"red\")", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "9", "0", ",", " fig", "size", "=(", "1", "5", ",", " ", "5", ")", ",", " color", "=", "\"", "red", "\"", ")", "\n"], "logprobs": [-0.09112316370010376, -0.04024993255734444, -0.174547016620636, -0.29883700609207153, -0.008046000264585018, -0.4135793447494507, -0.33655625581741333, -0.05509375035762787, -0.0010024051880463958, -0.0010211020708084106, -4.1483970562694594e-05, -0.005603795405477285, -0.05499435216188431, -0.0017970141489058733, -0.00026854246971197426, -4.7801782784517854e-05, -2.2291887944447808e-05, -0.28277388215065, -0.5309926867485046, -0.0031876962166279554, -1.7313942909240723, -0.00032944019767455757, -0.918463945388794, -0.6609551906585693, -0.0009782296838238835, -0.02454906329512596, -0.469184935092926, -1.5010710954666138, -0.0021944984328001738, -0.017127225175499916, -0.6547437310218811, -1.7941604852676392, -0.00045503751607611775, -2.1792469024658203, -0.12589149177074432, -0.005014106631278992, -2.045360565185547, -0.0008599635912105441, -0.15786521136760712, -0.007996568456292152], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.012526405975222588, -2.3364747903542593e-05, -2.622600959512056e-06, -0.503399133682251, -0.0024883756414055824], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.002079231198877096, -9.48860906646587e-05, -8.83301836438477e-05, -0.0045806001871824265, -2.50339189733495e-06, -1.156323378381785e-05, -0.11383828520774841, -0.004199258517473936], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.1441056877374649, -0.06355584412813187, -0.2078838348388672], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04860334098339081, -0.009329295717179775], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.02047264389693737], "finish_reason": "stop"}], "metrics": {"energy": 376.59980534839644, "energy_per_token": 1.6963955195873714, "time": 15.1294775, "time_per_token": 0.06815079954954956}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -0.4749401807785034, -0.000929400441236794, -0.37533655762672424], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS, vocabulary=None)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " vocabulary", "=", "None", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -1.3099956512451172, -2.075294017791748, -0.34678807854652405, -0.4601735472679138, -0.5496228933334351, -0.03557690232992172], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", "\n"], "logprobs": [-0.07183632254600525, -1.1444026313256472e-05, -5.757642793469131e-05, -0.005563489627093077, -0.004018684849143028, -0.00019905969384126365, -0.013923770748078823, -0.01022163312882185, -0.0626089870929718, -0.00041214076918549836, -8.511180931236595e-05, -3.58813522325363e-05, -0.06466630846261978, -0.7582079768180847, -0.2800877094268799, -0.0027482856530696154, -0.14985018968582153, -0.40158018469810486], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-1.0150574445724487, -0.02960900403559208], "finish_reason": "stop"}, {"text": "\"Input data is empty or contains only stop words\")", "tokens": ["\"", "Input", " data", " is", " empty", " or", " contains", " only", " stop", " words", "\"", ")", "\n"], "logprobs": [-0.051697321236133575, -1.313776969909668, -1.3348673582077026, -0.0942879468202591, -0.09794018417596817, -0.06757636368274689, -0.06621722877025604, -0.019386107102036476, -0.009194650687277317, -0.11210604757070541, -1.0854778289794922, -0.0014761515194550157, -0.0006677066558040679], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.23232682049274445, -0.009036595933139324], "finish_reason": "stop"}, {"text": " sum(word_count.sum(axis=0).tolist())", "tokens": [" sum", "(", "word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", ".", "tol", "ist", "())", "\n"], "logprobs": [-1.2452256679534912, -0.07120994478464127, -0.21697133779525757, -0.0072598932310938835, -0.0009299959056079388, -0.016074584797024727, -0.4842514097690582, -0.0006389960180968046, -0.010699170641601086, -0.0007861146004870534, -0.3216826915740967, -1.0617939233779907, -0.0049106706865131855, -1.0175349712371826, -2.0265558760002023e-06, -0.010444740764796734, -0.030864940956234932], "finish_reason": "stop"}, {"text": " sorted(sum_words, key=sum_words.get, reverse=True)[:10]", "tokens": [" sorted", "(", "sum", "_", "words", ",", " key", "=", "sum", "_", "words", ".", "get", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-3.930230140686035, -0.13635429739952087, -0.5555837154388428, -0.0004532501916401088, -1.537788011773955e-05, -1.3471773862838745, -0.10989478975534439, -0.0038636098615825176, -0.08238032460212708, -0.00017391123401466757, -1.156323378381785e-05, -0.0015412606298923492, -0.004969981499016285, -0.02156103402376175, -0.0019224273273721337, -0.00020704510097857565, -0.00020632999076042324, -0.021496282890439034, -1.132832407951355, -0.025580158457159996, -0.009339689277112484, -0.0032703985925763845, -0.01068324875086546], "finish_reason": "stop"}, {"text": "word, sum_words[i])", "tokens": ["word", ",", " sum", "_", "words", "[", "i", "])", "\n"], "logprobs": [-0.145032599568367, -0.05522472411394119, -0.961690366268158, -0.01611517369747162, -0.01007896475493908, -0.09349657595157623, -0.49445903301239014, -0.07726766914129257, -0.7685548067092896], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.08898799121379852, -0.0003237200144212693, -0.09476938843727112, -0.0006417360855266452, -5.686121585313231e-05, -0.0018436592072248459, -0.01914580725133419, -0.001110532903112471, -0.0017410848522558808, -0.052005525678396225, -0.0003332536434754729, -0.3554098606109619, -0.011260057799518108, -0.001904699020087719, -0.05955661088228226, -0.00015031162183731794, -0.0009943069890141487, -0.00014149141497910023, -0.0002244459028588608, -0.0107793677598238, -1.6278893947601318, -0.026768513023853302, -0.00727646192535758, -0.004451841115951538, -0.007751858793199062], "finish_reason": "stop"}, {"text": " [word[0] for word in words_freq[:10]]", "tokens": [" [", "word", "[", "0", "]", " for", " word", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-1.2046771049499512, -0.5287226438522339, -2.125464677810669, -0.002969976281747222, -0.0017202593153342605, -0.03538338094949722, -0.0013684204313904047, -0.00014911970356479287, -0.002699899487197399, -5.304672595229931e-05, -2.3603161025675945e-05, -0.605498194694519, -0.02619558945298195, -0.009702075272798538, -0.011409393511712551, -0.006493539549410343], "finish_reason": "stop"}, {"text": " pd.DataFrame(words_freq, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "words", "_", "freq", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.008506610989570618, -0.0021224133670330048, -0.001947650802321732, -0.07420576363801956, -2.774916648864746, -0.008442900143563747, -0.0004042761866003275, -0.06381847709417343, -0.010771112516522408, -0.019517401233315468, -0.0017207353375852108, -1.7765692472457886, -0.004241399932652712, -0.00012683063687290996, -0.0009280903614126146, -0.4747668504714966, -0.002929921029135585, -0.0020204621832817793, -0.0358990840613842], "finish_reason": "stop"}, {"text": " df.plot.barh(title=\"Top 10 Most Common Words\", x=\"Word\", y=\"Count\", figsize=(10, 6))", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "title", "=", "\"", "Top", " ", "1", "0", " Most", " Common", " Words", "\"", ",", " x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " fig", "size", "=(", "1", "0", ",", " ", "6", "))", "\n"], "logprobs": [-0.09271185845136642, -0.03782311826944351, -0.15980872511863708, -0.30659568309783936, -0.011024280451238155, -1.1181986331939697, -0.01375258807092905, -3.171433210372925, -0.015238061547279358, -0.020984303206205368, -0.7400550842285156, -0.08244125545024872, -0.0018042728770524263, -0.0001554368354845792, -0.5294186472892761, -0.334747314453125, -0.009732061997056007, -0.28532513976097107, -0.274641752243042, -1.2208739519119263, -0.02869604527950287, -0.0011943596182391047, -0.008520440198481083, -6.508615479106084e-05, -0.009538787417113781, -0.03023410402238369, -0.0008598444401286542, -0.00013100242358632386, -3.8742269680369645e-05, -3.0278701160568744e-05, -0.9622150659561157, -0.4924600124359131, -0.0004895919119007885, -0.019804945215582848, -0.39882421493530273, -0.6299285888671875, -0.0023231918457895517, -0.013473881408572197, -1.5341933965682983, -0.1676919311285019, -0.002978058299049735], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.012147023342549801, -2.407998726994265e-05, -2.622600959512056e-06, -0.9785351157188416, -0.038105446845293045, -9.894321920000948e-06, -0.011009542271494865, -0.0012681784573942423], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=300)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "3", "0", "0", ")", "\n"], "logprobs": [-0.002045445377007127, -0.00012218205665703863, -7.70062324590981e-05, -0.004059527534991503, -2.622600959512056e-06, -9.417489309271332e-06, -2.151658773422241, -2.103591203689575, -0.00012754580529872328, -0.00031895318534225225, -0.29910194873809814, -0.002356135519221425, -0.0003583981015253812, -0.30795222520828247, -0.014761277474462986], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.13544824719429016, -0.06272834539413452, -0.18465685844421387], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04343545809388161, -0.010376786813139915], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.02313968539237976, -1.0807174444198608, -5.960462772236497e-07, -1.311301275563892e-06, -7.426462980220094e-05, -0.0001134808044298552, -0.08278238028287888], "finish_reason": "stop"}], "metrics": {"energy": 416.5302532005309, "energy_per_token": 1.5837652212947944, "time": 15.97671, "time_per_token": 0.06074794676806084}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -0.4749401807785034, -0.000929400441236794, -0.37533655762672424], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -0.3178824782371521, -0.017030777409672737], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"].values)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "]", ".", "values", ")", "\n"], "logprobs": [-0.06323696672916412, -9.059865078597795e-06, -5.364274329622276e-05, -0.005345338024199009, -0.003966085612773895, -0.0002489972102921456, -0.013667807914316654, -0.012809259817004204, -0.05914623290300369, -0.00033682872890494764, -9.190614218823612e-05, -3.814624506048858e-05, -2.844416379928589, -0.009721554815769196, -0.686048150062561, -0.1793893277645111, -0.5704096555709839], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-0.9912388324737549, -0.02996828407049179], "finish_reason": "stop"}, {"text": "\"Error: The data is empty or contains only stop words\")", "tokens": ["\"", "Error", ":", " The", " data", " is", " empty", " or", " contains", " only", " stop", " words", "\"", ")", "\n"], "logprobs": [-0.04835723713040352, -2.672513723373413, -0.3127993941307068, -2.3426513671875, -2.3550221920013428, -0.661729097366333, -0.22043746709823608, -0.060907430946826935, -0.11436197906732559, -0.022776544094085693, -0.010340920649468899, -0.10992586612701416, -1.35833740234375, -0.0018704315880313516, -0.0009307105210609734], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.23927591741085052, -0.00957881473004818], "finish_reason": "stop"}, {"text": " word_count.sum(axis=1)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "1", ")", "\n"], "logprobs": [-0.9408992528915405, -1.4066597032069694e-05, -5.98412734689191e-05, -0.0004409771354403347, -0.042875662446022034, -0.24389368295669556, -0.001192573574371636, -0.0014180614380165935, -1.373508334159851, -0.020296737551689148, -0.2877233326435089], "finish_reason": "stop"}, {"text": " pd.DataFrame(sum_words, columns=[\"Total\"], index=vectorizer.get_feature_names())", "tokens": [" pd", ".", "DataFrame", "(", "sum", "_", "words", ",", " columns", "=[", "\"", "Total", "\"", "]", ",", " index", "=", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-1.3231256008148193, -3.9934315282152966e-05, -0.4709314703941345, -0.12920477986335754, -2.282651424407959, -0.00017248096992261708, -1.764281842042692e-05, -0.3713349401950836, -1.1646349430084229, -0.023402435705065727, -0.0011992413783445954, -3.5855913162231445, -1.474410057067871, -1.277437448501587, -0.00021252757869660854, -0.008632130920886993, -0.38507184386253357, -0.0518074594438076, -8.583032467868179e-06, -5.23315102327615e-05, -0.14054259657859802, -1.0728830375228426e-06, -0.0002445875434204936, -1.1920928244535389e-07, -0.0014885308919474483, -0.11544645577669144, -0.12101013958454132], "finish_reason": "stop"}, {"text": "word, word_count[i].sum()) for word, i in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " word", "_", "count", "[", "i", "]", ".", "sum", "())", " for", " word", ",", " i", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.14423468708992004, -0.056975945830345154, -2.9621634483337402, -0.0032430693972855806, -0.0029454915784299374, -0.25847679376602173, -1.1936029195785522, -0.3363099992275238, -0.10855351388454437, -1.433220624923706, -0.23809310793876648, -0.6041982173919678, -0.17237643897533417, -0.0012754409108310938, -0.0066054570488631725, -0.004362471401691437, -0.13781048357486725, -1.3351351299206726e-05, -8.475421054754406e-05, -0.008922705426812172, -0.005312610883265734, -0.00013219437096267939, -0.0001431601122021675, -0.0002512616047170013, -3.302042750874534e-05, -0.0284089595079422], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.08612227439880371, -0.000262106885202229, -0.09380367398262024, -0.0007102350937202573, -6.210611172718927e-05, -0.002228397876024246, -0.016791317611932755, -0.001127441762946546, -0.00160624657291919, -0.047722797840833664, -0.00033444532891735435, -0.40487587451934814, -0.008726556785404682, -0.0019471748964861035, -0.05773620679974556, -0.00012885693286079913, -0.0009810879128053784, -0.0001234931987710297, -0.000179036331246607, -0.014412940479815006, -0.2141217440366745], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5657168030738831, -0.000105375460407231, -7.497983460780233e-05, -0.12281002104282379, -0.01951494626700878, -0.006692021619528532, -0.0015413797227665782, -0.011750413104891777], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.008024950511753559, -0.00151078961789608, -0.0016859853640198708, -0.05926218256354332, -0.35683292150497437, -0.00015472168161068112, -9.298280929215252e-06, -0.05829989165067673, -0.005449558608233929, -0.018921591341495514, -0.0010212211636826396, -0.23080871999263763, -0.002237794455140829, -9.393251093570143e-05, -0.0007683662115596235, -0.6026005148887634, -0.008689685724675655, -0.0020172500517219305, -0.02933051437139511], "finish_reason": "stop"}, {"text": " df.plot.bar(x='Word', y='Count', rot=0, figsize=(8, 4))", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "='", "Word", "'", ",", " y", "='", "Count", "'", ",", " rot", "=", "0", ",", " fig", "size", "=(", "8", ",", " ", "4", "))", "\n"], "logprobs": [-0.09289731085300446, -0.040124423801898956, -0.15440943837165833, -0.29693683981895447, -0.01056777685880661, -0.37261849641799927, -0.366919606924057, -2.7976086139678955, -0.0009303532424382865, -0.01771789789199829, -1.847726889536716e-05, -0.09165855497121811, -0.006205813027918339, -5.1616290875244886e-05, -0.37582874298095703, -0.00017867876158561558, -0.4804859459400177, -0.006071220152080059, -0.4921257793903351, -0.7973727583885193, -0.6389856934547424, -0.0018775707576423883, -0.019323201850056648, -2.090270519256592, -0.009253471158444881, -0.03892524540424347, -0.9378886818885803, -0.36221662163734436, -0.007049568463116884], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.012802787125110626, -2.3007127310847864e-05, -2.7418097943154862e-06, -0.398402601480484, -0.0027100048027932644], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.002281683497130871, -0.00011252723925281316, -8.11782301752828e-05, -0.003686776151880622, -2.264974000354414e-06, -9.775113539944869e-06, -0.11749053001403809, -0.0046440837904810905], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.11974240094423294, -0.06061631813645363, -0.20514307916164398], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05312221124768257, -0.01114983856678009], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.025881998240947723, -1.0139826536178589, -5.960462772236497e-07, -9.536738616588991e-07, -7.176141662057489e-05, -0.00010942813969450071, -0.08361132442951202], "finish_reason": "stop"}], "metrics": {"energy": 427.301934370041, "energy_per_token": 1.773037072074859, "time": 16.178646, "time_per_token": 0.0671313112033195}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -0.4749401807785034, -0.000929400441236794, -0.37533655762672424], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS, lowercase=True)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " lower", "case", "=", "True", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -1.3099956512451172, -2.175380229949951, -0.0004408579843584448, -0.001278536394238472, -0.36054837703704834, -0.43667131662368774, -0.016384700313210487], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).sum(axis=0).tolist()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "sum", "(", "axis", "=", "0", ")", ".", "tol", "ist", "()", "\n"], "logprobs": [-0.07183632254600525, -1.1444026313256472e-05, -5.757642793469131e-05, -0.005563489627093077, -0.004018684849143028, -0.00019905969384126365, -0.013923770748078823, -0.01022163312882185, -0.0626089870929718, -0.00041214076918549836, -8.511180931236595e-05, -3.58813522325363e-05, -0.06466630846261978, -0.7582079768180847, -1.9224960803985596, -0.004760005045682192, -0.004447093699127436, -0.001292585046030581, -0.05067400634288788, -0.0051820483058691025, -1.6251354217529297, -1.1399861574172974, -1.7881377516459906e-06, -0.507935643196106, -0.03950352966785431], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-1.0150574445724487, -0.02960900403559208], "finish_reason": "stop"}, {"text": "\"No valid words found in the data.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " data", ".", "\"", ")", "\n"], "logprobs": [-0.051697321236133575, -1.391057014465332, -0.4150899350643158, -0.2977057993412018, -0.07045909017324448, -0.18888266384601593, -0.12441857159137726, -1.7388064861297607, -0.32075273990631104, -0.08822035044431686, -0.001617315225303173, -0.0008895016508176923], "finish_reason": "stop"}, {"text": "", "tokens": ["\n"], "logprobs": [-1.3637776374816895], "finish_reason": "stop"}, {"text": " np.sum(word_count.toarray(), axis=0)", "tokens": [" np", ".", "sum", "(", "word", "_", "count", ".", "to", "array", "()", ",", " axis", "=", "0", ")", "\n"], "logprobs": [-1.8924293518066406, -9.393251093570143e-05, -0.5004448294639587, -0.0039702411741018295, -0.0620182603597641, -2.706014311115723e-05, -2.253030106658116e-05, -0.04371997341513634, -0.04281056299805641, -0.0010569468140602112, -0.2232668548822403, -0.020350821316242218, -0.013919304125010967, -0.0014848408754915, -0.026375338435173035, -0.011458538472652435, -0.06519080698490143], "finish_reason": "stop"}, {"text": " pd.DataFrame(word_count.toarray(), columns=vectorizer.get_feature_names())", "tokens": [" pd", ".", "DataFrame", "(", "word", "_", "count", ".", "to", "array", "()", ",", " columns", "=", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-1.2334195375442505, -3.2543604902457446e-05, -0.5310770869255066, -0.11415323615074158, -0.41722193360328674, -7.533743337262422e-05, -4.768360213347478e-06, -0.008863627910614014, -0.6337333917617798, -0.002248380333185196, -0.012582206167280674, -0.0684305876493454, -0.4589987099170685, -0.014194242656230927, -0.013714726082980633, -8.821448318485636e-06, -4.732496745418757e-05, -0.014902809634804726, -9.536738616588991e-07, -0.000120751719805412, -1.1920928244535389e-07, -0.0012673450401052833, -0.3385965824127197, -0.45058244466781616], "finish_reason": "stop"}, {"text": "word, count / sum_words.sum()) for word, count in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " count", " /", " sum", "_", "words", ".", "sum", "())", " for", " word", ",", " count", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.13591016829013824, -0.0485355444252491, -0.8892210125923157, -1.8586876392364502, -0.05214573070406914, -0.014627713710069656, -0.00021801002731081098, -1.3976635932922363, -1.5420315265655518, -0.20170947909355164, -0.5724045038223267, -0.03474537283182144, -0.00032860602368600667, -0.0002867764269467443, -0.0016121974913403392, -1.6861357688903809, -0.000723576988093555, -0.00012015574611723423, -0.033646002411842346, -0.002789299236610532, -0.00028761065914295614, -0.00011967896716669202, -0.001176856574602425, -3.2066785934148356e-05, -0.019728509709239006], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.0842370018362999, -0.00023469554434996098, -0.08233586698770523, -0.0010895751183852553, -6.0437283536884934e-05, -0.0016265984158962965, -0.017711572349071503, -0.0010006187949329615, -0.0013983005192130804, -0.042752768844366074, -0.0003090619284193963, -0.44411593675613403, -0.007819399237632751, -0.0016282646683976054, -0.05482226982712746, -0.00010752100206445903, -0.0009031984372995794, -0.00012838016846217215, -0.00022766382608097047, -0.01251439843326807, -1.59148371219635, -0.025507880374789238, -0.006347139831632376, -0.0029936281498521566, -0.006719375029206276], "finish_reason": "stop"}, {"text": " [w[0] for w in words_freq[:10]]", "tokens": [" [", "w", "[", "0", "]", " for", " w", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-1.2239208221435547, -2.4033236503601074, -0.3452550172805786, -0.002868230454623699, -0.0021203910000622272, -0.02738448791205883, -0.0008312584250234067, -6.687417771900073e-05, -0.0034079826436936855, -1.3589766240329482e-05, -1.4781842764932662e-05, -0.5873099565505981, -0.020831603556871414, -0.009979828260838985, -0.013358496129512787, -0.008397861383855343], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.007431126665323973, -0.0019061268540099263, -0.001836043898947537, -0.06500528007745743, -0.38099154829978943, -0.00010406429646536708, -8.821448318485636e-06, -0.046207550913095474, -0.005730391480028629, -0.017361098900437355, -0.0010082405060529709, -0.23699824512004852, -0.002107900334522128, -0.00010990492592100054, -0.0008220868767239153, -0.6484662294387817, -0.009408421814441681, -0.002059602178633213, -0.0311114564538002], "finish_reason": "stop"}, {"text": " df.hist(x=\"Word\", bins=30, color=\"blue\", alpha=0.5)", "tokens": [" df", ".", "h", "ist", "(", "x", "=", "\"", "Word", "\"", ",", " bins", "=", "3", "0", ",", " color", "=", "\"", "blue", "\"", ",", " alpha", "=", "0", ".", "5", ")", "\n"], "logprobs": [-0.07161684334278107, -0.03435416892170906, -2.7311394214630127, -3.599494695663452, -0.4185650646686554, -1.7796599864959717, -0.055646300315856934, -0.0018851857166737318, -1.1065890789031982, -0.00021181246847845614, -0.06948961317539215, -0.8834006786346436, -0.019006745889782906, -2.544044256210327, -0.06250842660665512, -0.24922393262386322, -1.6315302848815918, -0.05742638558149338, -0.00672517716884613, -2.143734931945801, -0.0005303407087922096, -0.558728039264679, -0.6191588044166565, -0.0002734287118073553, -0.09463633596897125, -0.0009490515221841633, -0.39239081740379333, -0.5547601580619812, -0.03843507543206215], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.01004474051296711, -2.098061486321967e-05, -2.3841830625315197e-06, -0.43726834654808044, -0.002312725642696023], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0021437061950564384, -0.00010001159535022452, -8.248942322097719e-05, -0.003782144282013178, -2.9802276912960224e-06, -1.0251946150674485e-05, -0.10726472735404968, -0.003843184793367982], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.13473068177700043, -0.06194419786334038, -0.20269939303398132], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.056493986397981644, -0.010137497447431087], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.027573997154831886, -1.340907335281372], "finish_reason": "stop"}], "metrics": {"energy": 385.8170085611343, "energy_per_token": 1.4896409596954991, "time": 16.167187, "time_per_token": 0.062421571428571423}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].apply(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "apply", "(", "str", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.800536870956421, -0.011861036531627178, -1.9939435720443726, -0.16352474689483643, -0.2880787253379822], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -0.3859751522541046, -0.012796196155250072], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.08213822543621063, -1.0490362910786644e-05, -5.185469490243122e-05, -0.007920405827462673, -0.0035200081765651703, -0.00023398046323563904, -0.009247093461453915, -0.012697681784629822, -0.06290388107299805, -0.0004122599493712187, -9.48860906646587e-05, -2.992108420585282e-05, -0.0737694650888443, -0.5549620985984802], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-1.0263607501983643, -0.02688584290444851], "finish_reason": "stop"}, {"text": "\"File is empty or contains only stop words.\")", "tokens": ["\"", "File", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.05799452215433121, -4.067643642425537, -0.32064366340637207, -0.025140831246972084, -0.04759936034679413, -0.044686246663331985, -0.022828396409749985, -0.009031279943883419, -0.08234552294015884, -0.9799370169639587, -0.13956432044506073, -0.0012688927818089724, -0.0007065422250889242], "finish_reason": "stop"}, {"text": "", "tokens": ["\n"], "logprobs": [-1.503070592880249], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.9166378974914551, -9.775113539944869e-06, -5.4596363042946905e-05, -0.0005173536483198404, -0.048206958919763565, -0.26646578311920166, -0.0014323461800813675, -0.001379848807118833, -0.253238320350647, -0.012998255901038647, -0.24745024740695953], "finish_reason": "stop"}, {"text": " [(word, count) for word, count in zip(vectorizer.get_feature_names_out(), sum_words.tolist())]", "tokens": [" [(", "word", ",", " count", ")", " for", " word", ",", " count", " in", " zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "()", ",", " sum", "_", "words", ".", "tol", "ist", "()", ")]", "\n"], "logprobs": [-2.929225444793701, -0.20923279225826263, -0.079502634704113, -0.5318044424057007, -0.1285347193479538, -0.07717729359865189, -0.05915679410099983, -0.0005040090763941407, -0.0012721073580905795, -0.001746439840644598, -0.22454999387264252, -0.004546068608760834, -0.05580279603600502, -3.2186455882765586e-06, -4.851700214203447e-05, -0.04313213750720024, -2.7418097943154862e-06, -0.00024148885859176517, -2.3841855067985307e-07, -0.0024989587254822254, -0.9259944558143616, -4.1126360883936286e-05, -0.00546923978254199, -0.0020367607939988375, -0.15576030313968658, -0.0008198237628675997, -6.318072337307967e-06, -2.628831624984741, -0.5792156457901001, -2.264974000354414e-06, -0.09569419920444489, -0.0012441283324733377, -0.0024629279505461454], "finish_reason": "stop"}, {"text": "word, count) for word, count in zip(vectorizer.get_feature_names(), sum_words)", "tokens": ["word", ",", " count", ")", " for", " word", ",", " count", " in", " zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ",", " sum", "_", "words", ")", "\n"], "logprobs": [-0.12794990837574005, -0.05608099699020386, -0.869542121887207, -0.2642706632614136, -0.49517491459846497, -0.18298915028572083, -0.0007327494095079601, -0.0018133163684979081, -0.0017628619680181146, -0.13414530456066132, -0.003329687984660268, -0.04896846413612366, -3.4570634852570947e-06, -5.411955135059543e-05, -0.036374520510435104, -2.622600959512056e-06, -0.00025781645672395825, -2.3841855067985307e-07, -0.0015170981641858816, -0.34930717945098877, -0.0008997444529086351, -0.07403451949357986, -0.0003868784988299012, -7.271740287251305e-06, -0.15840166807174683, -0.034774962812662125], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.07731997221708298, -0.0003424296446610242, -0.057719215750694275, -0.0007765853079035878, -5.638440416078083e-05, -0.0020378315821290016, -0.013358143158257008, -0.0009952597320079803, -0.001482579275034368, -0.05189269036054611, -0.0003513672563713044, -0.395082950592041, -0.007509456016123295, -0.001935396110638976, -0.052944380789995193, -0.00013100242358632386, -0.000910106289666146, -0.00013016807497479022, -0.00020823694649152458, -0.011205833405256271, -0.22049130499362946], "finish_reason": "stop"}, {"text": " words_freq[0:10]", "tokens": [" words", "_", "freq", "[", "0", ":", "1", "0", "]", "\n"], "logprobs": [-0.5951852798461914, -0.00010823617776622996, -5.6503606174374e-05, -2.029869794845581, -0.0251882616430521, -0.02507735788822174, -0.030422495678067207, -0.008063383400440216, -0.0020674539264291525, -0.012213443405926228], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.006761291529983282, -0.0016621834365651011, -0.001563875237479806, -0.06352351605892181, -0.42311394214630127, -0.0001586549769854173, -1.0847986231965479e-05, -0.05497900769114494, -0.007765462156385183, -0.017905617132782936, -0.001259725191630423, -1.9951763153076172, -0.0038371284026652575, -9.846202738117427e-05, -0.0007525234250351787, -0.4222602844238281, -0.0029530986212193966, -0.0020679295994341373, -0.04104108735918999], "finish_reason": "stop"}, {"text": " df.plot.barh(x=\"Word\", y=\"Count\", color=\"skyblue\")", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " color", "=", "\"", "sky", "blue", "\"", ")", "\n"], "logprobs": [-0.08616777509450912, -0.04158921539783478, -0.18172159790992737, -0.28365516662597656, -0.00979793444275856, -1.1483498811721802, -0.015971112996339798, -0.3263317048549652, -0.023578030988574028, -0.0008996253600344062, -0.01081639714539051, -2.8609820219571702e-05, -0.002131453948095441, -0.057896874845027924, -0.0010186012368649244, -0.0003184764937032014, -5.864924969500862e-05, -1.537788011773955e-05, -0.2289232462644577, -2.4332761764526367, -0.062220919877290726, -0.006674377713352442, -3.6206884384155273, -0.00731302984058857, -0.0003156163729727268, -0.7766869068145752, -0.01648109406232834], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.01343694981187582, -2.276871418871451e-05, -2.9802276912960224e-06, -1.0598881244659424, -0.04010678827762604, -1.168244216387393e-05, -0.013459885492920876, -0.0014529397012665868], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.002205917378887534, -0.00011324241495458409, -8.237022848334163e-05, -0.003924170508980751, -3.2186455882765586e-06, -1.0371154530730564e-05, -0.11059090495109558, -0.0035590894985944033], "finish_reason": "stop"}, {"text": "clf()  # Clear the plot for further operations", "tokens": ["cl", "f", "()", "  ", "#", " Clear", " the", " plot", " for", " further", " operations", "\n"], "logprobs": [-2.6544313430786133, -0.015986600890755653, -0.0004468158003874123, -1.9700279235839844, -0.0010343207977712154, -0.21399933099746704, -0.11999353766441345, -0.9544391632080078, -1.6886684894561768, -1.615545630455017, -2.634796142578125, -0.056692250072956085], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.052539534866809845, -0.009648594073951244], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.0220640916377306, -1.1997429132461548], "finish_reason": "stop"}], "metrics": {"energy": 394.95026858615864, "energy_per_token": 1.5925414055893494, "time": 15.355851, "time_per_token": 0.061918754032258064}}
{"outputs": [{"text": " pd.read_csv(file_path, keep_default_na=False)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " keep", "_", "default", "_", "na", "=", "False", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -1.1519023180007935, -3.59928822517395, -1.1444026313256472e-05, -0.1050681620836258, -9.298280929215252e-06, -0.21108295023441315, -0.0012369846226647496, -0.021638382226228714, -0.42387136816978455, -0.08554002642631531], "finish_reason": "stop"}, {"text": " df[\"Text\"].apply(lambda x: x.lower())", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "apply", "(", "lambda", " x", ":", " x", ".", "lower", "())", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.800536870956421, -0.011861036531627178, -0.26765576004981995, -0.13337089121341705, -0.000940595637075603, -0.3939773738384247, -0.022056277841329575, -1.0188205242156982, -0.25290974974632263, -0.19035355746746063], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -0.3859751522541046, -0.012796196155250072], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray().sum(axis=0)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.08213822543621063, -1.0490362910786644e-05, -5.185469490243122e-05, -0.007920405827462673, -0.0035200081765651703, -0.00023398046323563904, -0.009247093461453915, -0.012697681784629822, -0.06290388107299805, -0.0004122599493712187, -9.48860906646587e-05, -2.992108420585282e-05, -0.0737694650888443, -0.8698386549949646, -0.33098796010017395, -0.0031708222813904285, -0.131098210811615, -1.2848833799362183, -0.6666809320449829, -0.05195425823330879, -0.02697090245783329, -0.002679926110431552, -0.03787063807249069, -0.01465743500739336, -0.07468236237764359], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-1.0263607501983643, -0.02688584290444851], "finish_reason": "stop"}, {"text": "\"Input file is empty or contains only stop words.\")", "tokens": ["\"", "Input", " file", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.05799452215433121, -1.2033532857894897, -1.1196528673171997, -0.072927325963974, -0.025309031829237938, -0.03515943884849548, -0.022987082600593567, -0.012391484342515469, -0.008246419951319695, -0.07742434740066528, -0.6319409608840942, -0.1166851744055748, -0.0013013951247557998, -0.0007808735244907439], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.25942009687423706, -0.00946392398327589], "finish_reason": "stop"}, {"text": " word_count.sum(axis=1)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "1", ")", "\n"], "logprobs": [-0.9166378974914551, -9.775113539944869e-06, -5.4596363042946905e-05, -0.0005173536483198404, -0.048206958919763565, -0.26646578311920166, -0.0014323461800813675, -0.001379848807118833, -1.5102325677871704, -0.018618697300553322, -0.29209575057029724], "finish_reason": "stop"}, {"text": " sum_words.to_dict()", "tokens": [" sum", "_", "words", ".", "to", "_", "dict", "()", "\n"], "logprobs": [-2.5417001247406006, -0.0022501645144075155, -2.4914430468925275e-05, -0.2738305330276489, -0.8083105683326721, -0.6653984785079956, -0.09031730145215988, -0.048053134232759476, -0.013539273291826248], "finish_reason": "stop"}, {"text": "word, sum_words[word_count.indptr[i : i + 2]]) for i, word in enumerate(vectorizer.get_feature_names_out())", "tokens": ["word", ",", " sum", "_", "words", "[", "word", "_", "count", ".", "ind", "ptr", "[", "i", " :", " i", " +", " ", "2", "]]", ")", " for", " i", ",", " word", " in", " enumerate", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "())", "\n"], "logprobs": [-0.12461002916097641, -0.05556375905871391, -0.9417624473571777, -0.012658720836043358, -0.012033015489578247, -0.1000472828745842, -2.3465487957000732, -2.7137162685394287, -0.8506461381912231, -0.12937699258327484, -0.8245186805725098, -0.005146825686097145, -0.018503786996006966, -0.253839373588562, -2.8775475025177, -0.013928120955824852, -0.0032870331779122353, -0.00045753977610729635, -0.005270870868116617, -0.044639162719249725, -0.05664437636733055, -1.3993628025054932, -0.34767359495162964, -0.008514412678778172, -0.010021844878792763, -0.015369783155620098, -0.03240463510155678, -0.00032658010604791343, -0.24712452292442322, -5.4834770708112046e-05, -0.0001554368354845792, -0.22021731734275818, -2.9802276912960224e-06, -0.0002474478678777814, -1.1920928244535389e-07, -0.0018275955226272345, -0.9224890470504761, -4.017272294731811e-05, -0.008636976592242718, -0.004358198493719101], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.08517280220985413, -0.00030155404238030314, -0.08853212743997574, -0.0008435266790911555, -6.69933797325939e-05, -0.0023137959651649, -0.021477147936820984, -0.001116367639042437, -0.0015553055563941598, -0.044069547206163406, -0.000259365770034492, -0.3924696445465088, -0.009769248776137829, -0.0018243826925754547, -0.05934666842222214, -0.00013565097469836473, -0.0009190387791022658, -0.00012396997772157192, -0.0002040654799202457, -0.011209841817617416, -0.28774136304855347], "finish_reason": "stop"}, {"text": " words_freq[0:10]", "tokens": [" words", "_", "freq", "[", "0", ":", "1", "0", "]", "\n"], "logprobs": [-0.5074936151504517, -0.00012706902634818107, -6.01988795096986e-05, -1.9735026359558105, -0.028970425948500633, -0.025049572810530663, -0.030622757971286774, -0.00773222278803587, -0.0020776845049113035, -0.013278624042868614], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.008206930942833424, -0.001693720929324627, -0.0016177912475541234, -0.06102048605680466, -3.1257383823394775, -0.12714527547359467, -0.11325833201408386, -6.770858453819528e-05, -6.437280717364047e-06, -0.04251733422279358, -0.015117477625608444, -0.019937586039304733, -0.0020256969146430492, -0.21344375610351562, -0.0021672588773071766, -0.0001515035255579278, -0.0008131535141728818, -0.6330738067626953, -0.00951458141207695, -0.0039339070208370686, -0.033229000866413116], "finish_reason": "stop"}, {"text": " df.plot.bar(x='Word', y='Count', rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "='", "Word", "'", ",", " y", "='", "Count", "'", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.07002115249633789, -0.03281736373901367, -0.1561918407678604, -0.2775906026363373, -0.012143254280090332, -0.40294209122657776, -0.3671712577342987, -3.009855031967163, -0.000898077036254108, -0.01654534973204136, -1.4543427823809907e-05, -0.07721889019012451, -0.0058569712564349174, -5.0424259825376794e-05, -0.3181650638580322, -0.00015400654228869826, -0.44239044189453125, -0.005147181451320648, -0.45044925808906555, -0.5770390629768372, -0.0159186702221632], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.013098505325615406, -2.098061486321967e-05, -2.622600959512056e-06, -1.0498309135437012, -0.03817475587129593, -1.1444026313256472e-05, -0.010569428093731403, -0.0015303102554753423], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0022481423802673817, -0.00010418349120300263, -7.235741941258311e-05, -0.0033014100044965744, -2.9802276912960224e-06, -9.179073458653875e-06, -0.09174806624650955, -0.003647818695753813], "finish_reason": "stop"}, {"text": "close(fig=None)  # Ensure the plot is closed after saving", "tokens": ["close", "(", "fig", "=", "None", ")", "  ", "#", " Ensure", " the", " plot", " is", " closed", " after", " saving", "\n"], "logprobs": [-0.11789948493242264, -3.0193772315979004, -1.9660112857818604, -0.049266841262578964, -0.5161629319190979, -0.002881662454456091, -1.8791996240615845, -0.0012354368809610605, -2.7756266593933105, -1.2942851781845093, -0.39664337038993835, -0.15618807077407837, -0.16285188496112823, -2.0305733680725098, -0.02288409136235714, -0.10744760930538177], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05766632780432701, -0.010389883071184158], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.022333068773150444], "finish_reason": "stop"}], "metrics": {"energy": 405.0798354606629, "energy_per_token": 1.5402275112572732, "time": 15.973429, "time_per_token": 0.060735471482889734}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -0.4749401807785034, -0.000929400441236794, -0.37533655762672424], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -0.3178824782371521, -0.017030777409672737], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).todense()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "t", "od", "ense", "()", "\n"], "logprobs": [-0.06323696672916412, -9.059865078597795e-06, -5.364274329622276e-05, -0.005345338024199009, -0.003966085612773895, -0.0002489972102921456, -0.013667807914316654, -0.012809259817004204, -0.05914623290300369, -0.00033682872890494764, -9.190614218823612e-05, -3.814624506048858e-05, -0.062393974512815475, -0.8757088780403137, -2.9123282432556152, -0.00010501786891836673, -0.00392618915066123, -0.014009351842105389, -0.3844643831253052], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-0.9912388324737549, -0.02996828407049179], "finish_reason": "stop"}, {"text": "\"No valid words found in the data.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " data", ".", "\"", ")", "\n"], "logprobs": [-0.04835723713040352, -1.4620225429534912, -0.41416141390800476, -0.3039606213569641, -0.06703125685453415, -0.19624696671962738, -0.12876133620738983, -1.7291216850280762, -0.32911890745162964, -0.1023046150803566, -0.0015732781030237675, -0.000764792668633163], "finish_reason": "stop"}, {"text": "", "tokens": ["\n"], "logprobs": [-1.4698673486709595], "finish_reason": "stop"}, {"text": " word_count.sum()", "tokens": [" word", "_", "count", ".", "sum", "()", "\n"], "logprobs": [-0.9671698212623596, -1.537788011773955e-05, -4.184158387943171e-05, -0.00039962888695299625, -0.052819326519966125, -1.3285242319107056, -0.05444302409887314], "finish_reason": "stop"}, {"text": " (sum_words.toarray().T / sum_words.sum(axis=0).toarray()).T", "tokens": [" (", "sum", "_", "words", ".", "to", "array", "()", ".", "T", " /", " sum", "_", "words", ".", "sum", "(", "axis", "=", "0", ")", ".", "to", "array", "())", ".", "T", "\n"], "logprobs": [-1.7820520401000977, -1.4146068096160889, -0.0018797124503180385, -7.211902266135439e-05, -0.8266507387161255, -1.4144315719604492, -0.014833513647317886, -0.24474100768566132, -0.2865157425403595, -2.2873775959014893, -0.6241201758384705, -0.6252256631851196, -0.005513580050319433, -1.3947389561508317e-05, -0.13157615065574646, -0.25768348574638367, -0.4939826726913452, -0.010699761100113392, -0.004453621339052916, -0.6528676152229309, -0.7881057262420654, -0.140866219997406, -0.3389096260070801, -0.000433112756581977, -0.10345384478569031, -0.26451537013053894, -0.33158499002456665, -0.15847450494766235], "finish_reason": "stop"}, {"text": "word, count / sum_words.get(\"count\"))", "tokens": ["word", ",", " count", " /", " sum", "_", "words", ".", "get", "(", "\"", "count", "\"", "))", "\n"], "logprobs": [-0.1385936141014099, -0.05494019389152527, -0.8414077758789062, -1.665346384048462, -0.0596461221575737, -0.014868514612317085, -0.00033825874561443925, -1.3254995346069336, -0.6045138239860535, -0.015131332911550999, -0.1555272787809372, -0.23680955171585083, -0.2192794382572174, -2.7660586833953857, -0.8236696720123291], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.08692649751901627, -0.00024911639047786593, -0.10533309727907181, -0.00086758635006845, -7.366862701019272e-05, -0.0018290233565494418, -0.021028894931077957, -0.0008897398365661502, -0.001640761154703796, -0.047685518860816956, -0.0002543602604418993, -0.4344460368156433, -0.009537843056023121, -0.0019296851241961122, -0.05690455064177513, -0.0001284993631998077, -0.0009077242575585842, -0.0001586549769854173, -0.0002026352594839409, -0.011722136288881302, -0.28695791959762573], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5114873051643372, -8.976056415122002e-05, -4.8040190449682996e-05, -0.16322660446166992, -0.016932561993598938, -0.007023644633591175, -0.0012553200358524919, -0.011402557604014874], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.006979490164667368, -0.0018434212543070316, -0.0019119569333270192, -0.06184973567724228, -0.37137892842292786, -0.00013529339048545808, -1.0013530300057027e-05, -0.055508725345134735, -0.007615699432790279, -0.018185783177614212, -0.000922849983908236, -0.2348947376012802, -0.002336750039830804, -9.83428253675811e-05, -0.0007284610182978213, -0.7947800755500793, -0.08017907291650772, -0.004218489397317171, -0.0023205752950161695, -0.03199854493141174], "finish_reason": "stop"}, {"text": " df.hstack().plot(x=\"Word\", y=\"Count\", kind=\"bar\")", "tokens": [" df", ".", "h", "stack", "()", ".", "plot", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " kind", "=", "\"", "bar", "\"", ")", "\n"], "logprobs": [-0.07552114129066467, -0.0390535406768322, -2.5806849002838135, -0.8571662902832031, -2.000178337097168, -0.32059311866760254, -0.22938509285449982, -0.7901012301445007, -2.8178281784057617, -0.07918109744787216, -0.005573210772126913, -0.012048562988638878, -0.00021836756786797196, -0.006521135102957487, -0.02809443697333336, -0.021834464743733406, -0.0017298986203968525, -0.0009321396937593818, -0.00012516192509792745, -0.3182154595851898, -0.5099704265594482, -0.011002586223185062, -0.0009109399979934096, -0.017789335921406746, -0.6695263981819153, -0.6103399991989136, -0.023898979648947716], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.010000600479543209, -2.002696055569686e-05, -2.622600959512056e-06, -0.4631122946739197, -0.0019516960019245744], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0023313979618251324, -8.141662692651153e-05, -8.451581379631534e-05, -0.0031735554803162813, -2.50339189733495e-06, -9.775113539944869e-06, -0.13065926730632782, -0.003920489456504583], "finish_reason": "stop"}, {"text": "close()  # Ensure that the plot is closed after saving", "tokens": ["close", "()", "  ", "#", " Ensure", " that", " the", " plot", " is", " closed", " after", " saving", "\n"], "logprobs": [-0.1205669566988945, -0.05373459681868553, -1.777782917022705, -0.0005308172549121082, -2.3428969383239746, -1.7976033687591553, -0.3715720474720001, -0.1715274602174759, -0.09035716205835342, -0.20387093722820282, -1.1237142086029053, -0.0200019720941782, -0.12471336871385574], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05084215849637985, -0.008752908557653427], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.025883391499519348, -0.9593029618263245, -4.768370445162873e-07, -1.311301275563892e-06, -7.486063259420916e-05, -0.00010978573118336499, -0.0807899609208107], "finish_reason": "stop"}], "metrics": {"energy": 372.05974830341347, "energy_per_token": 1.589998924373562, "time": 15.354371, "time_per_token": 0.06561697008547009}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].apply(lambda x: x.lower() if not pd.isna(x) else \"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "apply", "(", "lambda", " x", ":", " x", ".", "lower", "()", " if", " not", " pd", ".", "is", "na", "(", "x", ")", " else", " \"", "\"", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.800536870956421, -0.011861036531627178, -0.26765576004981995, -0.13337089121341705, -0.000940595637075603, -0.3939773738384247, -0.022056277841329575, -1.0188205242156982, -1.5003653764724731, -3.0308964252471924, -1.5874738693237305, -0.24834394454956055, -0.0003352795320097357, -0.006375094875693321, -0.5799313187599182, -0.00011228884250158444, -0.0022590849548578262, -0.0036802436225116253, -0.020563973113894463, -0.29158371686935425, -0.10020971298217773, -0.002380396705120802, -0.14035654067993164], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -0.3859751522541046, -0.012796196155250072], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.08213822543621063, -1.0490362910786644e-05, -5.185469490243122e-05, -0.007920405827462673, -0.0035200081765651703, -0.00023398046323563904, -0.009247093461453915, -0.012697681784629822, -0.06290388107299805, -0.0004122599493712187, -9.48860906646587e-05, -2.992108420585282e-05, -0.0737694650888443, -0.5549620985984802], "finish_reason": "stop"}, {"text": " ValueError as e:", "tokens": [" Value", "Error", " as", " e", ":", "\n"], "logprobs": [-1.7627403736114502, -1.9550132492440753e-05, -0.9518560171127319, -0.23553027212619781, -0.0009239218779839575, -0.02408147044479847], "finish_reason": "stop"}, {"text": "\"Input data is empty or contains only stop words.\")", "tokens": ["\"", "Input", " data", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.05118483304977417, -1.3254835605621338, -1.3453491926193237, -0.09585462510585785, -0.10305517911911011, -0.08000236004590988, -0.06941387802362442, -0.018397290259599686, -0.007970432750880718, -0.12289825826883316, -0.5820090174674988, -0.14437152445316315, -0.00129270413890481, -0.0006417360855266452], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.3026447892189026, -0.009665004909038544], "finish_reason": "stop"}, {"text": " sum([word.count(\" \") for word in df[\"Text\"]])", "tokens": [" sum", "([", "word", ".", "count", "(", "\"", " \"", ")", " for", " word", " in", " df", "[", "\"", "Text", "\"", "]]", ")", "\n"], "logprobs": [-1.3477206230163574, -2.9212486743927, -1.8430132865905762, -1.6516331434249878, -0.338336706161499, -0.3546304404735565, -0.3527103364467621, -0.03226855769753456, -0.024702126160264015, -0.9597089290618896, -0.0775088518857956, -0.005260909907519817, -1.0910112857818604, -0.030796626582741737, -0.00017426878912374377, -0.000442407006630674, -3.099393507000059e-05, -0.27599573135375977, -0.009131573140621185, -0.10243461281061172], "finish_reason": "stop"}, {"text": " [", "tokens": [" [", "\n"], "logprobs": [-3.6290059089660645, -1.175534963607788], "finish_reason": "stop"}, {"text": "word, sum_words[0][i])", "tokens": ["word", ",", " sum", "_", "words", "[", "0", "][", "i", "])", "\n"], "logprobs": [-0.15636874735355377, -0.054817307740449905, -1.051330804824829, -0.018898196518421173, -0.013116389513015747, -0.08651156723499298, -2.9545154571533203, -0.36542192101478577, -0.6413679718971252, -0.076957106590271, -0.8715846538543701], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.08849229663610458, -0.0002516191452741623, -0.06805724650621414, -0.0011375630274415016, -6.007967749610543e-05, -0.0017138330731540918, -0.015168554149568081, -0.0010804060148075223, -0.0010412277188152075, -0.040080104023218155, -0.00029738296871073544, -0.43401476740837097, -0.008418548852205276, -0.0016446886584162712, -0.0637134537100792, -0.00014411364099942148, -0.0009191579301841557, -0.0001255195093108341, -0.00018356545479036868, -0.013517397455871105, -0.16878417134284973], "finish_reason": "stop"}, {"text": " [word for word, _ in words_freq[:10]]", "tokens": [" [", "word", " for", " word", ",", " _", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-1.326381802558899, -0.5953212380409241, -0.17090055346488953, -0.09047456085681915, -0.008227621205151081, -0.358819842338562, -0.002501931507140398, -0.005254743155092001, -9.107174992095679e-05, -2.992108420585282e-05, -0.37485697865486145, -0.021347278729081154, -0.00863284058868885, -0.0075617507100105286, -0.006969072390347719], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.007790065370500088, -0.001591369160450995, -0.0016920547932386398, -0.06365551054477692, -0.27568766474723816, -0.00014208737411536276, -1.0013530300057027e-05, -0.04704371094703674, -0.007747363764792681, -0.02018984779715538, -0.0010894560255110264, -0.2183346450328827, -0.0022611068561673164, -0.00012313561455812305, -0.00086758635006845, -0.6225271224975586, -0.007972680032253265, -0.002153698354959488, -0.02829284965991974], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", title=\"Top 10 Words\", figsize=(10, 5))", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " title", "=", "\"", "Top", " ", "1", "0", " Words", "\"", ",", " fig", "size", "=(", "1", "0", ",", " ", "5", "))", "\n"], "logprobs": [-0.07877461612224579, -0.03200293332338333, -0.15120866894721985, -0.29213136434555054, -0.01018340140581131, -0.475741446018219, -0.3502955436706543, -0.048044610768556595, -0.0011513754725456238, -0.0008605591137893498, -3.4689302992774174e-05, -0.004298138897866011, -0.03756608068943024, -0.0017547699389979243, -0.0002456601650919765, -4.255681051290594e-05, -1.8596476365928538e-05, -0.33307820558547974, -2.205885648727417, -0.007967357523739338, -0.01801462471485138, -0.8394690155982971, -0.08241183310747147, -0.0018720973748713732, -0.00020001317898277193, -1.0989928245544434, -0.8233240842819214, -0.7252522706985474, -0.6772487163543701, -0.0006908176001161337, -0.016015343368053436, -0.3270466923713684, -0.6719101667404175, -0.0025362963788211346, -0.0074089993722736835, -0.3552089035511017, -0.21323640644550323, -0.003394795348867774], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.011482462286949158, -2.407998726994265e-05, -2.7418097943154862e-06, -0.9116970896720886, -0.04005192965269089, -1.0251946150674485e-05, -0.01141057163476944, -0.001167211914435029], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0024273712188005447, -0.00011336160969221964, -8.010543388081715e-05, -0.004037800244987011, -2.622600959512056e-06, -1.2159273865108844e-05, -0.122987762093544, -0.003981521353125572], "finish_reason": "stop"}, {"text": "show()", "tokens": ["show", "()", "\n"], "logprobs": [-2.737854480743408, -0.06754706054925919, -0.038944508880376816], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.045775145292282104, -0.009756852872669697], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.02316403016448021, -1.106344223022461], "finish_reason": "stop"}], "metrics": {"energy": 355.55541916275, "energy_per_token": 1.469237269267562, "time": 15.126275, "time_per_token": 0.06250526859504132}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].fillna(\"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "fill", "na", "(", "\"", "\"", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.75266432762146, -1.0251946150674485e-05, -0.023330699652433395, -0.09762462228536606, -0.26671287417411804, -0.000740968796890229, -0.3735211491584778], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS, min_df=1)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " min", "_", "df", "=", "1", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -1.1483709812164307, -3.44325590133667, -1.9788545614574105e-05, -0.06235454976558685, -0.002486353972926736, -1.1060148477554321, -0.6059301495552063, -0.02501399628818035], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).sum(axis=0)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.06617683917284012, -1.07287787614041e-05, -5.376194530981593e-05, -0.0070527647621929646, -0.004342530854046345, -0.00016366096679121256, -0.015089061111211777, -0.011216914281249046, -0.07005982846021652, -0.0003856868715956807, -8.809178689261898e-05, -2.7417760065873154e-05, -0.06754259765148163, -0.7628987431526184, -1.8537400960922241, -0.004047180060297251, -0.0029498895164579153, -0.0011828100541606545, -0.04347095638513565, -0.004480323754251003, -0.21195447444915771], "finish_reason": "stop"}, {"text": " IndexError as e:", "tokens": [" Index", "Error", " as", " e", ":", "\n"], "logprobs": [-3.384732723236084, -3.218599158572033e-05, -2.4243147373199463, -0.2835733890533447, -0.001374848885461688, -0.0296857301145792], "finish_reason": "stop"}, {"text": "\"The input data is empty or contains only stop words.\")", "tokens": ["\"", "The", " input", " data", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.050182193517684937, -2.316488265991211, -0.4783623516559601, -1.6913018226623535, -0.10144956409931183, -0.15245650708675385, -0.05379853770136833, -0.05866677686572075, -0.015008974820375443, -0.010382096283137798, -0.11712860316038132, -0.25597846508026123, -0.11719190329313278, -0.0015163839561864734, -0.0006996329175308347], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.21010003983974457, -0.00888135191053152], "finish_reason": "stop"}, {"text": " np.array(word_count.sum(axis=0))", "tokens": [" np", ".", "array", "(", "word", "_", "count", ".", "sum", "(", "axis", "=", "0", "))", "\n"], "logprobs": [-1.953137755393982, -9.083335316972807e-05, -1.7106454372406006, -0.08183873444795609, -0.03428908810019493, -2.9205850296420977e-05, -5.185469490243122e-05, -0.003727513598278165, -0.03463241457939148, -0.00038961926475167274, -0.003424733877182007, -0.0008175606490112841, -0.34123679995536804, -0.11927897483110428, -0.30526891350746155], "finish_reason": "stop"}, {"text": " word_count.T / sum_words", "tokens": [" word", "_", "count", ".", "T", " /", " sum", "_", "words", "\n"], "logprobs": [-1.9181848764419556, -0.001765717868693173, -1.966933996300213e-05, -0.029423121362924576, -1.4829962253570557, -0.965147852897644, -0.03029482252895832, -0.0020965994335711002, -4.7444173105759546e-05, -0.1399676650762558], "finish_reason": "stop"}, {"text": "word, int(word_count[word])) for word in vectorizer.get_feature_names()", "tokens": ["word", ",", " int", "(", "word", "_", "count", "[", "word", "]))", " for", " word", " in", " vector", "izer", ".", "get", "_", "feature", "_", "names", "()", "\n"], "logprobs": [-0.1372813731431961, -0.054554782807826996, -2.8440818786621094, -0.013477527536451817, -3.2250869274139404, -0.004342530854046345, -0.017145272344350815, -0.21869325637817383, -0.9196308255195618, -1.1376749277114868, -0.6561712622642517, -0.026466593146324158, -0.2769263982772827, -0.3315209746360779, -9.65590606938349e-06, -0.0001294529065489769, -0.14866669476032257, -1.9073468138230965e-06, -0.0003219324571546167, -1.1920928244535389e-07, -0.0014659145381301641, -0.4605409502983093, -0.030528413131833076], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.09775900840759277, -0.0003367095487192273, -0.11057265102863312, -0.0009127265075221658, -5.3165931603871286e-05, -0.0021089711226522923, -0.020131314173340797, -0.001158519764430821, -0.001336158369667828, -0.04642573371529579, -0.00025340684805996716, -0.4184824526309967, -0.009712346829473972, -0.0019991665612906218, -0.059662070125341415, -0.00013362467871047556, -0.0008997444529086351, -0.00011991735664196312, -0.00019393471302464604, -0.01105151604861021, -1.752866268157959, -0.025307519361376762, -0.006729676853865385, -0.0033545196056365967, -0.009828861802816391], "finish_reason": "stop"}, {"text": " words_freq[0:10]", "tokens": [" words", "_", "freq", "[", "0", ":", "1", "0", "]", "\n"], "logprobs": [-0.5888634920120239, -0.0001333863037871197, -6.496695277746767e-05, -1.9815752506256104, -0.026396004483103752, -0.027754541486501694, -0.0327238030731678, -0.008799348957836628, -0.0021454906091094017, -0.012015347369015217], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=top_words, columns=[\"Word\", \"Freq\"])", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "req", "\"", "])", "\n"], "logprobs": [-0.006644062697887421, -0.001522930571809411, -0.0015852991491556168, -0.06600511074066162, -3.2194395065307617, -0.14752376079559326, -0.13324610888957977, -6.19869097135961e-05, -7.033323527139146e-06, -0.04809630662202835, -0.01549526583403349, -0.02118786610662937, -0.0015398323303088546, -0.2369113266468048, -0.0025572238955646753, -0.00011574551899684593, -0.0008447177824564278, -0.835319459438324, -2.318591594696045, -0.03793387860059738, -0.003907546401023865, -0.03750384971499443], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.08135531097650528, -0.03714956343173981, -0.1427251100540161, -0.31087878346443176, -0.012698741629719734, -0.4370051920413971, -0.39104411005973816, -0.06392259150743484, -0.0008385241380892694, -0.0011453027836978436, -3.58813522325363e-05, -0.005009480752050877, -0.053549643605947495, -0.0016195764765143394, -0.00027855331427417696, -4.3987260141875595e-05, -2.169585604860913e-05, -0.27295443415641785, -0.615136981010437, -0.004052166361361742, -0.4424038529396057, -0.6665622591972351, -0.01656610332429409], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.014402717351913452, -2.47952248173533e-05, -2.3841830625315197e-06, -0.514597475528717, -0.0020648366771638393], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.002077327808365226, -0.00011586471373448148, -7.366862701019272e-05, -0.003304499201476574, -2.7418097943154862e-06, -1.1324817933200393e-05, -0.12165813893079758, -0.00406641373410821], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.14678670465946198, -0.04857086390256882, -0.1926424354314804], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.052236925810575485, -0.008568309247493744], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.02325604483485222, -1.0778937339782715, -4.768370445162873e-07, -1.0728830375228426e-06, -8.594620157964528e-05, -0.00010048838157672435, -0.08374881744384766], "finish_reason": "stop"}], "metrics": {"energy": 361.59032743263225, "energy_per_token": 1.481927571445214, "time": 15.13166, "time_per_token": 0.062015}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].apply(lambda s: \" \".join([w for w in s.split() if w not in STOP_WORDS]))", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "apply", "(", "lambda", " s", ":", " \"", " \"", ".", "join", "([", "w", " for", " w", " in", " s", ".", "split", "()", " if", " w", " not", " in", " ST", "OP", "_", "W", "OR", "DS", "]))", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.800536870956421, -0.011861036531627178, -0.26765576004981995, -2.859189510345459, -0.0008867622236721218, -1.31766939163208, -0.04673761501908302, -0.0018954183906316757, -0.00014840454969089478, -0.40019914507865906, -0.6023914813995361, -0.09968125075101852, -0.00016652150952722877, -0.00031132620642893016, -0.04176347702741623, -0.001729184528812766, -0.18690115213394165, -0.13237586617469788, -0.03716380521655083, -0.05280892178416252, -0.2754817008972168, -0.00011169286881340668, -0.0038226405158638954, -7.152555099310121e-07, -6.961580220377073e-05, -9.298280929215252e-06, -1.0728830375228426e-06, -3.40932747349143e-05, -0.025157803669571877, -0.060715384781360626], "finish_reason": "stop"}, {"text": " CountVectorizer(lowercase=True, stop_words=STOP_WORDS, token_pattern=\"\\\\w+\")", "tokens": [" Count", "Vector", "izer", "(", "low", "erc", "ase", "=", "True", ",", " stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ",", " token", "_", "pattern", "=", "\"", "\\\\", "w", "+", "\"", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -2.5649709701538086, -0.00028701478731818497, -5.972207145532593e-05, -0.0007078525959514081, -0.04806233569979668, -0.03318829461932182, -0.1467299908399582, -5.8887653722194955e-05, -5.709961988031864e-05, -0.01859845407307148, -0.05762356519699097, -7.152555099310121e-07, -0.0003238391946069896, -3.099436753473128e-06, -1.9073468138230965e-06, -2.3603161025675945e-05, -1.6554373502731323, -1.7594348192214966, -0.20037414133548737, -0.017317282035946846, -0.06590232998132706, -0.8760606646537781, -0.09713850170373917, -0.9493061304092407, -0.0781409963965416, -0.0393044650554657, -0.1510334014892578, -0.013060619123280048], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray().sum(axis=0).tolist()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", ".", "sum", "(", "axis", "=", "0", ")", ".", "tol", "ist", "()", "\n"], "logprobs": [-0.07234232872724533, -1.1205610462639015e-05, -5.829164365422912e-05, -0.008356131613254547, -0.0031547797843813896, -0.0002703301142901182, -0.01030812133103609, -0.012856689281761646, -0.057394981384277344, -0.0003927174839191139, -8.368142152903602e-05, -3.957670196541585e-05, -0.07278767228126526, -0.7321472764015198, -0.33158549666404724, -0.0038466285914182663, -0.16205954551696777, -1.1906638145446777, -0.8215287327766418, -0.040980443358421326, -0.02420179173350334, -0.002004758222028613, -0.02767360769212246, -0.014439848251640797, -3.01804780960083, -1.0133850574493408, -1.0728830375228426e-06, -0.6105702519416809, -0.043622177094221115], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-1.6416051387786865, -2.038458114839159e-05, -0.4343394935131073, -0.06158546358346939], "finish_reason": "stop"}, {"text": "\"Input is empty or contains only stop words.\")", "tokens": ["\"", "Input", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.0495629757642746, -1.3125498294830322, -1.571616530418396, -0.06341543793678284, -0.05805054306983948, -0.07430990785360336, -0.018589911982417107, -0.007707026321440935, -0.08881500363349915, -0.9318491220474243, -0.18264718353748322, -0.0012026942567899823, -0.0008476955117657781], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.24584877490997314, -0.00785074383020401], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.9260762929916382, -1.5735502529423684e-05, -4.124556289752945e-05, -0.0004362108593340963, -0.0391930527985096, -0.2916794419288635, -0.0012906801421195269, -0.0012351986952126026, -0.294345885515213, -0.011360363103449345, -0.2384534329175949], "finish_reason": "stop"}, {"text": " pd.DataFrame(word_count.toarray(), columns=vectorizer.get_feature_names())", "tokens": [" pd", ".", "DataFrame", "(", "word", "_", "count", ".", "to", "array", "()", ",", " columns", "=", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-1.3678734302520752, -3.635817120084539e-05, -0.4286699891090393, -0.15922841429710388, -0.4651505649089813, -8.630380034446716e-05, -4.887569048150908e-06, -0.015294062905013561, -0.5187615752220154, -0.0026918151415884495, -0.012483200058341026, -0.07713402807712555, -0.4677203297615051, -0.013973263092339039, -0.014492839574813843, -8.4638240878121e-06, -4.9470632802695036e-05, -0.017051871865987778, -9.536738616588991e-07, -0.00011419598013162613, -1.1920928244535389e-07, -0.0014834124594926834, -0.39272353053092957, -0.42488640546798706], "finish_reason": "stop"}, {"text": "word, sum_words[i]) for word, i in zip(vectorizer.get_feature_names_out(), range(sum_words.shape[0]))", "tokens": ["word", ",", " sum", "_", "words", "[", "i", "])", " for", " word", ",", " i", " in", " zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "()", ",", " range", "(", "sum", "_", "words", ".", "shape", "[", "0", "]))", "\n"], "logprobs": [-0.14329174160957336, -0.05629117414355278, -1.1466796398162842, -0.01829710230231285, -0.015090822242200375, -0.08613540232181549, -0.4417805075645447, -0.09532537311315536, -0.6441489458084106, -0.25173962116241455, -0.000865561596583575, -0.003555526025593281, -0.0029428766574710608, -1.8043276071548462, -0.0010758809512481093, -0.06264929473400116, -3.933898824470816e-06, -5.149708886165172e-05, -0.05976347625255585, -2.622600959512056e-06, -0.0003828269545920193, -1.1920928244535389e-07, -0.001515550771728158, -0.9041508436203003, -3.635817120084539e-05, -0.005397627595812082, -0.0012431758223101497, -0.16124629974365234, -0.000289159914245829, -1.8270437717437744, -0.00468157883733511, -2.539125671319198e-05, -0.004285438451915979, -0.4561693072319031, -0.0029986200388520956, -0.0184260793030262, -0.013348144479095936, -0.014809083193540573], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: -x[1])[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " -", "x", "[", "1", "])", "[:", "1", "0", "]", "\n"], "logprobs": [-0.08534225821495056, -0.00033241944038309157, -0.07185263186693192, -0.0010705222375690937, -7.283422019099817e-05, -0.0017639329889789224, -0.015953749418258667, -0.0011693552369251847, -0.00144710682798177, -0.0633627399802208, -0.00032646095496602356, -1.1643084287643433, -0.0050921509973704815, -0.012259840965270996, -0.00043215948971919715, -0.001665277755819261, -2.436093807220459, -0.02697914093732834, -0.007337407674640417, -0.0030629171524196863, -0.011906272731721401], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.646105945110321, -0.00014935807848814875, -6.222531374078244e-05, -0.15807296335697174, -0.016075171530246735, -0.006403760053217411, -0.0018040348077192903, -0.013214627280831337], "finish_reason": "stop"}, {"text": " pd.DataFrame({\"Word\": [word for word, _ in top_words], \"Frequency\": [count for _, count in top_words]})", "tokens": [" pd", ".", "DataFrame", "(", "{\"", "Word", "\":", " [", "word", " for", " word", ",", " _", " in", " top", "_", "words", "]", ",", " \"", "F", "requency", "\":", " [", "count", " for", " _", ",", " count", " in", " top", "_", "words", "]", "})", "\n"], "logprobs": [-0.006920416373759508, -0.0017789267003536224, -0.0017507239244878292, -0.06860390305519104, -2.1909282207489014, -0.5300948619842529, -0.014763626269996166, -0.09531670063734055, -0.38200515508651733, -0.07299569994211197, -0.03391291946172714, -0.0052846260368824005, -0.2379385381937027, -0.0031466991640627384, -0.0030788423027843237, -4.410734163684538e-06, -3.3378546504536644e-06, -0.007250307127833366, -0.01867053098976612, -0.09451150894165039, -0.6756860613822937, -0.23136788606643677, -0.008022939786314964, -0.004692969378083944, -1.5646756887435913, -0.005531244445592165, -0.004328406415879726, -0.00014101465058047324, -0.004185844212770462, -0.0016614693449810147, -0.0013252056669443846, -7.152531907195225e-06, -2.264974000354414e-06, -0.001328777172602713, -0.0021807001903653145, -0.006223110016435385], "finish_reason": "stop"}, {"text": " df.plot.barh(title=\"10 Most Frequent Words\", x=\"Word\", y=\"Count\")", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "title", "=", "\"", "1", "0", " Most", " F", "requent", " Words", "\"", ",", " x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.092983677983284, -0.03855586424469948, -0.175737202167511, -0.2487429976463318, -0.010497472248971462, -1.1936917304992676, -0.0150154335424304, -3.2848544120788574, -0.014516337774693966, -0.019715769216418266, -3.421128273010254, -0.0018030828796327114, -0.26264727115631104, -1.804689884185791, -0.019975680857896805, -0.027542568743228912, -0.31374940276145935, -0.22257748246192932, -1.1413569450378418, -0.048282500356435776, -0.0016131495358422399, -0.008152777329087257, -5.864924969500862e-05, -0.010803543031215668, -0.04784712567925453, -0.0010754045797511935, -0.00014983485743869096, -5.0424259825376794e-05, -3.576214658096433e-05, -0.5175034403800964, -0.007034416776150465], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.010667680762708187, -2.312633478140924e-05, -3.099436753473128e-06, -0.46946191787719727, -0.001951339072547853], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.002258371328935027, -0.00012444675667211413, -7.402622577501461e-05, -0.003051032545045018, -2.622600959512056e-06, -1.1324817933200393e-05, -0.1262049674987793, -0.00411295285448432], "finish_reason": "stop"}, {"text": "close()  # Close the figure window", "tokens": ["close", "()", "  ", "#", " Close", " the", " figure", " window", "\n"], "logprobs": [-0.123760886490345, -0.05800892040133476, -1.6087327003479004, -0.0007111880695447326, -0.8004169464111328, -0.028675192967057228, -2.237743854522705, -2.6912388801574707, -0.571575403213501], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.048918742686510086, -0.01064479909837246], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.021709773689508438, -0.989498496055603, -4.768370445162873e-07, -1.311301275563892e-06, -6.997340824455023e-05, -0.00010287232726113871, -0.08541734516620636], "finish_reason": "stop"}], "metrics": {"energy": 488.15906062889064, "energy_per_token": 1.4882898189905203, "time": 19.124798, "time_per_token": 0.058307310975609754}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -0.4749401807785034, -0.000929400441236794, -0.37533655762672424], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -0.3178824782371521, -0.017030777409672737], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.06323696672916412, -9.059865078597795e-06, -5.364274329622276e-05, -0.005345338024199009, -0.003966085612773895, -0.0002489972102921456, -0.013667807914316654, -0.012809259817004204, -0.05914623290300369, -0.00033682872890494764, -9.190614218823612e-05, -3.814624506048858e-05, -0.062393974512815475, -0.5512097477912903], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-1.7364170551300049, -1.883488948806189e-05, -0.43747618794441223, -0.052275169640779495], "finish_reason": "stop"}, {"text": "\"No valid words found in the input file.\")", "tokens": ["\"", "No", " valid", " words", " found", " in", " the", " input", " file", ".", "\"", ")", "\n"], "logprobs": [-0.0432562381029129, -1.3518446683883667, -0.3869962692260742, -0.2849676012992859, -0.06432610005140305, -0.22419823706150055, -0.1604989916086197, -0.6911971569061279, -1.0794092416763306, -0.19043880701065063, -0.08528796583414078, -0.0014590105274692178, -0.0007121411035768688], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.28625646233558655, -0.00950891338288784], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.8435202836990356, -1.490105023549404e-05, -4.815939246327616e-05, -0.0004138090298511088, -0.04443347826600075, -0.3212012052536011, -0.0009376181988045573, -0.0013393727131187916, -0.28944650292396545, -0.01604854315519333, -0.2512311637401581], "finish_reason": "stop"}, {"text": " pd.DataFrame(sum_words, columns=[\"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "sum", "_", "words", ",", " columns", "=[", "\"", "count", "\"", "])", "\n"], "logprobs": [-1.1403254270553589, -3.71926071238704e-05, -0.4268360733985901, -0.12500682473182678, -2.7996256351470947, -0.00024005869636312127, -2.6225699912174605e-05, -0.3667917549610138, -1.1833016872406006, -0.019289059564471245, -0.0011188682401552796, -1.9402724504470825, -0.013023554347455502, -0.4225118160247803, -0.13387350738048553], "finish_reason": "stop"}, {"text": "word, sum_words[i])", "tokens": ["word", ",", " sum", "_", "words", "[", "i", "])", "\n"], "logprobs": [-0.13607509434223175, -0.05865137651562691, -1.1713768243789673, -0.022703830152750015, -0.014722160995006561, -0.08328726887702942, -0.49438726902008057, -0.09127431362867355, -0.7559477090835571], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: -x[1])", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " -", "x", "[", "1", "])", "\n"], "logprobs": [-0.0832751989364624, -0.00028546550311148167, -0.08709226548671722, -0.0007538337376900017, -6.067568756407127e-05, -0.0018897071713581681, -0.015484936535358429, -0.0011288706446066499, -0.0013003236381337047, -0.0573728084564209, -0.0003332536434754729, -1.1701152324676514, -0.007137515116482973, -0.013039321638643742, -0.0003713871701620519, -0.0022693139035254717, -0.14770960807800293], "finish_reason": "stop"}, {"text": " [", "tokens": [" [", "\n"], "logprobs": [-1.2860298156738281, -1.8095455169677734], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.0073888832703232765, -0.0015774437924847007, -0.0018322361866012216, -0.06013476103544235, -0.3836904466152191, -0.00013457823661156, -9.65590606938349e-06, -0.05340172350406647, -0.007136686705052853, -0.017695410177111626, -0.0010039533954113722, -0.2390797734260559, -0.0022754985839128494, -0.00010334911348763853, -0.0007291757501661777, -0.5900769829750061, -0.006958655081689358, -0.0021418030373752117, -0.030673395842313766], "finish_reason": "stop"}, {"text": " df.hplot(xlabel=\"Words\", ylabel=\"Count\")", "tokens": [" df", ".", "h", "plot", "(", "xlabel", "=", "\"", "Words", "\"", ",", " y", "label", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.07692907005548477, -0.034356243908405304, -2.7019379138946533, -1.9853489398956299, -0.17187535762786865, -1.2790671586990356, -0.03346467390656471, -0.02054551988840103, -2.7448930740356445, -0.015727175399661064, -0.006855775136500597, -0.2060098946094513, -0.001122797722928226, -0.003975703381001949, -0.00031883400515653193, -0.2937105894088745, -0.4207299053668976, -0.8003233075141907, -0.08609679341316223], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.009990568272769451, -2.2649508537142538e-05, -2.264974000354414e-06, -0.4615534245967865, -0.0025144172832369804], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=300)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "3", "0", "0", ")", "\n"], "logprobs": [-0.0020768519025295973, -9.536288416711614e-05, -9.154854342341423e-05, -0.003966560587286949, -2.7418097943154862e-06, -1.0967194612021558e-05, -2.2478318214416504, -1.827310562133789, -0.00013243274588603526, -0.00027581225731410086, -0.3104493319988251, -0.0023473347537219524, -0.00035315475543029606, -0.2885330617427826, -0.013724720105528831], "finish_reason": "stop"}, {"text": "clf()", "tokens": ["cl", "f", "()", "\n"], "logprobs": [-2.594902753829956, -0.017129335552453995, -0.0004433602443896234, -0.1819005012512207], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05850870534777641, -0.010315082967281342], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.02645161747932434], "finish_reason": "stop"}], "metrics": {"energy": 318.4956589908599, "energy_per_token": 1.6675165392191618, "time": 13.22707, "time_per_token": 0.06925167539267016}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].apply(str)  # Convert text values to strings", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "apply", "(", "str", ")", "  ", "#", " Convert", " text", " values", " to", " strings", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.800536870956421, -0.011861036531627178, -1.9939435720443726, -0.16352474689483643, -1.8285565376281738, -0.0008927173912525177, -0.7189781665802002, -3.009011745452881, -2.741100311279297, -0.07544398307800293, -0.3724534809589386, -0.11551697552204132], "finish_reason": "stop"}, {"text": " CountVectorizer(lowercase=True,", "tokens": [" Count", "Vector", "izer", "(", "low", "erc", "ase", "=", "True", ",", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -2.5649709701538086, -0.00028701478731818497, -5.972207145532593e-05, -0.0007078525959514081, -0.04806233569979668, -0.03318829461932182, -2.9261794090270996], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.07234232872724533, -1.1205610462639015e-05, -5.829164365422912e-05, -0.008356131613254547, -0.0031547797843813896, -0.0002703301142901182, -0.01030812133103609, -0.012856689281761646, -0.057394981384277344, -0.0003927174839191139, -8.368142152903602e-05, -3.957670196541585e-05, -0.07278767228126526, -0.6697254776954651], "finish_reason": "stop"}, {"text": " (AttributeError, TypeError, ValueError):", "tokens": [" (", "Attribute", "Error", ",", " Type", "Error", ",", " Value", "Error", "):", "\n"], "logprobs": [-2.709066152572632, -1.8277143239974976, -2.777537883957848e-05, -0.01802235282957554, -1.2839206457138062, -3.266281055402942e-05, -1.4394702911376953, -0.22507335245609283, -1.4185804502631072e-05, -0.2864075005054474, -0.06604785472154617], "finish_reason": "stop"}, {"text": "\"Input contains only stop words\")", "tokens": ["\"", "Input", " contains", " only", " stop", " words", "\"", ")", "\n"], "logprobs": [-0.05673933029174805, -1.1835527420043945, -2.6855812072753906, -0.10799209773540497, -0.01288869883865118, -0.13215038180351257, -1.1497406959533691, -0.0025648337323218584, -0.0016229089815169573], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.2605212926864624, -0.009277093224227428], "finish_reason": "stop"}, {"text": " np.sum(word_count.toarray())", "tokens": [" np", ".", "sum", "(", "word", "_", "count", ".", "to", "array", "())", "\n"], "logprobs": [-1.843805193901062, -8.844937838148326e-05, -0.5292588472366333, -0.003991376142948866, -0.041490182280540466, -2.4318398573086597e-05, -2.825220326485578e-05, -0.04867817088961601, -0.046760592609643936, -0.00114292127545923, -1.7013628482818604, -0.025278229266405106], "finish_reason": "stop"}, {"text": " sum_words / sum(sum_words)", "tokens": [" sum", "_", "words", " /", " sum", "(", "sum", "_", "words", ")", "\n"], "logprobs": [-2.540320873260498, -0.0024952725507318974, -2.52720492426306e-05, -2.840639352798462, -0.96953284740448, -2.1788699626922607, -0.03671988844871521, -0.00013517419574782252, -2.8609820219571702e-05, -0.07584253698587418, -0.40423449873924255], "finish_reason": "stop"}, {"text": "word, sum_words.get(word, 0) / word_count.shape[1])", "tokens": ["word", ",", " sum", "_", "words", ".", "get", "(", "word", ",", " ", "0", ")", " /", " word", "_", "count", ".", "shape", "[", "1", "])", "\n"], "logprobs": [-0.14137470722198486, -0.055707525461912155, -1.0622135400772095, -0.018268657848238945, -0.014087755233049393, -2.6745517253875732, -0.07119373977184296, -0.016690732911229134, -0.10557576268911362, -0.05149579793214798, -0.017149724066257477, -0.0009442876325920224, -2.1819632053375244, -0.17623485624790192, -1.1231474876403809, -0.0006824786541983485, -0.00017557987303007394, -0.0015688742278143764, -0.2102280557155609, -0.0027119070291519165, -2.2632699012756348, -0.013954454101622105, -0.22917447984218597], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.07625219970941544, -0.0003082277253270149, -0.08680769801139832, -0.000876638398040086, -4.625213477993384e-05, -0.0019767999183386564, -0.016920607537031174, -0.0010783816687762737, -0.0015509016811847687, -0.04678528010845184, -0.00027724236133508384, -0.4177740812301636, -0.008344192057847977, -0.0017860665684565902, -0.06724616885185242, -0.00014959646796341985, -0.000992997083812952, -0.00013636612857226282, -0.000198821333469823, -0.013730835169553757, -1.3309615850448608, -0.02786688692867756, -0.0064308843575417995, -0.00318211130797863, -0.007670591119676828], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5332655906677246, -0.0001225396408699453, -5.23315102327615e-05, -0.13369351625442505, -0.01638411357998848, -0.0072688874788582325, -0.0014143711887300014, -0.010739860124886036], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.007294923532754183, -0.0015073377871885896, -0.0017811877187341452, -0.048070404678583145, -0.3460250794887543, -0.0001573438785271719, -9.417489309271332e-06, -0.05932509899139404, -0.006209959741681814, -0.015690326690673828, -0.001077786204405129, -0.240565687417984, -0.002355540869757533, -7.378782902378589e-05, -0.0007071378640830517, -0.6898282766342163, -0.09146133810281754, -0.003328856313601136, -0.0027254601009190083, -0.03643084317445755], "finish_reason": "stop"}, {"text": " df.plot.barh(x=\"Word\", y=\"Count\", legend=False)", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " legend", "=", "False", ")", "\n"], "logprobs": [-0.07261352986097336, -0.03423483297228813, -0.186910942196846, -0.26976096630096436, -0.009216856211423874, -1.2553045749664307, -0.019105926156044006, -0.3021504580974579, -0.01955925114452839, -0.0007761088781990111, -0.009930020198225975, -3.0278701160568744e-05, -0.0029723532497882843, -0.0756293535232544, -0.00109112320933491, -0.0003426679759286344, -5.1973900554003194e-05, -1.9788545614574105e-05, -0.24637462198734283, -1.9364486932754517, -0.0015726829878985882, -0.12296941876411438, -0.6834845542907715, -0.01140137854963541], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.009940879419445992, -2.5033637939486653e-05, -3.099436753473128e-06, -0.416228324174881, -0.0025880197063088417], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0020591262727975845, -0.00012373158824630082, -7.748303323751315e-05, -0.0031739119440317154, -2.861018856492592e-06, -9.775113539944869e-06, -0.11392094194889069, -0.0034620368387550116], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.12433414906263351, -0.052222445607185364, -0.20959004759788513], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.053372226655483246, -0.008801002986729145], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.03119604103267193, -1.0453296899795532, -4.768370445162873e-07, -1.0728830375228426e-06, -7.748303323751315e-05, -0.00013016807497479022, -0.08672965317964554], "finish_reason": "stop"}], "metrics": {"energy": 362.8742128152848, "energy_per_token": 1.6127742791790436, "time": 14.911245, "time_per_token": 0.0662722}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.strip()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "strip", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -1.4731777906417847, -0.015683403238654137, -0.43511253595352173], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -0.3178824782371521, -0.017030777409672737], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"].values)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "]", ".", "values", ")", "\n"], "logprobs": [-0.06323696672916412, -9.059865078597795e-06, -5.364274329622276e-05, -0.005345338024199009, -0.003966085612773895, -0.0002489972102921456, -0.013667807914316654, -0.012809259817004204, -0.05914623290300369, -0.00033682872890494764, -9.190614218823612e-05, -3.814624506048858e-05, -2.844416379928589, -0.009721554815769196, -0.686048150062561, -0.1793893277645111, -0.5704096555709839], "finish_reason": "stop"}, {"text": " (ValueError, IndexError) as e:", "tokens": [" (", "Value", "Error", ",", " Index", "Error", ")", " as", " e", ":", "\n"], "logprobs": [-2.8255865573883057, -1.139678955078125, -4.339123915997334e-05, -0.016078924760222435, -2.3484046459198, -0.00011288482346571982, -2.361586093902588, -0.008138470351696014, -0.23052173852920532, -0.0011010067537426949, -0.01598401926457882], "finish_reason": "stop"}, {"text": "\"Input DataFrame is empty or contains only stop words. No histogram will be generated.\")", "tokens": ["\"", "Input", " Data", "Frame", " is", " empty", " or", " contains", " only", " stop", " words", ".", " No", " histogram", " will", " be", " generated", ".", "\"", ")", "\n"], "logprobs": [-0.04835723713040352, -1.2262017726898193, -3.1990914344787598, -0.062219128012657166, -0.07132106274366379, -0.032283563166856766, -0.06607072800397873, -0.05503463372588158, -0.02385195530951023, -0.018859826028347015, -0.0754382386803627, -0.7303773760795593, -3.051990270614624, -1.0095174312591553, -2.067106246948242, -0.0005421360838226974, -1.3405320644378662, -0.039049871265888214, -0.005992659833282232, -0.0008681819308549166, -0.00035589560866355896], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.23927591741085052, -0.00957881473004818], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.9408992528915405, -1.4066597032069694e-05, -5.98412734689191e-05, -0.0004409771354403347, -0.042875662446022034, -0.24389368295669556, -0.001192573574371636, -0.0014180614380165935, -0.29589447379112244, -0.011691974475979805, -0.2493634819984436], "finish_reason": "stop"}, {"text": " word_count.toarray().ravel().tolist()", "tokens": [" word", "_", "count", ".", "to", "array", "()", ".", "ravel", "()", ".", "tol", "ist", "()", "\n"], "logprobs": [-2.1576859951019287, -0.0018164102220907807, -1.7523612768854946e-05, -0.03538038954138756, -1.2423040866851807, -0.009110783226788044, -0.05444832891225815, -0.5236876606941223, -3.1729354858398438, -0.009480809792876244, -1.167288899421692, -0.10190442204475403, -0.002074115676805377, -0.014926533214747906, -0.023035895079374313], "finish_reason": "stop"}, {"text": "word, sum_words[i]) for word, i in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "i", "])", " for", " word", ",", " i", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.12103696912527084, -0.05656822770833969, -0.9784067273139954, -0.023273399099707603, -0.013940347358584404, -0.08995716273784637, -0.454980731010437, -0.10721685737371445, -0.5782281160354614, -0.1926153004169464, -0.0011688789818435907, -0.003521433798596263, -0.0026315366849303246, -0.3204907774925232, -1.0490362910786644e-05, -9.131014667218551e-05, -0.04748272895812988, -0.006020505912601948, -0.0002991705550812185, -0.00013350549852475524, -0.0008095800876617432, -3.6477376852417365e-05, -0.025724003091454506], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: -x[1])", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " -", "x", "[", "1", "])", "\n"], "logprobs": [-0.09202873706817627, -0.00038652101648040116, -0.08577703684568405, -0.0004981707315891981, -4.649054244509898e-05, -0.002240292262285948, -0.02125719003379345, -0.0009556017466820776, -0.001520311925560236, -0.05462431162595749, -0.00029702542815357447, -1.2225607633590698, -0.005551634822040796, -0.010850477032363415, -0.0003995097358711064, -0.0020728071685880423, -0.12770211696624756], "finish_reason": "stop"}, {"text": " [word for word, count in words_freq[:10]]", "tokens": [" [", "word", " for", " word", ",", " count", " in", " words", "_", "freq", "[:", "1", "0", "]]", "\n"], "logprobs": [-1.384770393371582, -0.5488038659095764, -0.20868830382823944, -0.08683283627033234, -0.007390303071588278, -2.2206015586853027, -0.0005488081369549036, -0.008640758693218231, -0.0008105330052785575, -3.123234637314454e-05, -0.32652366161346436, -0.017816854640841484, -0.008773942478001118, -0.018685976043343544, -0.007363323122262955], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"word\", \"count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "word", "\"", ",", " \"", "count", "\"", "])", "\n"], "logprobs": [-0.007169708609580994, -0.0015484021278098226, -0.0019606193527579308, -0.07329630106687546, -0.3696642220020294, -0.00010954733443213627, -1.1801649634435307e-05, -0.05031653493642807, -0.007040808908641338, -0.017312945798039436, -0.0011395872570574284, -2.0856547355651855, -0.004044567700475454, -7.295342220459133e-05, -0.0008465044084005058, -0.38773584365844727, -0.0029496517963707447, -0.0021005249582231045, -0.04501758888363838], "finish_reason": "stop"}, {"text": " df.plot.barh(", "tokens": [" df", ".", "plot", ".", "bar", "h", "(", "\n"], "logprobs": [-0.09122578054666519, -0.040844615548849106, -0.1822935789823532, -0.2628844082355499, -0.011550811119377613, -1.2584820985794067, -0.01415110845118761, -3.221921443939209], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.011389239691197872, -2.3245540432981215e-05, -2.622600959512056e-06, -0.4458046555519104, -0.002016893122345209], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0019062458304688334, -0.00010513706365600228, -7.128461584215984e-05, -0.004711003974080086, -2.622600959512056e-06, -9.417489309271332e-06, -0.11698184162378311, -0.004341700114309788], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.11668051034212112, -0.066192127764225, -0.20283935964107513], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.049074817448854446, -0.010343752801418304], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|", ">\""], "logprobs": [-0.018837833777070045, -0.9663284420967102, -4.768370445162873e-07, -1.311301275563892e-06, -7.64102369430475e-05, -0.00012838016846217215, -2.6001293659210205, -0.002293933881446719], "finish_reason": "stop"}], "metrics": {"energy": 351.42867256259905, "energy_per_token": 1.568878002511603, "time": 14.707421, "time_per_token": 0.06565812946428572}}
{"outputs": [{"text": " pd.read_csv(file_path,", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -1.1519023180007935, -4.157805442810059], "finish_reason": "stop"}, {"text": " df[\"Text\"].map(lambda x: x.strip().replace(\"\\n\", \"\").lower())", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "map", "(", "lambda", " x", ":", " x", ".", "strip", "()", ".", "replace", "(", "\"", "\\", "n", "\"", ",", " \"", "\"", ")", ".", "lower", "())", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -3.2994840145111084, -0.007461536210030317, -0.42013150453567505, -0.19585835933685303, -0.0008662762120366096, -0.3727014362812042, -0.05075899139046669, -0.9529094099998474, -2.1448004245758057, -0.4628533720970154, -2.1893489360809326, -0.17089301347732544, -0.028745390474796295, -0.3987225294113159, -0.087419793009758, -0.009739735163748264, -0.003706135554239154, -0.026344802230596542, -1.9227238893508911, -0.8427056074142456, -0.07105251401662827, -0.6365969181060791, -0.050204191356897354, -0.09002623707056046], "finish_reason": "stop"}, {"text": " CountVectorizer(", "tokens": [" Count", "Vector", "izer", "(", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -2.2581491470336914], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", "\n"], "logprobs": [-0.07234232872724533, -1.1205610462639015e-05, -5.829164365422912e-05, -0.008356131613254547, -0.0031547797843813896, -0.0002703301142901182, -0.01030812133103609, -0.012856689281761646, -0.057394981384277344, -0.0003927174839191139, -8.368142152903602e-05, -3.957670196541585e-05, -0.07278767228126526, -0.7321472764015198, -0.33158549666404724, -0.0038466285914182663, -0.16205954551696777, -0.3726591467857361], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-1.6416051387786865, -2.038458114839159e-05, -0.4343394935131073, -0.06158546358346939], "finish_reason": "stop"}, {"text": "\"Input contains only stop words.\")", "tokens": ["\"", "Input", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.0495629757642746, -1.3125498294830322, -2.6448798179626465, -0.1118212565779686, -0.011180724948644638, -0.14611484110355377, -0.6837932467460632, -0.22585193812847137, -0.0013100859941914678, -0.0011082704877480865], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.24584877490997314, -0.00785074383020401], "finish_reason": "stop"}, {"text": " pd.DataFrame(", "tokens": [" pd", ".", "DataFrame", "(", "\n"], "logprobs": [-2.4449381828308105, -1.6927575416048057e-05, -1.6411845684051514, -0.10680587589740753, -1.2883599996566772], "finish_reason": "stop"}, {"text": " (", "tokens": [" (", "\n"], "logprobs": [-1.6122909784317017, -3.3544397354125977], "finish_reason": "stop"}, {"text": "word, sum_words[i]) for word, i in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "i", "])", " for", " word", ",", " i", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.13471192121505737, -0.06102575734257698, -1.113706350326538, -0.013599603436887264, -0.015115363523364067, -0.07751943916082382, -0.5175025463104248, -0.09171652793884277, -0.6525918841362, -0.17937219142913818, -0.0007253637886606157, -0.0032835875172168016, -0.0021023093722760677, -0.31777918338775635, -7.152531907195225e-06, -8.05822346592322e-05, -0.04203829541802406, -0.006759397219866514, -0.0002557904226705432, -9.870042413240299e-05, -0.0003924791526515037, -4.494089080253616e-05, -0.029489677399396896], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.0834856629371643, -0.0002575780963525176, -0.07830029726028442, -0.000747758662328124, -4.6967357775429264e-05, -0.0019286142196506262, -0.01623317413032055, -0.0008978387922979891, -0.0013724679592996836, -0.04943455010652542, -0.00031287543242797256, -0.39932090044021606, -0.008353767916560173, -0.001749176881276071, -0.05971361696720123, -0.00012635385792236775, -0.0009565545478835702, -0.0001357701694360003, -0.00019691436318680644, -0.012015936896204948, -0.273794561624527], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5665982961654663, -0.00010287232726113871, -5.864924969500862e-05, -0.15176978707313538, -0.016339432448148727, -0.006658036261796951, -0.0016102931695058942, -0.012872576713562012], "finish_reason": "stop"}, {"text": " pd.DataFrame(words_freq, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "words", "_", "freq", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0069470531307160854, -0.0016137446509674191, -0.001700980355963111, -0.05740668997168541, -2.597771406173706, -0.011798598803579807, -0.0002859421947505325, -0.06642305850982666, -0.00908443983644247, -0.019112475216388702, -0.0016066036187112331, -0.2812466621398926, -0.0018755479250103235, -0.00012146688823122531, -0.0006898645660839975, -0.8634354472160339, -0.10838591307401657, -0.003522621700540185, -0.0018593657296150923, -0.03378027677536011], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=90, figsize=(8, 5), fontsize=8)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "9", "0", ",", " fig", "size", "=(", "8", ",", " ", "5", ")", ",", " fonts", "ize", "=", "8", ")", "\n"], "logprobs": [-0.09076963365077972, -0.03377370908856392, -0.13682974874973297, -0.28074318170547485, -0.009839249774813652, -0.39389264583587646, -0.419391393661499, -0.05470580235123634, -0.0007725353352725506, -0.0012216256000101566, -4.100715523236431e-05, -0.004176110029220581, -0.06494339555501938, -0.0017006233101710677, -0.00018082413589581847, -4.660974445869215e-05, -2.312633478140924e-05, -0.2969237267971039, -0.6034201979637146, -0.004108204040676355, -1.6742026805877686, -0.00030357998912222683, -0.9351998567581177, -0.6195553541183472, -0.0009913297835737467, -0.027891000732779503, -2.261470317840576, -0.011441567912697792, -0.016248656436800957, -1.6557819843292236, -1.5096172094345093, -0.0003486264031380415, -1.9346164464950562, -0.00018249277491122484, -0.006572297774255276, -1.311636209487915, -0.28573641180992126, -0.0051773046143352985], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.011677482165396214, -2.4437606043647975e-05, -2.50339189733495e-06, -0.49870648980140686, -0.002474462613463402], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0024402146227657795, -9.154854342341423e-05, -9.321732068201527e-05, -0.0032778840977698565, -2.0265558760002023e-06, -1.0251946150674485e-05, -0.11789418756961823, -0.003663140581920743], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.11736926436424255, -0.06725631654262543, -0.1962088644504547], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04854985326528549, -0.009599713608622551], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.025082821026444435], "finish_reason": "stop"}], "metrics": {"energy": 342.8744606552122, "energy_per_token": 1.5800666389641116, "time": 14.286891, "time_per_token": 0.0658382073732719}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -0.4749401807785034, -0.000929400441236794, -0.37533655762672424], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -0.3178824782371521, -0.017030777409672737], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).sum(axis=0)", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.06323696672916412, -9.059865078597795e-06, -5.364274329622276e-05, -0.005345338024199009, -0.003966085612773895, -0.0002489972102921456, -0.013667807914316654, -0.012809259817004204, -0.05914623290300369, -0.00033682872890494764, -9.190614218823612e-05, -3.814624506048858e-05, -0.062393974512815475, -0.8757088780403137, -1.8356127738952637, -0.003983302507549524, -0.004366862587630749, -0.0014829363208264112, -0.053029850125312805, -0.004360334947705269, -0.21135781705379486], "finish_reason": "stop"}, {"text": " pd.errors.EmptyDataError as e:", "tokens": [" pd", ".", "errors", ".", "Empty", "Data", "Error", " as", " e", ":", "\n"], "logprobs": [-3.4869444370269775, -2.253030106658116e-05, -0.09790623933076859, -0.0009239218779839575, -0.1893719583749771, -0.003730007680132985, -0.0020260538440197706, -2.0188863277435303, -0.262995183467865, -0.002946917898952961, -0.02316589467227459], "finish_reason": "stop"}, {"text": "\"Empty or invalid data\")", "tokens": ["\"", "Empty", " or", " invalid", " data", "\"", ")", "\n"], "logprobs": [-0.04835723713040352, -1.9338338375091553, -0.6974937319755554, -2.1556084156036377, -0.9480891823768616, -1.4637360572814941, -0.004084103740751743, -0.001972635742276907], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.23927591741085052, -0.00957881473004818], "finish_reason": "stop"}, {"text": " sum(word_count.sum(axis=0))", "tokens": [" sum", "(", "word", "_", "count", ".", "sum", "(", "axis", "=", "0", "))", "\n"], "logprobs": [-1.3198970556259155, -0.07422202825546265, -0.187668114900589, -0.007142841350287199, -0.0008336406317539513, -0.012585620395839214, -0.4980323612689972, -0.0005747812101617455, -0.009511511772871017, -0.0006759266252629459, -0.31586548686027527, -0.3669654428958893, -0.03347424045205116], "finish_reason": "stop"}, {"text": " [word_count[i] / sum_words[i] for i in range(word_count.shape[1])]", "tokens": [" [", "word", "_", "count", "[", "i", "]", " /", " sum", "_", "words", "[", "i", "]", " for", " i", " in", " range", "(", "word", "_", "count", ".", "shape", "[", "1", "]", ")]", "\n"], "logprobs": [-3.2545344829559326, -2.435947895050049, -1.320281982421875, -0.01843181438744068, -0.38439449667930603, -0.5603928565979004, -0.23142510652542114, -0.6393676996231079, -0.03461260721087456, -0.004463827703148127, -0.00024637524620629847, -0.18621037900447845, -0.08313743025064468, -0.002072688192129135, -0.1292702555656433, -0.0028007111977785826, -0.024952838197350502, -0.006344652269035578, -0.00020895205670967698, -1.3909194469451904, -0.000720956246368587, -1.811964830267243e-05, -3.2782016205601394e-05, -0.0101502425968647, -0.0016182672698050737, -0.12095067650079727, -0.011380753479897976, -0.0006597249885089695, -0.00471646199002862], "finish_reason": "stop"}, {"text": "word, int(count / sum_words[0] * 100)) for (word, count) in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " int", "(", "count", " /", " sum", "_", "words", "[", "0", "]", " *", " ", "1", "0", "0", "))", " for", " (", "word", ",", " count", ")", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.1420639455318451, -0.05573638901114464, -2.6177122592926025, -0.013844414614140987, -0.40462982654571533, -1.2863523960113525, -0.04115629568696022, -0.009299888275563717, -0.0005052005290053785, -1.2336457967758179, -0.6302332878112793, -0.928261399269104, -0.045552607625722885, -0.0018835199298337102, -0.0007751559023745358, -0.0004886387032456696, -0.001830213237553835, -0.030020112171769142, -1.0291444063186646, -2.415896415710449, -0.004471541848033667, -4.60137271147687e-05, -0.0009204680100083351, -0.0014699617167934775, -0.016967490315437317, -1.4968225955963135, -0.0007314390386454761, -0.00024232311989180744, -0.020590832456946373, -0.0038752472028136253, -0.0004189328756183386, -0.0005000770906917751, -0.002198542468249798, -3.981510963058099e-05, -0.012444819323718548], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: -x[1])", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " -", "x", "[", "1", "])", "\n"], "logprobs": [-0.08767121285200119, -0.0002562671434134245, -0.080260269343853, -0.0007593132322654128, -5.94836674281396e-05, -0.002167734783142805, -0.019702328369021416, -0.0011214879341423512, -0.0014078239910304546, -0.0505158007144928, -0.0002960720448754728, -1.0245966911315918, -0.005281898658722639, -0.013252507895231247, -0.0004450284468475729, -0.0020368797704577446, -0.14520758390426636], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5197003483772278, -8.964136941358447e-05, -5.090107151772827e-05, -0.15905922651290894, -0.01721663400530815, -0.007010860368609428, -0.0012705596163868904, -0.01090483833104372], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.007763214409351349, -0.0014649622607976198, -0.0016183863626793027, -0.07175210863351822, -0.35963568091392517, -0.0001658063702052459, -8.4638240878121e-06, -0.05425526574254036, -0.006327831652015448, -0.014988541603088379, -0.001050516264513135, -0.22411362826824188, -0.0023400799836963415, -0.00010656742961145937, -0.0007949291029945016, -0.7689283490180969, -0.006968125235289335, -0.0017701209289953113, -0.03632095083594322], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.07610826194286346, -0.035571034997701645, -0.17408126592636108, -0.2812660038471222, -0.01011330634355545, -0.4306226670742035, -0.3784189820289612, -0.05190185829997063, -0.000849482137709856, -0.0010658780811354518, -3.862306402879767e-05, -0.004382648505270481, -0.049987852573394775, -0.0015445933677256107, -0.00023803261865396053, -5.173549288883805e-05, -1.9788545614574105e-05, -0.2880743443965912, -0.518021821975708, -0.0037408152129501104, -0.4320114254951477, -0.7064768671989441, -0.014401894994080067], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.01065848022699356, -2.1934269170742482e-05, -2.3841830625315197e-06, -0.56087726354599, -0.0022573007736355066], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0023749261163175106, -8.95221673999913e-05, -7.915183232398704e-05, -0.003959198947995901, -2.264974000354414e-06, -9.775113539944869e-06, -0.10964932292699814, -0.0040708063170313835], "finish_reason": "stop"}, {"text": "close()  # Clean up the plot resources", "tokens": ["close", "()", "  ", "#", " Clean", " up", " the", " plot", " resources", "\n"], "logprobs": [-0.10516238212585449, -0.05724526569247246, -1.7690677642822266, -0.0007383481133729219, -1.5981545448303223, -0.2750852108001709, -1.0322871208190918, -0.2828485369682312, -2.5720438957214355, -0.11732654273509979], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.04622041434049606, -0.009146574884653091], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.02097729779779911], "finish_reason": "stop"}], "metrics": {"energy": 364.756822769165, "energy_per_token": 1.4532144333432868, "time": 15.557437, "time_per_token": 0.06198182071713147}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].apply(lambda x: \" \".join([word for word in x.split() if word.lower() not in STOP_WORDS]))", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "apply", "(", "lambda", " x", ":", " \"", " \"", ".", "join", "([", "word", " for", " word", " in", " x", ".", "split", "()", " if", " word", ".", "lower", "()", " not", " in", " ST", "OP", "_", "W", "OR", "DS", "]))", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -1.800536870956421, -0.011861036531627178, -0.26765576004981995, -0.13337089121341705, -0.000940595637075603, -1.7826931476593018, -0.05633523315191269, -0.00311211752705276, -0.00019965562387369573, -0.5633833408355713, -0.6740792393684387, -0.08753764629364014, -0.0004742213059216738, -0.0009149893885478377, -0.09244736284017563, -0.0024839758407324553, -0.23734167218208313, -0.1273595094680786, -0.08062303066253662, -0.03743242844939232, -1.3959153890609741, -0.0306394062936306, -0.0016946729738265276, -0.024312326684594154, -0.0006926044588908553, -0.007728319149464369, -8.344646857949556e-07, -4.458328112377785e-05, -1.0251946150674485e-05, -1.4305104514278355e-06, -3.516612196108326e-05, -0.042261503636837006, -0.03776480630040169], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -0.3859751522541046, -0.012796196155250072], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.08213822543621063, -1.0490362910786644e-05, -5.185469490243122e-05, -0.007920405827462673, -0.0035200081765651703, -0.00023398046323563904, -0.009247093461453915, -0.012697681784629822, -0.06290388107299805, -0.0004122599493712187, -9.48860906646587e-05, -2.992108420585282e-05, -0.0737694650888443, -0.5549620985984802], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-1.0263607501983643, -0.02688584290444851], "finish_reason": "stop"}, {"text": "\"No valid words found.\")", "tokens": ["\"", "No", " valid", " words", " found", ".", "\"", ")", "\n"], "logprobs": [-0.05799452215433121, -1.4861711263656616, -0.3790932893753052, -0.2893163859844208, -0.06845385581254959, -2.0990757942199707, -0.20598545670509338, -0.0014781750505790114, -0.001347229932434857], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.2613685131072998, -0.010330538265407085], "finish_reason": "stop"}, {"text": " vectorizer.get_feature_names_out().size", "tokens": [" vector", "izer", ".", "get", "_", "feature", "_", "names", "_", "out", "()", ".", "size", "\n"], "logprobs": [-3.5014374256134033, -0.0003651905863080174, -9.464769391342998e-05, -1.2816239595413208, -4.6491513785440475e-06, -0.0005869334563612938, -1.1920928244535389e-07, -0.007078687660396099, -0.5854032635688782, -9.369411418447271e-05, -0.05734534189105034, -0.4936884045600891, -0.7861086130142212, -0.04229201376438141], "finish_reason": "stop"}, {"text": " pd.Series(sum_words).sort_values(ascending=False).index", "tokens": [" pd", ".", "Series", "(", "sum", "_", "words", ")", ".", "sort", "_", "values", "(", "asc", "ending", "=", "False", ")", ".", "index", "\n"], "logprobs": [-1.2874846458435059, -5.018585216021165e-05, -0.8885306119918823, -0.1618437021970749, -1.6900237798690796, -0.004172192420810461, -2.062299427052494e-05, -1.2423510551452637, -0.38174596428871155, -0.18716712296009064, -0.0007295330869965255, -0.03183978050947189, -0.12476524710655212, -0.04137752577662468, -2.0265558760002023e-06, -0.000782183778937906, -0.021690409630537033, -0.0425543487071991, -1.2771549224853516, -1.9241631031036377, -0.35504215955734253], "finish_reason": "stop"}, {"text": "word, sum_words[i])", "tokens": ["word", ",", " sum", "_", "words", "[", "i", "])", "\n"], "logprobs": [-0.1346198171377182, -0.06383066624403, -1.1291019916534424, -0.01864689588546753, -0.014881551265716553, -0.10638753324747086, -0.5429276823997498, -0.09854496270418167, -0.8321434259414673], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1])", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "])", "\n"], "logprobs": [-0.08951711654663086, -0.0003492222458589822, -0.1064489483833313, -0.0008197046699933708, -6.01988795096986e-05, -0.0019351581577211618, -0.015318598598241806, -0.0012007892364636064, -0.0014965059235692024, -0.05607321858406067, -0.00033361115492880344, -0.40858227014541626, -0.010160272940993309, -0.0017339446349069476, -2.8056182861328125, -0.4900229275226593], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.49768197536468506, -0.00010787858627736568, -6.0437283536884934e-05, -0.1380947083234787, -0.01622285135090351, -0.006576087325811386, -0.0015442363219335675, -0.01148528978228569], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.007553114090114832, -0.0014043716946616769, -0.0018742390675470233, -0.0688241496682167, -0.3071845471858978, -0.0001597276859683916, -1.0609570381348021e-05, -0.051625773310661316, -0.006435503717511892, -0.016069892793893814, -0.001347468001767993, -0.22886303067207336, -0.002329376293346286, -9.095255518332124e-05, -0.0006492413813248277, -0.7454032897949219, -0.08587244153022766, -0.0041175829246640205, -0.0019884591456502676, -0.03369014710187912], "finish_reason": "stop"}, {"text": " df.plot(kind=\"barh\", x=\"Word\", y=\"Count\", figsize=(10, 5))", "tokens": [" df", ".", "plot", "(", "kind", "=", "\"", "bar", "h", "\"", ",", " x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " fig", "size", "=(", "1", "0", ",", " ", "5", "))", "\n"], "logprobs": [-0.09696584194898605, -0.03507703170180321, -0.16287274658679962, -1.226511836051941, -0.5666385293006897, -0.08152090758085251, -9.333651541965082e-05, -0.015938028693199158, -0.16956545412540436, -5.900685573578812e-05, -0.028667664155364037, -0.6753419041633606, -0.008012651465833187, -0.0016656348016113043, -0.05303855612874031, -5.61460001335945e-05, -0.00399458222091198, -0.046861838549375534, -0.0005113962688483298, -0.0005066303419880569, -5.006664650863968e-05, -3.1470757676288486e-05, -0.5238708257675171, -0.7371928691864014, -0.0003946240758523345, -0.022171948105096817, -0.4163696765899658, -0.7857781052589417, -0.004468574654310942, -0.012008633464574814, -0.8002658486366272, -0.4382545053958893, -0.003898046677932143], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.014388264156877995, -1.9788545614574105e-05, -2.50339189733495e-06, -0.9684527516365051, -0.0415881872177124, -1.0371154530730564e-05, -0.010785028338432312, -0.0015904169995337725], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.00261869584210217, -0.00012242044613230973, -8.916457591112703e-05, -0.0045225718058645725, -2.145764938177308e-06, -1.156323378381785e-05, -0.11678510159254074, -0.003509911010041833], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.11474117636680603, -0.05361529067158699, -0.1687782257795334], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.051236592233181, -0.009865455329418182], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.027946529909968376, -0.9647491574287415, -4.768370445162873e-07, -1.0728830375228426e-06, -6.317892984952778e-05, -0.00011789103882620111, -0.08914000540971756], "finish_reason": "stop"}], "metrics": {"energy": 367.5486813478472, "energy_per_token": 1.512546013777149, "time": 15.342236, "time_per_token": 0.06313677366255144}}
{"outputs": [{"text": " pd.read_csv(file_path, usecols=[0])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " use", "cols", "=[", "0", "])", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -1.1519023180007935, -1.7836906909942627, -0.0002790300059132278, -0.11544199287891388, -1.0510934591293335, -0.24293577671051025, -0.12297648191452026], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -0.4749401807785034, -0.000929400441236794, -0.37533655762672424], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -0.3178824782371521, -0.017030777409672737], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).sum(axis=0).tolist()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "sum", "(", "axis", "=", "0", ")", ".", "tol", "ist", "()", "\n"], "logprobs": [-0.06323696672916412, -9.059865078597795e-06, -5.364274329622276e-05, -0.005345338024199009, -0.003966085612773895, -0.0002489972102921456, -0.013667807914316654, -0.012809259817004204, -0.05914623290300369, -0.00033682872890494764, -9.190614218823612e-05, -3.814624506048858e-05, -0.062393974512815475, -0.8757088780403137, -1.8356127738952637, -0.003983302507549524, -0.004366862587630749, -0.0014829363208264112, -0.053029850125312805, -0.004360334947705269, -1.733433485031128, -0.9448782801628113, -1.5497195136049413e-06, -0.38121962547302246, -0.03957800939679146], "finish_reason": "stop"}, {"text": ":", "tokens": [":", "\n"], "logprobs": [-0.9912388324737549, -0.02996828407049179], "finish_reason": "stop"}, {"text": "\"Input is empty or contains only stop words\")", "tokens": ["\"", "Input", " is", " empty", " or", " contains", " only", " stop", " words", "\"", ")", "\n"], "logprobs": [-0.04835723713040352, -1.2262017726898193, -1.5111470222473145, -0.06649467349052429, -0.0625552386045456, -0.07128065824508667, -0.019115634262561798, -0.00994807854294777, -0.08946620672941208, -0.7044109106063843, -0.0017392997397109866, -0.0009715604246594012], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.23927591741085052, -0.00957881473004818], "finish_reason": "stop"}, {"text": " np.asarray(word_count.sum(axis=0), dtype=int).reshape(-1, 1)", "tokens": [" np", ".", "as", "array", "(", "word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", ",", " dtype", "=", "int", ")", ".", "resh", "ape", "(-", "1", ",", " ", "1", ")", "\n"], "logprobs": [-1.8653453588485718, -8.95221673999913e-05, -2.34195613861084, -0.019815348088741302, -0.008731046691536903, -0.04899014160037041, -1.5139465176616795e-05, -5.769562994828448e-05, -0.010161217302083969, -0.05594956874847412, -0.0008228015503846109, -0.002701207296922803, -0.001257224939763546, -0.1660485714673996, -2.484705924987793, -1.2234867811203003, -0.24472616612911224, -0.05551718547940254, -1.2981770038604736, -0.011379928328096867, -2.888866662979126, -0.9486412405967712, -5.125986263010418e-06, -0.31408774852752686, -2.47952248173533e-05, -1.6846427917480469, -0.20229282975196838, -0.005874274298548698, -0.008039141073822975, -0.025207092985510826], "finish_reason": "stop"}, {"text": " pd.DataFrame(word_count.todense(), columns=vectorizer.get_feature_names(), index=[0])", "tokens": [" pd", ".", "DataFrame", "(", "word", "_", "count", ".", "t", "od", "ense", "()", ",", " columns", "=", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ",", " index", "=[", "0", "])", "\n"], "logprobs": [-1.274407148361206, -3.6954195820726454e-05, -0.4812679886817932, -0.14849694073200226, -0.51754230260849, -7.438383181579411e-05, -5.483612312673358e-06, -0.009048763662576675, -1.76857590675354, -0.0001419681793777272, -0.0020149897318333387, -0.02383880317211151, -0.0256320983171463, -0.685431182384491, -0.029337575659155846, -0.013254625722765923, -6.556489552167477e-06, -4.351044481154531e-05, -0.02041074074804783, -1.311301275563892e-06, -0.0001255195093108341, -2.3841855067985307e-07, -0.0011756658786907792, -1.4487485885620117, -0.0015829188050702214, -0.5366583466529846, -0.022259972989559174, -0.9797405004501343, -0.003481400664895773, -0.18238428235054016], "finish_reason": "stop"}, {"text": "word, count) for word, count in zip(vectorizer.get_feature_names(), sum_words)", "tokens": ["word", ",", " count", ")", " for", " word", ",", " count", " in", " zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ",", " sum", "_", "words", ")", "\n"], "logprobs": [-0.1420639455318451, -0.05573638901114464, -0.7960759997367859, -0.23706699907779694, -0.5168136954307556, -0.19278283417224884, -0.0005436849314719439, -0.001634096377529204, -0.0018980359891429543, -0.10796181857585907, -0.0033492918591946363, -0.03838196396827698, -4.0531076592742465e-06, -6.317892984952778e-05, -0.03192199394106865, -3.099436753473128e-06, -0.00026603974401950836, -1.1920928244535389e-07, -0.0017226393101736903, -0.4210149049758911, -0.0008397151832468808, -0.0849265605211258, -0.0004233417857903987, -8.821448318485636e-06, -0.19748862087726593, -0.036838699132204056], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.08767121285200119, -0.0002562671434134245, -0.080260269343853, -0.0007593132322654128, -5.94836674281396e-05, -0.002167734783142805, -0.019702328369021416, -0.0011214879341423512, -0.0014078239910304546, -0.0505158007144928, -0.0002960720448754728, -0.460176020860672, -0.008214379660785198, -0.0017119289841502905, -0.06705544888973236, -0.00012385078298393637, -0.0009654866298660636, -0.00011598391574807465, -0.00019941726350225508, -0.011150192469358444, -0.2513067126274109], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.6198724508285522, -9.97731985989958e-05, -6.09140915912576e-05, -0.1521243155002594, -0.018710078671574593, -0.007227820809930563, -0.0014579391572624445, -0.011316517367959023], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.007366636767983437, -0.001819028053432703, -0.001791778369806707, -0.07390144467353821, -0.4796399772167206, -0.00016258825780823827, -1.0132738680113107e-05, -0.05256883054971695, -0.005793683230876923, -0.0185546912252903, -0.0009183242218568921, -0.22894695401191711, -0.002404776168987155, -0.00010311071673640981, -0.0006632988806813955, -0.664645254611969, -0.00831570103764534, -0.0017961811972782016, -0.03711004927754402], "finish_reason": "stop"}, {"text": " df.plot(x=\"Word\", y=\"Count\", kind=\"bar\", title=\"Histogram of Top 10 Words\")", "tokens": [" df", ".", "plot", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " kind", "=", "\"", "bar", "\"", ",", " title", "=", "\"", "H", "ist", "ogram", " of", " Top", " ", "1", "0", " Words", "\"", ")", "\n"], "logprobs": [-0.08288190513849258, -0.03965890407562256, -0.16605231165885925, -1.447238564491272, -1.50941002368927, -0.022565029561519623, -0.0009951406391337514, -0.002899967832490802, -4.255681051290594e-05, -0.000794214429333806, -0.044464148581027985, -0.001894704531878233, -0.00040236959466710687, -3.71926071238704e-05, -2.6940935640595853e-05, -0.05524006858468056, -0.1913391351699829, -0.01041973102837801, -0.00036638224264606833, -0.014426570385694504, -0.7658306956291199, -0.6591007113456726, -1.832716941833496, -0.007066258694976568, -0.018852105364203453, -1.4450089931488037, -0.005195567850023508, -0.0031104539521038532, -0.25998547673225403, -1.137894630432129, -0.3960552513599396, -0.0021258629858493805, -0.00032860602368600667, -0.09858320653438568, -0.12080880999565125, -0.28889739513397217, -0.0037719309329986572], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.014950139448046684, -2.3364747903542593e-05, -2.7418097943154862e-06, -0.49241557717323303, -0.0020393780432641506], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.002193189924582839, -0.00010013079008786008, -7.545663538621739e-05, -0.0037024538032710552, -2.622600959512056e-06, -1.168244216387393e-05, -0.11714397370815277, -0.004968676716089249], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.1254143863916397, -0.06432498246431351, -0.22374103963375092], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.043064314872026443, -0.008937709964811802], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "\n"], "logprobs": [-0.02307736687362194, -1.1573916673660278], "finish_reason": "stop"}], "metrics": {"energy": 392.25571271133424, "energy_per_token": 1.421216350403385, "time": 16.59973, "time_per_token": 0.060143949275362325}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower().str.replace(r'[^a-zA-Z\\s]+', '', regex=True).str.strip()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", ".", "str", ".", "replace", "(", "r", "'", "[", "^", "a", "-", "z", "A", "-", "Z", "\\", "s", "]+", "'", ",", " ''", ",", " regex", "=", "True", ")", ".", "str", ".", "strip", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -0.4749401807785034, -0.000929400441236794, -1.6249703168869019, -0.16733704507350922, -0.00031132620642893016, -1.432841181755066, -0.15515415370464325, -1.1672699451446533, -1.8356375694274902, -0.28638359904289246, -1.626244306564331, -0.17354907095432281, -0.000300600629998371, -0.0004267973708920181, -1.370247483253479, -7.915183232398704e-05, -0.0013335392577573657, -1.0009301900863647, -0.17671039700508118, -1.4758180379867554, -0.007487921975553036, -8.940297266235575e-05, -1.526350975036621, -0.00018535325943958014, -0.01072216872125864, -0.0011516136582940817, -0.0015455455286428332, -0.008429897017776966, -0.8883136510848999, -0.08570493757724762, -0.0005042473785579205, -0.18870095908641815, -0.006130225490778685, -0.23409365117549896], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -0.3178824782371521, -0.017030777409672737], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.06323696672916412, -9.059865078597795e-06, -5.364274329622276e-05, -0.005345338024199009, -0.003966085612773895, -0.0002489972102921456, -0.013667807914316654, -0.012809259817004204, -0.05914623290300369, -0.00033682872890494764, -9.190614218823612e-05, -3.814624506048858e-05, -0.062393974512815475, -0.5512097477912903], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-1.7364170551300049, -1.883488948806189e-05, -0.43747618794441223, -0.052275169640779495], "finish_reason": "stop"}, {"text": "\"Input is empty or contains only stopwords, or input is invalid.\")", "tokens": ["\"", "Input", " is", " empty", " or", " contains", " only", " stop", "words", ",", " or", " input", " is", " invalid", ".", "\"", ")", "\n"], "logprobs": [-0.0432562381029129, -1.3290461301803589, -1.3683263063430786, -0.06577107310295105, -0.06145363673567772, -0.06870874762535095, -0.01713191345334053, -0.009347483515739441, -2.4396207332611084, -3.0778543949127197, -3.0913453102111816, -3.5784921646118164, -1.337634801864624, -0.7606858611106873, -0.6707972288131714, -0.22113730013370514, -0.0022178117651492357, -0.00119138287845999], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.2448752075433731, -0.009157324209809303], "finish_reason": "stop"}, {"text": " vectorizer.get_feature_names()", "tokens": [" vector", "izer", ".", "get", "_", "feature", "_", "names", "()", "\n"], "logprobs": [-3.1420347690582275, -0.00036971885128878057, -0.00013028726971242577, -1.1673625707626343, -3.814689989667386e-06, -0.0005301024066284299, -1.1920928244535389e-07, -0.0062950183637440205, -0.7666831612586975, -0.038259778171777725], "finish_reason": "stop"}, {"text": " word_count.toarray()", "tokens": [" word", "_", "count", ".", "to", "array", "()", "\n"], "logprobs": [-2.2035186290740967, -0.0016828911611810327, -1.8000440832111053e-05, -0.02931835874915123, -1.234484314918518, -0.009362364187836647, -0.059733156114816666, -1.7465921640396118], "finish_reason": "stop"}, {"text": "word, count)", "tokens": ["word", ",", " count", ")", "\n"], "logprobs": [-0.15465742349624634, -0.0602666512131691, -0.876806914806366, -0.23095057904720306, -0.879770815372467], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: -x[1])", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " -", "x", "[", "1", "])", "\n"], "logprobs": [-0.0827140063047409, -0.0002563863235991448, -0.08391313999891281, -0.0011086276499554515, -6.05564855504781e-05, -0.0014550823252648115, -0.016290174797177315, -0.0008528171456418931, -0.0014035383937880397, -0.04498305544257164, -0.0003545847721397877, -1.232534646987915, -0.005435805767774582, -0.012324015609920025, -0.0004471732536330819, -0.0019200476817786694, -0.12094496935606003], "finish_reason": "stop"}, {"text": " words_freq[0:10]", "tokens": [" words", "_", "freq", "[", "0", ":", "1", "0", "]", "\n"], "logprobs": [-0.519126296043396, -0.000129691296024248, -5.9602869441732764e-05, -2.018531560897827, -0.02798907272517681, -0.024914003908634186, -0.033184949308633804, -0.008839641697704792, -0.0023606547620147467, -0.013238743878901005], "finish_reason": "stop"}, {"text": " pd.DataFrame(words_freq, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "words", "_", "freq", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.007274923380464315, -0.001328062848187983, -0.0019583588000386953, -0.05928701162338257, -2.494584321975708, -0.005328026134520769, -0.0004291805380489677, -0.06382854282855988, -0.010548785328865051, -0.021092509850859642, -0.0014081810368224978, -0.2586354911327362, -0.0021431115455925465, -0.00011836781777674332, -0.0007499027997255325, -0.6546857357025146, -0.0075143068097531796, -0.0015512587269768119, -0.059057798236608505], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", figsize=(20, 6), fontsize=16, rot=0, linewidth=2)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " fig", "size", "=(", "2", "0", ",", " ", "6", ")", ",", " fonts", "ize", "=", "1", "6", ",", " rot", "=", "0", ",", " linewidth", "=", "2", ")", "\n"], "logprobs": [-0.08379112929105759, -0.034952253103256226, -0.17036835849285126, -0.21925511956214905, -0.010801302269101143, -0.3813769221305847, -0.405365914106369, -0.05137214809656143, -0.0009398810216225684, -0.0008768765837885439, -4.1126360883936286e-05, -0.004196290858089924, -0.048271480947732925, -0.001375206047669053, -0.0002329078270122409, -4.8874615458771586e-05, -2.2172682292875834e-05, -0.3152220547199249, -1.5213522911071777, -0.00024577934527769685, -0.014204349368810654, -2.2558681964874268, -0.16167019307613373, -0.005304073449224234, -0.015678707510232925, -2.840686559677124, -0.7725793719291687, -0.00044467096449807286, -1.6654648780822754, -9.48860906646587e-05, -0.00414346344769001, -0.8022245764732361, -2.4543814659118652, -1.2933852672576904, -0.5032142996788025, -0.0008288762182928622, -0.6614409685134888, -2.042174816131592, -2.599982261657715, -0.001090289675630629, -1.4185538291931152, -0.88759446144104, -0.010974760167300701], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.01224559172987938, -2.3841574147809297e-05, -2.861018856492592e-06, -0.45891937613487244, -0.0022513538133352995], "finish_reason": "stop"}, {"text": "savefig(save_path, dpi=400)", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " d", "pi", "=", "4", "0", "0", ")", "\n"], "logprobs": [-0.0021642851643264294, -0.0001081169830285944, -8.070142939686775e-05, -0.004218133166432381, -2.9802276912960224e-06, -9.775113539944869e-06, -2.1284077167510986, -2.1899023056030273, -0.00011050090688513592, -0.0003352795320097357, -3.128061294555664, -0.005668399389833212, -0.001259725191630423, -0.26912960410118103, -0.010407933034002781], "finish_reason": "stop"}, {"text": "close(plt.gcf())", "tokens": ["close", "(", "plt", ".", "g", "cf", "())", "\n"], "logprobs": [-0.14781789481639862, -2.98738694190979, -2.0003323554992676, -0.000337305391440168, -0.009690860286355019, -0.009285124950110912, -0.0004764852055814117, -0.1532059609889984], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.0515139140188694, -0.012054216116666794], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.0199887678027153, -0.9947838187217712, -3.576278118089249e-07, -9.536738616588991e-07, -7.950943836476654e-05, -7.855583680793643e-05, -0.09124612808227539], "finish_reason": "stop"}], "metrics": {"energy": 374.7433486042021, "energy_per_token": 1.4581453253081793, "time": 15.770635, "time_per_token": 0.06136433852140078}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.strip()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "strip", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -1.4731777906417847, -0.015683403238654137, -0.43511253595352173], "finish_reason": "stop"}, {"text": " CountVectorizer(lowercase=True, stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "low", "erc", "ase", "=", "True", ",", " stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -2.75148344039917, -0.00029094755882397294, -6.258291978156194e-05, -0.0006681832019239664, -0.046971604228019714, -0.034041628241539, -0.12487109750509262, -8.177422569133341e-05, -5.8053239627042785e-05, -0.017899995669722557, -0.04929975047707558, -9.536738616588991e-07, -0.000324673397699371, -3.099436753473128e-06, -2.0265558760002023e-06, -2.753696753643453e-05, -0.18647772073745728, -0.014405302703380585], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.07804551720619202, -9.536697689327411e-06, -4.935142715112306e-05, -0.006372844334691763, -0.004252913873642683, -0.00022682955022901297, -0.013011316768825054, -0.010952357202768326, -0.06639862805604935, -0.0003363520372658968, -9.7508447652217e-05, -3.135155202471651e-05, -0.06988609582185745, -0.626802921295166], "finish_reason": "stop"}, {"text": " ValueError as e:", "tokens": [" Value", "Error", " as", " e", ":", "\n"], "logprobs": [-1.7538301944732666, -1.8954096958623268e-05, -1.080514907836914, -0.280314177274704, -0.001042775809764862, -0.022018495947122574], "finish_reason": "stop"}, {"text": "\"Input file is empty or contains only stop words\")", "tokens": ["\"", "Input", " file", " is", " empty", " or", " contains", " only", " stop", " words", "\"", ")", "\n"], "logprobs": [-0.04618000611662865, -1.2775486707687378, -1.0036412477493286, -0.07975263893604279, -0.03158963471651077, -0.033128559589385986, -0.02101932279765606, -0.014709005132317543, -0.006847369018942118, -0.07185906171798706, -1.0056498050689697, -0.001396991079673171, -0.0007294139941222966], "finish_reason": "stop"}, {"text": "", "tokens": ["\n"], "logprobs": [-1.5736907720565796], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.9936696290969849, -1.0490362910786644e-05, -4.994744449504651e-05, -0.00039200251922011375, -0.04695943370461464, -0.1926255226135254, -0.0010186012368649244, -0.0016191004542633891, -0.31260162591934204, -0.015082838013768196, -0.3203614354133606], "finish_reason": "stop"}, {"text": " dict(zip(vectorizer.get_feature_names(), sum_words))", "tokens": [" dict", "(", "zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ",", " sum", "_", "words", "))", "\n"], "logprobs": [-3.566410779953003, -0.07368464022874832, -0.08942238986492157, -0.039890773594379425, -0.04612957686185837, -3.933898824470816e-06, -5.328513361746445e-05, -0.11873815953731537, -2.50339189733495e-06, -0.00022003613412380219, -1.1920928244535389e-07, -0.0007952864980325103, -0.3014034926891327, -0.0012473430251702666, -0.434891939163208, -0.000814582861494273, -8.821448318485636e-06, -0.29622748494148254, -0.004742445424199104], "finish_reason": "stop"}, {"text": "word, count)", "tokens": ["word", ",", " count", ")", "\n"], "logprobs": [-0.1291790008544922, -0.06015283614397049, -0.8177633285522461, -0.2949879765510559, -0.8348427414894104], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)[:10]", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "[:", "1", "0", "]", "\n"], "logprobs": [-0.08344607800245285, -0.0002735478919930756, -0.07628500461578369, -0.0008111285860650241, -5.4834770708112046e-05, -0.002126219915226102, -0.01733778417110443, -0.0012168630491942167, -0.0012422234285622835, -0.051419708877801895, -0.0002693767019081861, -0.42818766832351685, -0.008655649609863758, -0.001924449927173555, -0.05657487362623215, -0.00011991735664196312, -0.0008557948167435825, -0.00014053787162993103, -0.00020966715237591416, -0.011559649370610714, -1.536075472831726, -0.0278038177639246, -0.007465085946023464, -0.0037163496017456055, -0.007410892751067877], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.6050708889961243, -0.00011908298620255664, -5.721882189391181e-05, -0.13248959183692932, -0.017237022519111633, -0.007494666147977114, -0.001511027687229216, -0.013182392343878746], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Words\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Words", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.007112422026693821, -0.0019723977893590927, -0.0017070496687665582, -0.06502516567707062, -0.35806137323379517, -0.00011073929636040702, -8.4638240878121e-06, -0.0563502199947834, -0.007273621391505003, -0.016391854733228683, -0.0010633773636072874, -3.1709585189819336, -0.004967016167938709, -0.0003449321957305074, -0.0006349454633891582, -0.5681133270263672, -0.11753547191619873, -0.006855538114905357, -0.0018225978128612041, -0.05023763328790665], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\")", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.09607230871915817, -0.03739407658576965, -0.17645612359046936, -0.3280467092990875, -0.01238865777850151, -0.3878268301486969, -0.38179928064346313, -0.05277782678604126, -0.0009314250783063471, -0.000780754373408854, -3.755022044060752e-05, -0.005508719012141228, -0.05649420991539955, -0.001894704531878233, -0.000270091783022508, -4.4225667807040736e-05, -2.2172682292875834e-05, -1.481633186340332, -0.08838394284248352], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.012677318416535854, -2.1934269170742482e-05, -2.9802276912960224e-06, -1.018293857574463, -0.04159104824066162, -1.0609570381348021e-05, -0.0103707704693079, -0.0015256681945174932], "finish_reason": "stop"}, {"text": "savefig(save_path, bbox_inches=\"tight\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ")", "\n"], "logprobs": [-0.0022897711023688316, -0.0001037067049765028, -8.082063141046092e-05, -0.0031442036852240562, -2.7418097943154862e-06, -8.821448318485636e-06, -2.2335245609283447, -0.22155652940273285, -0.00043847484630532563, -7.807903602952138e-05, -8.201262971851975e-05, -0.00351204932667315, -0.11792459338903427, -0.0008354272576980293, -0.0032607740722596645, -2.932505594799295e-05, -0.2351049929857254, -0.02511211857199669], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.12704689800739288, -0.058464743196964264, -0.16676127910614014], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05345144867897034, -0.009758269414305687], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.020707137882709503], "finish_reason": "stop"}], "metrics": {"energy": 362.60687539482143, "energy_per_token": 1.663334290801933, "time": 13.886706, "time_per_token": 0.06370048623853211}}
{"outputs": [{"text": " pd.read_csv(file_path, dtype=str, encoding='utf-8')", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " dtype", "=", "str", ",", " encoding", "='", "utf", "-", "8", "')", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -1.1519023180007935, -2.4658501148223877, -0.4353579878807068, -0.17277057468891144, -1.1628488302230835, -1.4364947080612183, -0.4390019476413727, -0.1649407148361206, -0.11559174209833145, -0.0009369035833515227, -0.5065102577209473, -0.07828023284673691], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower()", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -0.4749401807785034, -0.000929400441236794, -0.37533655762672424], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -0.32020142674446106, -0.0006420934805646539, -8.201262971851975e-05, -0.027813788503408432, -0.05215002968907356, -4.768370445162873e-07, -0.0003570872650016099, -5.006777428206988e-06, -2.264974000354414e-06, -2.2172682292875834e-05, -0.3178824782371521, -0.017030777409672737], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.06323696672916412, -9.059865078597795e-06, -5.364274329622276e-05, -0.005345338024199009, -0.003966085612773895, -0.0002489972102921456, -0.013667807914316654, -0.012809259817004204, -0.05914623290300369, -0.00033682872890494764, -9.190614218823612e-05, -3.814624506048858e-05, -0.062393974512815475, -0.5512097477912903], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-1.7364170551300049, -1.883488948806189e-05, -0.43747618794441223, -0.052275169640779495], "finish_reason": "stop"}, {"text": "\"No valid words found.\")", "tokens": ["\"", "No", " valid", " words", " found", ".", "\"", ")", "\n"], "logprobs": [-0.0432562381029129, -1.3518446683883667, -0.3869962692260742, -0.2849676012992859, -0.06432610005140305, -2.0540685653686523, -0.19378124177455902, -0.0014424644177779555, -0.0011106519959867], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.28625646233558655, -0.00950891338288784], "finish_reason": "stop"}, {"text": " pd.Series(word_count.sum(axis=1), index=vectorizer.get_feature_names())", "tokens": [" pd", ".", "Series", "(", "word", "_", "count", ".", "sum", "(", "axis", "=", "1", ")", ",", " index", "=", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-2.607903480529785, -1.490105023549404e-05, -0.18379999697208405, -0.010115901939570904, -0.4075694680213928, -5.8410845667822286e-05, -7.986703712958843e-05, -0.0005013877525925636, -0.08122134208679199, -0.00031323294388130307, -0.001977394800633192, -0.0008737798780202866, -1.5397584438323975, -0.3527626097202301, -0.6239444017410278, -0.019959555938839912, -0.0016285026213154197, -0.04172951728105545, -1.2278481335670222e-05, -4.589452510117553e-05, -0.21226680278778076, -1.4305104514278355e-06, -0.0002609150833450258, -1.1920928244535389e-07, -0.0018941095331683755, -0.0617210827767849, -0.0959160327911377], "finish_reason": "stop"}, {"text": " pd.DataFrame(data=word_count.toarray(), columns=vectorizer.get_feature_names())", "tokens": [" pd", ".", "DataFrame", "(", "data", "=", "word", "_", "count", ".", "to", "array", "()", ",", " columns", "=", "vector", "izer", ".", "get", "_", "feature", "_", "names", "())", "\n"], "logprobs": [-1.2339156866073608, -3.886147169396281e-05, -0.45911192893981934, -0.11296024173498154, -3.121626138687134, -0.19088467955589294, -0.5101895332336426, -0.00013076403411105275, -5.8412379075889476e-06, -0.03872308135032654, -0.5197548270225525, -0.0035476861521601677, -0.021424759179353714, -0.06525636464357376, -0.9599171280860901, -0.02378479205071926, -0.013601131737232208, -8.22540732769994e-06, -4.7444173105759546e-05, -0.026346776634454727, -1.311301275563892e-06, -0.00011073929636040702, -1.1920928244535389e-07, -0.001341158407740295, -0.22654828429222107, -0.24160291254520416], "finish_reason": "stop"}, {"text": "word, sum_words[i]) for word, i in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "i", "])", " for", " word", ",", " i", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.13789990544319153, -0.06578156352043152, -1.204970121383667, -0.024987371638417244, -0.016728952527046204, -0.1074361577630043, -0.5166702270507812, -0.09916190057992935, -0.6114006042480469, -0.21002167463302612, -0.0009631047723814845, -0.0030820509418845177, -0.0023487620055675507, -0.2984035909175873, -7.271740287251305e-06, -7.629103492945433e-05, -0.06771685183048248, -0.0075092194601893425, -0.00029416524921543896, -0.00010311071673640981, -0.0004893536097370088, -3.6954195820726454e-05, -0.027946529909968376], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: -x[1])", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " -", "x", "[", "1", "])", "\n"], "logprobs": [-0.08368588238954544, -0.00029130507027730346, -0.09293946623802185, -0.0006837890832684934, -6.532455881824717e-05, -0.00168658047914505, -0.016735047101974487, -0.0010372978867962956, -0.0011812621960416436, -0.04942717403173447, -0.00032062159152701497, -1.041337013244629, -0.004093007650226355, -0.012235582806169987, -0.0004182179400231689, -0.0017836865736171603, -0.1276392638683319], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5737467408180237, -0.00010847456724150106, -5.936446541454643e-05, -0.14164914190769196, -0.017772004008293152, -0.006838962901383638, -0.001280084135942161, -0.010511391796171665], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.006907986011356115, -0.0016888416139408946, -0.0019359909929335117, -0.06685744971036911, -0.31840750575065613, -0.0001294529065489769, -1.1444026313256472e-05, -0.05076103284955025, -0.006824163254350424, -0.016835860908031464, -0.0011588770430535078, -0.23112212121486664, -0.002174871740862727, -8.725739462533966e-05, -0.0007447806419804692, -0.6287441849708557, -0.10061123222112656, -0.004088971298187971, -0.0025239300448447466, -0.03600764274597168], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\")", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ")", "\n"], "logprobs": [-0.08005782216787338, -0.03239990025758743, -0.14837588369846344, -0.27755501866340637, -0.009928721934556961, -0.4079365134239197, -0.4206940531730652, -0.06676757335662842, -0.0008942657150328159, -0.0011616155970841646, -3.2782016205601394e-05, -0.0054039116948843, -0.056410837918519974, -0.001522930571809411, -0.0003023882454726845, -4.5536911784438416e-05, -2.2649508537142538e-05, -1.396252155303955, -0.08390568941831589], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.008935228921473026, -2.3841574147809297e-05, -3.099436753473128e-06, -0.41036102175712585, -0.0020803017541766167], "finish_reason": "stop"}, {"text": "savefig(save_path, bbox_inches=\"tight\")", "tokens": ["save", "fig", "(", "save", "_", "path", ",", " b", "box", "_", "in", "ches", "=", "\"", "tight", "\"", ")", "\n"], "logprobs": [-0.002029741881415248, -9.48860906646587e-05, -9.83428253675811e-05, -0.003373766550794244, -2.622600959512056e-06, -1.1324817933200393e-05, -2.2435505390167236, -0.23763826489448547, -0.0004070168943144381, -8.940297266235575e-05, -9.655486064730212e-05, -0.00513733783736825, -0.10214651376008987, -0.0009613183210603893, -0.0028194934129714966, -3.135155202471651e-05, -0.19779805839061737, -0.02190222591161728], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.11409367620944977, -0.05349992588162422, -0.17498993873596191], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.06303182244300842, -0.009597588330507278], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.022134052589535713, -1.1270591020584106, -4.768370445162873e-07, -1.1920922133867862e-06, -6.675497570540756e-05, -0.00014995403762441128, -0.089520163834095], "finish_reason": "stop"}], "metrics": {"energy": 356.3869349832538, "energy_per_token": 1.408644011791517, "time": 15.330842, "time_per_token": 0.06059621343873518}}
{"outputs": [{"text": " pd.read_csv(file_path, names=['text'])", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " names", "=['", "text", "'])", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -1.1519023180007935, -3.7281391620635986, -1.391223430633545, -1.766718864440918, -0.11556976288557053, -0.039094697684049606], "finish_reason": "stop"}, {"text": " df[\"Text\"].map(lambda x: x.lower())", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "map", "(", "lambda", " x", ":", " x", ".", "lower", "())", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -3.2994840145111084, -0.007461536210030317, -0.42013150453567505, -0.19585835933685303, -0.0008662762120366096, -0.3727014362812042, -0.05075899139046669, -0.8395366072654724, -0.17723862826824188, -0.2805418372154236], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -0.3859751522541046, -0.012796196155250072], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"]).toarray()", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", ".", "to", "array", "()", "\n"], "logprobs": [-0.08213822543621063, -1.0490362910786644e-05, -5.185469490243122e-05, -0.007920405827462673, -0.0035200081765651703, -0.00023398046323563904, -0.009247093461453915, -0.012697681784629822, -0.06290388107299805, -0.0004122599493712187, -9.48860906646587e-05, -2.992108420585282e-05, -0.0737694650888443, -0.8698386549949646, -0.33098796010017395, -0.0031708222813904285, -0.131098210811615, -0.33565887808799744], "finish_reason": "stop"}, {"text": " ValueError:", "tokens": [" Value", "Error", ":", "\n"], "logprobs": [-1.7627403736114502, -1.9550132492440753e-05, -0.4905904233455658, -0.05164230242371559], "finish_reason": "stop"}, {"text": "\"Input is empty or contains only stop words.\")", "tokens": ["\"", "Input", " is", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.0501292459666729, -1.3094972372055054, -1.4336347579956055, -0.06020267307758331, -0.05922297388315201, -0.06204469874501228, -0.014660724438726902, -0.008188249543309212, -0.1005639061331749, -0.8861917853355408, -0.13151238858699799, -0.001301871263422072, -0.000742398202419281], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.22135208547115326, -0.010246176272630692], "finish_reason": "stop"}, {"text": " pd.Series(", "tokens": [" pd", ".", "Series", "(", "\n"], "logprobs": [-2.632835626602173, -2.0146166207268834e-05, -0.19996291399002075, -0.009384093806147575, -2.4102094173431396], "finish_reason": "stop"}, {"text": " (sum_words[0], sum_words[0]/sum_words[1])", "tokens": [" (", "sum", "_", "words", "[", "0", "]", ",", " sum", "_", "words", "[", "0", "]/", "sum", "_", "words", "[", "1", "])", "\n"], "logprobs": [-1.771174430847168, -1.6511553525924683, -0.0016657537780702114, -7.402622577501461e-05, -2.118433952331543, -0.3596841096878052, -0.048896607011556625, -2.105220317840576, -0.6473362445831299, -0.01116622518748045, -3.242440288886428e-05, -0.1399458944797516, -1.5691989660263062, -2.324420213699341, -0.9320068955421448, -0.3760470449924469, -4.625213477993384e-05, -0.38800621032714844, -0.13998964428901672, -0.09575280547142029, -0.18118250370025635], "finish_reason": "stop"}, {"text": "word, count)", "tokens": ["word", ",", " count", ")", "\n"], "logprobs": [-0.15835048258304596, -0.05651494115591049, -0.9506542682647705, -0.21253401041030884, -0.924441397190094], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.08140278607606888, -0.0002669931564014405, -0.08665291965007782, -0.0006237468332983553, -6.282132380874828e-05, -0.002253019018098712, -0.017406320199370384, -0.0010748092317953706, -0.0013987767742946744, -0.04826909676194191, -0.0002892790944315493, -0.4160213768482208, -0.00899465661495924, -0.0016312400111928582, -0.055470723658800125, -0.00013136000779923052, -0.0008850948070175946, -0.00011717586312443018, -0.00019333878299221396, -0.01236310787498951, -0.24929380416870117], "finish_reason": "stop"}, {"text": " words_freq[:10]", "tokens": [" words", "_", "freq", "[:", "1", "0", "]", "\n"], "logprobs": [-0.5103781223297119, -6.711257447022945e-05, -5.066266385256313e-05, -0.13394461572170258, -0.01623458042740822, -0.006659338716417551, -0.0014762704959139228, -0.0110238092020154], "finish_reason": "stop"}, {"text": " pd.DataFrame({\"Words\": [word for word, _ in top_words], \"Frequency\": [count for _, count in top_words]})", "tokens": [" pd", ".", "DataFrame", "(", "{\"", "Words", "\":", " [", "word", " for", " word", ",", " _", " in", " top", "_", "words", "]", ",", " \"", "F", "requency", "\":", " [", "count", " for", " _", ",", " count", " in", " top", "_", "words", "]", "})", "\n"], "logprobs": [-0.007148285862058401, -0.001321515068411827, -0.0017997510731220245, -0.06839006394147873, -2.1022186279296875, -1.9661715030670166, -0.01725190319120884, -0.1509884148836136, -0.5049001574516296, -0.07799424231052399, -0.06174091994762421, -0.005348064936697483, -0.23379430174827576, -0.0032083725091069937, -0.004362945910543203, -4.768360213347478e-06, -4.0531076592742465e-06, -0.008554007858037949, -0.027400610968470573, -0.08348555117845535, -0.4691997766494751, -0.24698494374752045, -0.008371382020413876, -0.00450750021263957, -1.4198678731918335, -0.005554480012506247, -0.003744259476661682, -0.00011312322021694854, -0.004247097764164209, -0.0012371037155389786, -0.0019558602944016457, -8.22540732769994e-06, -2.264974000354414e-06, -0.001258772797882557, -0.002062219427898526, -0.00657324492931366], "finish_reason": "stop"}, {"text": " df.plot.bar(\"Word\", \"Count\", rot=90)", "tokens": [" df", ".", "plot", ".", "bar", "(", "\"", "Word", "\"", ",", " \"", "Count", "\"", ",", " rot", "=", "9", "0", ")", "\n"], "logprobs": [-0.09162940829992294, -0.038395728915929794, -0.18538157641887665, -0.27753251791000366, -0.009450698271393776, -0.43485227227211, -3.4096479415893555, -0.02216518484055996, -0.00011657988943625242, -0.001708358759060502, -0.009416216053068638, -5.304672595229931e-05, -3.790783375734463e-05, -0.1964384764432907, -0.7499489188194275, -0.0014747231034561992, -1.4989988803863525, -0.0004935238393954933, -0.598003625869751, -0.014230088330805302], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.008795449510216713, -2.253030106658116e-05, -2.145764938177308e-06, -0.5287969708442688, -0.0020369987469166517], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0022387460339814425, -0.00010775939153973013, -7.033100700937212e-05, -0.0035012392327189445, -2.7418097943154862e-06, -9.417489309271332e-06, -0.09993370622396469, -0.004228579346090555], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.16371311247348785, -0.05467803776264191, -0.19010795652866364], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05119457095861435, -0.009875370189547539], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.02327502891421318, -0.9837906360626221, -4.768370445162873e-07, -1.311301275563892e-06, -6.711257447022945e-05, -0.00014244495832826942, -0.07370158284902573], "finish_reason": "stop"}], "metrics": {"energy": 382.0238160848618, "energy_per_token": 1.6903708676321318, "time": 14.515689, "time_per_token": 0.06422871238938054}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.lower().str.strip().str.replace(\"[^a-z]\", \"\")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "lower", "()", ".", "str", ".", "strip", "()", ".", "str", ".", "replace", "(", "\"", "[", "^", "a", "-", "z", "]\"", ",", " \"", "\"", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -0.4749401807785034, -0.000929400441236794, -1.6249703168869019, -0.16733704507350922, -0.00031132620642893016, -0.35658159852027893, -0.005266245920211077, -1.3132905960083008, -0.08299019932746887, -0.0002146728802472353, -0.2805827260017395, -0.06880612671375275, -0.4638351798057556, -0.7968547940254211, -0.19725288450717926, -0.14513613283634186, -0.00021801002731081098, -0.00045849301386624575, -1.6080726385116577, -0.0001512651506345719, -0.025903254747390747, -1.5847573280334473, -0.0760297104716301, -0.5319507122039795], "finish_reason": "stop"}, {"text": " CountVectorizer(lowercase=True, stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "low", "erc", "ase", "=", "True", ",", " stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -2.75148344039917, -0.00029094755882397294, -6.258291978156194e-05, -0.0006681832019239664, -0.046971604228019714, -0.034041628241539, -0.12487109750509262, -8.177422569133341e-05, -5.8053239627042785e-05, -0.017899995669722557, -0.04929975047707558, -9.536738616588991e-07, -0.000324673397699371, -3.099436753473128e-06, -2.0265558760002023e-06, -2.753696753643453e-05, -0.18647772073745728, -0.014405302703380585], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.07804551720619202, -9.536697689327411e-06, -4.935142715112306e-05, -0.006372844334691763, -0.004252913873642683, -0.00022682955022901297, -0.013011316768825054, -0.010952357202768326, -0.06639862805604935, -0.0003363520372658968, -9.7508447652217e-05, -3.135155202471651e-05, -0.06988609582185745, -0.626802921295166], "finish_reason": "stop"}, {"text": " IndexError:", "tokens": [" Index", "Error", ":", "\n"], "logprobs": [-3.360780954360962, -3.242440288886428e-05, -0.1022796481847763, -0.04560044780373573], "finish_reason": "stop"}, {"text": "\"Empty or invalid input file.\")", "tokens": ["\"", "Empty", " or", " invalid", " input", " file", ".", "\"", ")", "\n"], "logprobs": [-0.04748818650841713, -1.8227399587631226, -0.7230014801025391, -2.1675448417663574, -1.046786904335022, -1.6599611043930054, -0.642757773399353, -0.343370258808136, -0.002239816589280963, -0.0011310139670968056], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.22963972389698029, -0.008501528762280941], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.9622397422790527, -1.2159273865108844e-05, -4.3748852476710454e-05, -0.0004502712981775403, -0.046444397419691086, -0.2819025218486786, -0.0014665097696706653, -0.0013256819220259786, -0.24277840554714203, -0.0134965805336833, -0.27930977940559387], "finish_reason": "stop"}, {"text": " list(word_count.data.T)", "tokens": [" list", "(", "word", "_", "count", ".", "data", ".", "T", ")", "\n"], "logprobs": [-3.9531328678131104, -0.05857368931174278, -1.5936602354049683, -0.007376458030194044, -6.580135959666222e-05, -0.006164584308862686, -2.959710121154785, -0.1613415628671646, -1.307224988937378, -1.5155339241027832, -0.04462422803044319], "finish_reason": "stop"}, {"text": "word, count / sum_words.get(\"count_words\", 1))", "tokens": ["word", ",", " count", " /", " sum", "_", "words", ".", "get", "(", "\"", "count", "_", "words", "\"", ",", " ", "1", "))", "\n"], "logprobs": [-0.12943731248378754, -0.06075610965490341, -0.7750610709190369, -2.0546348094940186, -0.05200710892677307, -0.016703160479664803, -0.0002985746832564473, -1.410975694656372, -0.6465041637420654, -0.011358005926012993, -0.2084558755159378, -0.22832846641540527, -1.3362767696380615, -1.8004783391952515, -0.018387574702501297, -0.06542733311653137, -0.016024257987737656, -0.0839855968952179, -0.06675920635461807, -0.4408622682094574], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.08728532493114471, -0.00032240914879366755, -0.06067880988121033, -0.000918681500479579, -5.411955135059543e-05, -0.001909934333525598, -0.017137771472334862, -0.001116010476835072, -0.001427465584129095, -0.052834704518318176, -0.0002706876548472792, -0.37482279539108276, -0.009354096837341785, -0.0018510365625843406, -0.053480714559555054, -0.00014506718434859067, -0.0009089152445085347, -0.00014256415306590497, -0.00020001317898277193, -0.01274123229086399, -0.2755568325519562], "finish_reason": "stop"}, {"text": " [word for word, _ in words_freq][:10]", "tokens": [" [", "word", " for", " word", ",", " _", " in", " words", "_", "freq", "][", ":", "1", "0", "]", "\n"], "logprobs": [-1.355151891708374, -0.5916972756385803, -0.21147659420967102, -0.10487356781959534, -0.01008970383554697, -0.34800687432289124, -0.0023563734721392393, -0.00456624198704958, -7.247662142617628e-05, -2.586808113846928e-05, -1.9114995002746582, -0.06097741797566414, -0.027613656595349312, -0.006093495525419712, -0.00181783817242831, -0.010124988853931427], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.008028734475374222, -0.0010626628063619137, -0.0018983930349349976, -0.08220114558935165, -0.3276591897010803, -0.00015877417172305286, -1.4185804502631072e-05, -0.052871450781822205, -0.00687542837113142, -0.017341768369078636, -0.001091480371542275, -0.21517503261566162, -0.002063408959656954, -0.00010835537250386551, -0.0007257211836986244, -0.6923993229866028, -0.007240602746605873, -0.0018623403739184141, -0.03046516515314579], "finish_reason": "stop"}, {"text": " df.plot.bar(rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "rot", "=", "0", ")", "\n"], "logprobs": [-0.09259048849344254, -0.04088306427001953, -0.15946871042251587, -0.2369920313358307, -0.009979946538805962, -0.42967939376831055, -2.5392422676086426, -0.001678725820966065, -0.5992647409439087, -1.1969947814941406, -0.019155748188495636], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.010876302607357502, -2.169585604860913e-05, -2.622600959512056e-06, -1.0126121044158936, -0.037875112146139145, -1.1324817933200393e-05, -0.010358382016420364, -0.0013552061282098293], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0021061161532998085, -0.00011240804451517761, -8.40390202938579e-05, -0.00420780573040247, -2.622600959512056e-06, -1.1205610462639015e-05, -0.15453486144542694, -0.0035097922664135695], "finish_reason": "stop"}, {"text": "show()", "tokens": ["show", "()", "\n"], "logprobs": [-2.708754539489746, -0.05987980589270592, -0.04091659560799599], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05935194715857506, -0.010126994922757149], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.027824338525533676, -0.9646991491317749, -5.960462772236497e-07, -1.1920922133867862e-06, -7.033100700937212e-05, -0.00010752100206445903, -0.09060250967741013], "finish_reason": "stop"}], "metrics": {"energy": 367.10753669738756, "energy_per_token": 1.5755688270274144, "time": 15.313541, "time_per_token": 0.06572335193133047}}
{"outputs": [{"text": " pd.read_csv(file_path, delimiter=\",\")", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ",", " del", "im", "iter", "=", "\"", ",", "\"", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -1.1519023180007935, -4.184422492980957, -0.00014506718434859067, -0.010468334890902042, -0.6108186841011047, -0.30871155858039856, -0.3437870740890503, -0.01085460465401411, -0.7790846824645996, -0.05666082352399826], "finish_reason": "stop"}, {"text": " df[\"Text\"].str.replace(\"\\n\", \" \")", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "str", ".", "replace", "(", "\"", "\\", "n", "\"", ",", " \"", " \"", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -0.6905915141105652, -0.0006338732782751322, -2.4062070846557617, -0.16907614469528198, -0.39945441484451294, -1.197782278060913, -0.2271033674478531, -0.03713268041610718, -0.0015238827327266335, -0.02127574384212494, -0.06996424496173859, -0.02521778643131256, -0.4434274137020111], "finish_reason": "stop"}, {"text": " CountVectorizer(lowercase=True, stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "low", "erc", "ase", "=", "True", ",", " stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0004413345886860043, -1.7165990357170813e-05, -1.7762025890988298e-05, -0.03462032228708267, -2.75148344039917, -0.00029094755882397294, -6.258291978156194e-05, -0.0006681832019239664, -0.046971604228019714, -0.034041628241539, -0.12487109750509262, -8.177422569133341e-05, -5.8053239627042785e-05, -0.017899995669722557, -0.04929975047707558, -9.536738616588991e-07, -0.000324673397699371, -3.099436753473128e-06, -2.0265558760002023e-06, -2.753696753643453e-05, -0.18647772073745728, -0.014405302703380585], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.07804551720619202, -9.536697689327411e-06, -4.935142715112306e-05, -0.006372844334691763, -0.004252913873642683, -0.00022682955022901297, -0.013011316768825054, -0.010952357202768326, -0.06639862805604935, -0.0003363520372658968, -9.7508447652217e-05, -3.135155202471651e-05, -0.06988609582185745, -0.626802921295166], "finish_reason": "stop"}, {"text": " AttributeError as e:", "tokens": [" Attribute", "Error", " as", " e", ":", "\n"], "logprobs": [-2.126344919204712, -9.798523387871683e-05, -1.8504719734191895, -0.3088204562664032, -0.0016095790779218078, -0.031202279031276703], "finish_reason": "stop"}, {"text": "\"Error: Input is either empty or contains only stop words.\")", "tokens": ["\"", "Error", ":", " Input", " is", " either", " empty", " or", " contains", " only", " stop", " words", ".", "\"", ")", "\n"], "logprobs": [-0.04748818650841713, -2.5940699577331543, -0.3025318682193756, -1.390973687171936, -2.0312440395355225, -2.720733880996704, -0.017674798145890236, -0.012692502699792385, -0.1426360160112381, -0.04830396547913551, -0.02032722532749176, -0.11768702417612076, -0.4835648238658905, -0.09279823303222656, -0.0016692051431164145, -0.0006575806182809174], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.22963972389698029, -0.008501528762280941], "finish_reason": "stop"}, {"text": " word_count.sum(axis=0)", "tokens": [" word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", "\n"], "logprobs": [-0.9622397422790527, -1.2159273865108844e-05, -4.3748852476710454e-05, -0.0004502712981775403, -0.046444397419691086, -0.2819025218486786, -0.0014665097696706653, -0.0013256819220259786, -0.24277840554714203, -0.0134965805336833, -0.27930977940559387], "finish_reason": "stop"}, {"text": " (sum_words.tolist()[0])[0:10]", "tokens": [" (", "sum", "_", "words", ".", "tol", "ist", "()[", "0", "])", "[", "0", ":", "1", "0", "]", "\n"], "logprobs": [-1.942394495010376, -1.4494612216949463, -0.0018691227305680513, -8.093983342405409e-05, -0.8679214119911194, -3.3636975288391113, -3.361645576660521e-05, -1.2187469005584717, -0.042197734117507935, -0.46309834718704224, -2.744029998779297, -1.5123002529144287, -2.7369840145111084, -0.0663522258400917, -0.0318300798535347, -0.007351844571530819, -0.040896911174058914], "finish_reason": "stop"}, {"text": "word, count / sum_words) for (word, count) in zip(vectorizer.get_feature_names(), word_count.toarray())", "tokens": ["word", ",", " count", " /", " sum", "_", "words", ")", " for", " (", "word", ",", " count", ")", " in", " zip", "(", "vector", "izer", ".", "get", "_", "feature", "_", "names", "()", ",", " word", "_", "count", ".", "to", "array", "())", "\n"], "logprobs": [-0.12943731248378754, -0.06075610965490341, -0.7750610709190369, -2.0546348094940186, -0.05200710892677307, -0.016703160479664803, -0.0002985746832564473, -1.519965410232544, -0.8342775702476501, -2.7744503021240234, -0.002857413375750184, -0.000129691296024248, -0.0022230453323572874, -0.0008854520856402814, -0.027814598754048347, -0.5834436416625977, -0.0027694464661180973, -0.04240753501653671, -5.483612312673358e-06, -7.128461584215984e-05, -0.05282226577401161, -2.264974000354414e-06, -0.00024732868769206107, -4.768370445162873e-07, -0.001313300454057753, -0.2782741189002991, -0.0005466635921038687, -0.8987601399421692, -5.8412379075889476e-06, -1.311301275563892e-06, -0.04166239872574806, -0.5019899010658264, -0.0010484919184818864, -0.5820392370223999, -0.0086794039234519], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.08728532493114471, -0.00032240914879366755, -0.06067880988121033, -0.000918681500479579, -5.411955135059543e-05, -0.001909934333525598, -0.017137771472334862, -0.001116010476835072, -0.001427465584129095, -0.052834704518318176, -0.0002706876548472792, -0.37482279539108276, -0.009354096837341785, -0.0018510365625843406, -0.053480714559555054, -0.00014506718434859067, -0.0009089152445085347, -0.00014256415306590497, -0.00020001317898277193, -0.01274123229086399, -0.2755568325519562], "finish_reason": "stop"}, {"text": " list(map(lambda x: x[0], words_freq[:10]))", "tokens": [" list", "(", "map", "(", "lambda", " x", ":", " x", "[", "0", "]", ",", " words", "_", "freq", "[:", "1", "0", "]))", "\n"], "logprobs": [-3.446092367172241, -0.007581981830298901, -0.6109119057655334, -0.0005581013392657042, -0.01030033454298973, -0.036158159375190735, -0.002304519060999155, -0.11393444985151291, -0.0061365049332380295, -0.0015485212206840515, -0.0018713833997026086, -0.008941254578530788, -0.0025475923903286457, -2.1219027985353023e-05, -9.059865078597795e-06, -0.6971326470375061, -0.012526994571089745, -0.006401983089745045, -0.0026966894511133432, -0.009240125305950642], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "Count", "\"", "])", "\n"], "logprobs": [-0.008028734475374222, -0.0010626628063619137, -0.0018983930349349976, -0.08220114558935165, -0.3276591897010803, -0.00015877417172305286, -1.4185804502631072e-05, -0.052871450781822205, -0.00687542837113142, -0.017341768369078636, -0.001091480371542275, -0.21517503261566162, -0.002063408959656954, -0.00010835537250386551, -0.0007257211836986244, -0.6923993229866028, -0.007240602746605873, -0.0018623403739184141, -0.03046516515314579], "finish_reason": "stop"}, {"text": " plt.bar(range(len(top_words)), df[\"Count\"])", "tokens": [" plt", ".", "bar", "(", "range", "(", "len", "(", "top", "_", "words", "))", ",", " df", "[", "\"", "Count", "\"", "])", "\n"], "logprobs": [-3.022400140762329, -0.00015448330668732524, -0.801627516746521, -0.37180665135383606, -1.0107696056365967, -0.0002615109842736274, -1.4673725366592407, -5.769562994828448e-05, -0.32647886872291565, -3.933898824470816e-06, -4.6491513785440475e-06, -0.0016839622985571623, -0.00012182447244413197, -0.8813839554786682, -0.039055947214365005, -0.0013047285610809922, -0.0016518294578418136, -4.5060096454108134e-05, -2.7147915363311768, -0.007336342707276344], "finish_reason": "stop"}, {"text": " save_path is not None:", "tokens": [" save", "_", "path", " is", " not", " None", ":", "\n"], "logprobs": [-0.009914912283420563, -2.586808113846928e-05, -2.7418097943154862e-06, -1.0468164682388306, -0.04550602287054062, -1.0967194612021558e-05, -0.008179972879588604, -0.0013100859941914678], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0020729261450469494, -0.00010311071673640981, -7.855583680793643e-05, -0.0032380789052695036, -2.50339189733495e-06, -1.0013530300057027e-05, -0.1237422376871109, -0.0039856769144535065], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.12948831915855408, -0.06894121319055557, -0.2142215371131897], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05100971460342407, -0.009946898557245731], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax"], "logprobs": [-0.023877913132309914], "finish_reason": "stop"}], "metrics": {"energy": 437.45052879142753, "energy_per_token": 1.6570095787554073, "time": 17.415068, "time_per_token": 0.06596616666666667}}
{"outputs": [{"text": " pd.read_csv(file_path)", "tokens": [" pd", ".", "read", "_", "csv", "(", "file", "_", "path", ")", "\n"], "logprobs": [-0.026135550811886787, -0.0002740246127359569, -0.006254266947507858, -3.0278701160568744e-05, -0.0007445423980243504, -0.0009586982196196914, -0.006491644773632288, -0.0007296522380784154, -2.5510462364763953e-05, -0.3907385766506195, -0.06840943545103073], "finish_reason": "stop"}, {"text": " df[\"Text\"].astype(str)", "tokens": [" df", "[", "\"", "Text", "\"", "]", ".", "ast", "ype", "(", "str", ")", "\n"], "logprobs": [-0.019284965470433235, -0.03419313207268715, -0.0002787916746456176, -0.00010549465514486656, -5.245195097813848e-06, -0.0002791491860989481, -0.004199970979243517, -2.2947709560394287, -0.00012635385792236775, -0.01766648329794407, -0.2837679088115692, -0.0007201223634183407, -0.4233097434043884], "finish_reason": "stop"}, {"text": " CountVectorizer(stop_words=STOP_WORDS)", "tokens": [" Count", "Vector", "izer", "(", "stop", "_", "words", "=", "ST", "OP", "_", "W", "OR", "DS", ")", "\n"], "logprobs": [-0.0003953390696551651, -1.6689160474925302e-05, -1.490105023549404e-05, -0.029216133058071136, -0.36728182435035706, -0.0009017692063935101, -6.532455881824717e-05, -0.02814045175909996, -0.05084782466292381, -7.152555099310121e-07, -0.0004961451631970704, -3.6954811548639555e-06, -2.50339189733495e-06, -2.5152843591058627e-05, -0.3859751522541046, -0.012796196155250072], "finish_reason": "stop"}, {"text": " vectorizer.fit_transform(df[\"Text\"])", "tokens": [" vector", "izer", ".", "fit", "_", "transform", "(", "df", "[", "\"", "Text", "\"", "])", "\n"], "logprobs": [-0.08213822543621063, -1.0490362910786644e-05, -5.185469490243122e-05, -0.007920405827462673, -0.0035200081765651703, -0.00023398046323563904, -0.009247093461453915, -0.012697681784629822, -0.06290388107299805, -0.0004122599493712187, -9.48860906646587e-05, -2.992108420585282e-05, -0.0737694650888443, -0.5549620985984802], "finish_reason": "stop"}, {"text": " AttributeError:", "tokens": [" Attribute", "Error", ":", "\n"], "logprobs": [-2.082515001296997, -0.00010394509445177391, -0.1640866994857788, -0.04667549580335617], "finish_reason": "stop"}, {"text": "\"Input file is empty or contains only stopwords.\")", "tokens": ["\"", "Input", " file", " is", " empty", " or", " contains", " only", " stop", "words", ".", "\"", ")", "\n"], "logprobs": [-0.05799452215433121, -1.2033532857894897, -1.1196528673171997, -0.072927325963974, -0.025309031829237938, -0.03515943884849548, -0.022987082600593567, -0.012391484342515469, -0.008246419951319695, -2.620755434036255, -0.7206478714942932, -0.1324058473110199, -0.0013368725776672363, -0.0008355463505722582], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.25942009687423706, -0.00946392398327589], "finish_reason": "stop"}, {"text": " pd.Series(word_count.sum(axis=0).tolist(),", "tokens": [" pd", ".", "Series", "(", "word", "_", "count", ".", "sum", "(", "axis", "=", "0", ")", ".", "tol", "ist", "()", ",", "\n"], "logprobs": [-2.6468167304992676, -1.9192511899746023e-05, -0.18202143907546997, -0.010603280737996101, -0.32258111238479614, -4.732496745418757e-05, -7.545663538621739e-05, -0.0005920564290136099, -0.09646575897932053, -0.00034588552080094814, -0.0021490592043846846, -0.0008281615446321666, -0.19435422122478485, -0.18209367990493774, -0.887811005115509, -2.0340723991394043, -1.4305104514278355e-06, -0.08414845168590546, -0.0006783091812394559, -1.6851065158843994], "finish_reason": "stop"}, {"text": " sum_words.data.tolist()", "tokens": [" sum", "_", "words", ".", "data", ".", "tol", "ist", "()", "\n"], "logprobs": [-2.6363730430603027, -0.0025733946822583675, -2.777537883957848e-05, -0.25652751326560974, -3.8430895805358887, -0.3259185552597046, -0.5702458024024963, -7.390948667307384e-06, -0.025175241753458977, -0.026444651186466217], "finish_reason": "stop"}, {"text": "word, sum_words[i]) for word, i in vectorizer.vocabulary_.items()", "tokens": ["word", ",", " sum", "_", "words", "[", "i", "])", " for", " word", ",", " i", " in", " vector", "izer", ".", "voc", "abulary", "_", ".", "items", "()", "\n"], "logprobs": [-0.14942370355129242, -0.058733776211738586, -1.055227518081665, -0.018053026869893074, -0.011641544289886951, -0.097014419734478, -0.394141286611557, -0.08333848416805267, -0.6633839011192322, -0.18328602612018585, -0.0010973153403028846, -0.0034077451564371586, -0.0018629353726282716, -0.3412112891674042, -8.34461570775602e-06, -7.068861305015162e-05, -0.04649162292480469, -0.008053568191826344, -0.00022873646230436862, -0.00013410145766101778, -0.00042691652197390795, -3.3854863431770355e-05, -0.024420754984021187], "finish_reason": "stop"}, {"text": " sorted(words_freq, key=lambda x: x[1], reverse=True)", "tokens": [" sorted", "(", "words", "_", "freq", ",", " key", "=", "lambda", " x", ":", " x", "[", "1", "]", ",", " reverse", "=", "True", ")", "\n"], "logprobs": [-0.07281139492988586, -0.00027736154152080417, -0.10030525177717209, -0.0007858763565309346, -4.5298504119273275e-05, -0.0015942256432026625, -0.015634816139936447, -0.0010605193674564362, -0.0012479382567107677, -0.040566034615039825, -0.00026890001026913524, -0.45262834429740906, -0.008218635804951191, -0.0018994638230651617, -0.06352496892213821, -0.00012659224739763886, -0.0009201106731779873, -0.00013362467871047556, -0.00019298121333122253, -0.01078207977116108, -0.19284211099147797], "finish_reason": "stop"}, {"text": " list(map(lambda x: x[0], words_freq[:10]))", "tokens": [" list", "(", "map", "(", "lambda", " x", ":", " x", "[", "0", "]", ",", " words", "_", "freq", "[:", "1", "0", "]))", "\n"], "logprobs": [-3.541980266571045, -0.007816797122359276, -0.6178917288780212, -0.0004587313160300255, -0.012941420078277588, -0.037474799901247025, -0.0025132279843091965, -0.13701634109020233, -0.006573955528438091, -0.0017753568245097995, -0.0015937496209517121, -0.006530373357236385, -0.0022549221757799387, -1.9907753085135482e-05, -9.894321920000948e-06, -0.5742392539978027, -0.014620781876146793, -0.00778568908572197, -0.0024222577922046185, -0.008028024807572365], "finish_reason": "stop"}, {"text": " pd.DataFrame(top_words, columns=[\"Word\", \"Frequency\"])", "tokens": [" pd", ".", "DataFrame", "(", "top", "_", "words", ",", " columns", "=[", "\"", "Word", "\"", ",", " \"", "F", "requency", "\"", "])", "\n"], "logprobs": [-0.0070237633772194386, -0.0015900599537417293, -0.001490554423071444, -0.07642371952533722, -0.3372756242752075, -0.00012885693286079913, -1.0132738680113107e-05, -0.05506137013435364, -0.008982133120298386, -0.018366975709795952, -0.0008629412623122334, -0.22936727106571198, -0.002212459221482277, -0.00013851160474587232, -0.0006551980040967464, -0.7912131547927856, -0.09123589843511581, -0.004099537618458271, -0.0025601964443922043, -0.039150066673755646], "finish_reason": "stop"}, {"text": " df.plot.bar(x=\"Word\", y=\"Count\", rot=0)", "tokens": [" df", ".", "plot", ".", "bar", "(", "x", "=", "\"", "Word", "\"", ",", " y", "=", "\"", "Count", "\"", ",", " rot", "=", "0", ")", "\n"], "logprobs": [-0.07396809756755829, -0.03355954959988594, -0.16328541934490204, -0.29804572463035583, -0.008789895102381706, -0.4190646708011627, -0.3674757480621338, -0.053383417427539825, -0.0009392855572514236, -0.0008964095613919199, -4.053033626405522e-05, -0.004736513365060091, -0.05966566503047943, -0.0015835139201954007, -0.00028463127091526985, -4.2914423829643056e-05, -1.9311717551317997e-05, -0.3111804723739624, -0.5684208869934082, -0.003154661040753126, -0.3564617335796356, -0.5970485806465149, -0.0118456045165658], "finish_reason": "stop"}, {"text": " save_path:", "tokens": [" save", "_", "path", ":", "\n"], "logprobs": [-0.011407624930143356, -2.5748875486897305e-05, -2.50339189733495e-06, -0.47383594512939453, -0.0021068297792226076], "finish_reason": "stop"}, {"text": "savefig(save_path)", "tokens": ["save", "fig", "(", "save", "_", "path", ")", "\n"], "logprobs": [-0.0020561523269861937, -9.285972191719338e-05, -8.49926145747304e-05, -0.003594843437895179, -2.622600959512056e-06, -8.22540732769994e-06, -0.11680440604686737, -0.004207330755889416], "finish_reason": "stop"}, {"text": "close()", "tokens": ["close", "()", "\n"], "logprobs": [-0.11094005405902863, -0.0662967786192894, -0.23157009482383728], "finish_reason": "stop"}, {"text": " None", "tokens": [" None", "\n"], "logprobs": [-0.05654975026845932, -0.011403500102460384], "finish_reason": "stop"}, {"text": " ax", "tokens": [" ax", "<", "|", "end", "of", "text", "|>"], "logprobs": [-0.02351643703877926, -1.2049063444137573, -4.768370445162873e-07, -1.0728830375228426e-06, -5.6622808187967166e-05, -0.00015639036428183317, -0.08222738653421402], "finish_reason": "stop"}], "metrics": {"energy": 380.03730386734026, "energy_per_token": 1.6103275587599164, "time": 14.950201, "time_per_token": 0.06334830932203389}}
