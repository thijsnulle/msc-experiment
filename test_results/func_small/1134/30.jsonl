{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError('The source directory does not exist.')\n    target_dir_created = False\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n        target_dir_created = True\n\n    files_with_path = glob.glob(os.path.join(source_dir, '*'))\n    files_with_path = [os.path.relpath(path, source_dir) for path in files_with_path]\n    files = [\n        os.path.basename(path)\n        for path in files_with_path\n        if os.path.isfile(path)]\n    hashed_files = []\n\n    for file in files:\n        hash_value = hashlib.md5(file.encode()).hexdigest()\n        hashed_file_name = f'{file}.{hash_value}'\n        hashed_file_path = os.path.join(target_dir, hashed_file_name)\n        with open(hashed_", "compilation_passed": false, "time": 0.0002779960632324219, "tests_passed": false, "error": "SyntaxError"}
{"code": "if not os.path.isdir(source_dir):\n        raise FileNotFoundError(f'The source directory {source_dir} does not exist.')\n    paths = glob.glob(source_dir + '/*')\n    file_paths = []\n    for path in paths:\n        hash_object = hashlib.md5()\n        with open(path, 'rb') as f:\n            while True:\n                buf = f.read(4096)\n                if len(buf) == 0:\n                    break\n                hash_object.update(buf)\n        filename = path.split('/')[-1]\n        file_path = f'{target_dir}/{prefix}{filename}'\n        file_paths.append(file_path)\n        with open(file_path, 'w') as f:\n            f.write(hash_object.hexdigest())\n            f.write(open(path, 'r').read())\n    return file_paths", "compilation_passed": true, "time": 0.0019381046295166016, "tests_passed": true, "error": null}
{"code": "if not os.path.isdir(source_dir):\n        raise FileNotFoundError(f'The source directory {source_dir} does not exist.')\n    target_dir = os.path.normpath(target_dir)\n    os.makedirs(target_dir, exist_ok=True)\n    md5s = []\n    for path in glob.glob(os.path.join(source_dir, '**/*', '*')):\n        if os.path.isdir(path):\n            continue\n        md5s.append(prefix + hashlib.md5(os.path.realpath(path).encode()).hexdigest())\n    return [os.path.join(target_dir, os.path.relpath(f, source_dir)) for f in md5s]", "compilation_passed": true, "time": 0.0024781227111816406, "tests_passed": true, "error": null}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Error: The source directory {source_dir} does not exist.\")\n\n    # Check if target_dir exists, if not create it\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # List all files in the source directory\n    files = glob.glob(os.path.join(source_dir, '**/*.*'))\n\n    # Create a new list of paths for each hash-prepended file\n    paths = [\n        os.path.join(\n            target_dir,\n            os.path.relpath(os.path.normpath(file_name), start=source_dir).replace(os.sep, '_')\n        )\n        for file_name in files\n    ]\n\n    # Open each path in read mode, write the hash and the original content, and append the new content to a new file\n    for path, new_path in zip(paths, paths):\n        with open(path, 'r') as f:", "compilation_passed": false, "time": 0.00020694732666015625, "tests_passed": false, "error": "IndentationError"}
{"code": "if not os.path.isdir(source_dir):\n        raise FileNotFoundError(f'The source directory does not exist: {source_dir}')\n    try:\n        os.mkdir(target_dir)\n    except FileExistsError:\n        pass\n    if os.listdir(target_dir) != []:\n        os.rmdir(target_dir)\n    os.mkdir(target_dir)\n\n    md5_hashes = []\n\n    # The glob.glob method will return a list of all files in the source directory whose name matches\n    # the specified pattern. This pattern can be used to match files by name, or by name with extensions.\n    files = glob.glob(os.path.join(source_dir, '*'))\n\n    # For each file in the list, compute its MD5 hash\n    for file in files:\n        hash_value = hashlib.md5()\n        with open(file, 'rb') as f:\n            for chunk in iter(lambda: f.read(4096), b''):\n                hash_value.update(chunk", "compilation_passed": false, "time": 0.00023293495178222656, "tests_passed": false, "error": "SyntaxError"}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(f'Source directory {source_dir} not found.')\n\n    source_files = glob.glob(os.path.join(source_dir, '*.txt'))\n    new_files = []\n\n    for source_file in source_files:\n        md5 = hashlib.md5()\n        with open(source_file, 'rb') as f:\n            md5.update(f.read())\n            md5 = md5.hexdigest()\n        hash_prefix = f'{prefix}{md5}'\n        target_file = os.path.join(target_dir, os.path.basename(source_file))\n        with open(target_file, 'w') as f:\n            f.write(hash_prefix + '\\n')\n            f.write(open(source_file, 'r').read())\n        new_files.append(target_file)\n\n    return new_files", "compilation_passed": true, "time": 0.005268096923828125, "tests_passed": true, "error": null}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"The source directory {source_dir} does not exist.\")\n\n    files_list = glob.glob(source_dir + '/**/*.txt', recursive=True)\n    if not files_list:\n        raise FileNotFoundError(f\"No files found in the source directory {source_dir}.\")\n\n    for file_path in files_list:\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                content = f.read()\n        except UnicodeDecodeError:\n            # Ignore files that cannot be read due to invalid encoding\n            continue\n        # Convert the file's content to a string\n        hash_object = hashlib.md5()\n        hash_object.update(content.encode('utf-8'))\n        hashed_content = f\"{prefix}{hash_object.hexdigest()}\\n{content}\"\n        hashed_file_path = os.path.join(target_dir, os.path.basename(", "compilation_passed": false, "time": 0.0002541542053222656, "tests_passed": false, "error": "SyntaxError"}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory '{source_dir}' not found.\")\n\n    for filename in glob.glob(os.path.join(source_dir, \"*.txt\")):\n        with open(filename, \"r\") as file:\n            original_content = file.read()\n            md5_hash = hashlib.md5(original_content.encode('utf-8')).hexdigest()\n            new_content = f\"{prefix}{md5_hash}\\n{original_content}\"\n            new_path = os.path.join(target_dir, os.path.basename(filename))\n            with open(new_path, \"w\") as file:\n                file.write(new_content)\n\n    return [new_path for new_path in glob.glob(os.path.join(target_dir, \"*.txt\"))]", "compilation_passed": true, "time": 0.004864931106567383, "tests_passed": true, "error": null}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError('source directory does not exist')\n    file_paths = glob.glob(os.path.join(source_dir, '*.txt'))\n    file_hashes = {}\n    for file_path in file_paths:\n        with open(file_path, 'rb') as f:\n            content = f.read()\n            hash_ = hashlib.md5(content).hexdigest()\n            file_hashes[file_path] = hash_\n\n    processed_paths = []\n    for file_path in file_paths:\n        file_hash = file_hashes[file_path]\n        new_path = os.path.join(target_dir, f'{prefix}{file_hash}{os.path.basename(file_path)}')\n        os.makedirs(os.path.dirname(new_path), exist_ok=True)\n        with open(new_path, 'w') as f:\n            f.write(prefix + content)\n        processed_paths.append(new_path)", "compilation_passed": true, "time": 0.0018777847290039062, "tests_passed": false, "error": "TypeError"}
{"code": "try:\n        # Create target directory if it does not exist\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n\n        # Find MD5 hashes of files in the source directory and write the hashed files to the target directory\n        for path, _, filenames in os.walk(source_dir):\n            for filename in filenames:\n                full_path = os.path.join(path, filename)\n                with open(full_path, 'rb') as file:\n                    content = file.read()\n                    hash_value = hashlib.md5(content).hexdigest()\n                    target_path = os.path.join(target_dir, f'{prefix}{hash_value}')\n                    with open(target_path, 'wb') as target_file:\n                        target_file.write(content)\n\n        return [os.path.join(target_dir, filename) for filename in os.listdir(target_dir)]\n    except FileNotFoundError as e:\n        raise FileNotFoundError(f'Source directory", "compilation_passed": false, "time": 0.0001049041748046875, "tests_passed": false, "error": "SyntaxError"}
{"code": "if not os.path.isdir(source_dir):\n        raise FileNotFoundError(f'Source directory {source_dir} not found')\n\n    files = glob.glob(f'{source_dir}/**/*', recursive=True)\n\n    target_dir = os.path.abspath(target_dir)\n    os.makedirs(target_dir, exist_ok=True)\n\n    paths = []\n\n    for file in files:\n        if not os.path.isfile(file):\n            continue\n\n        with open(file, 'rb') as f:\n            data = f.read()\n\n            hash = hashlib.md5(data).hexdigest()\n            path = os.path.join(target_dir, f'{prefix}{hash}')\n\n        with open(path, 'wb') as f:\n            f.write(data)\n        paths.append(path)\n\n    return paths", "compilation_passed": true, "time": 0.001828908920288086, "tests_passed": true, "error": null}
{"code": "# Check requirements.\n    for mod in (os, glob, hashlib):\n        if mod is None:\n            raise ImportError('Some requirements are missing.')\n\n    # Check input and get files to process.\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"The directory {source_dir} does not exist.\")\n    file_list = glob.glob(os.path.join(source_dir, '**/*'))\n    # Get file names, excluding files in the target directory.\n    files_to_process = [f for f in file_list if not os.path.isdir(f)]\n\n    # If files exists in the target directory, delete them.\n    target_dir = os.path.realpath(target_dir)\n    for filename in glob.glob(os.path.join(target_dir, '*')):\n        os.remove(filename)\n\n    # Create list of files created.\n    new_files = []\n\n    # Iterate through the files and create new files with the hash prepended.\n    for source_file in files", "compilation_passed": false, "time": 0.00022029876708984375, "tests_passed": false, "error": "SyntaxError"}
{"code": "# check if the source directory exists\n    if not os.path.exists(source_dir):\n        raise FileNotFoundError('Source directory does not exist.')\n\n    # create the target directory if it does not exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # calculate the MD5 hash of each file's content, append the hash with a prefix to the content, and write the modified\n    # content to new files in the target directory\n    paths = []\n    for file_path in glob.glob(f'{source_dir}/**/*.txt', recursive=True):\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n\n        # calculate the MD5 hash of the file's content\n        hash_md5 = hashlib.md5(content.encode('utf-8')).hexdigest()\n\n        # append the prefix to the content before writing to a new file\n        new_content = f'{prefix}{hash_md5}\\n{content}'\n\n        ", "compilation_passed": true, "time": 0.001580953598022461, "tests_passed": false, "error": "TypeError"}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"The directory {source_dir} does not exist.\")\n\n    files = glob.glob(os.path.join(source_dir, \"*\"))\n    hashes = {}\n    for f in files:\n        with open(f, \"rb\") as file:\n            content = file.read()\n            hashes[os.path.basename(f)] = hashlib.md5(content).hexdigest()\n    for file_name, hash_value in hashes.items():\n        path = os.path.join(target_dir, f\"{file_name}.txt\")\n        with open(path, \"w\") as new_file:\n            new_file.write(f\"{prefix} {hash_value}\\n\")\n            new_file.write(f\"{path}\\n\")\n            new_file.write(f\"{source_dir} {file_name}\\n\")\n            new_file.write(f\"{prefix} {hash_value}\\n", "compilation_passed": false, "time": 0.00011587142944335938, "tests_passed": false, "error": "SyntaxError"}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError\n    all_paths = []\n    for root, dir_names, file_names in os.walk(source_dir):\n        for file_name in file_names:\n            full_path = os.path.join(root, file_name)\n            all_paths.append(full_path)\n    all_paths = glob.glob(os.path.join(source_dir, '*.txt'))\n    for path in all_paths:\n        with open(path, 'rb') as file:\n            content = file.read()\n            content = prefix + hashlib.md5(content).hexdigest() + content\n        with open(path, 'w') as file:\n            file.write(content)\n    return all_paths", "compilation_passed": true, "time": 0.0016400814056396484, "tests_passed": false, "error": "TypeError"}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(source_dir)\n\n    files = glob.glob(os.path.join(source_dir, '**/*'))\n    for path in files:\n        with open(path, 'rb') as file:\n            content = file.read()\n        new_content = f\"{prefix}{hashlib.md5(content).hexdigest()}\\n\" + content\n        new_path = path.replace(source_dir, target_dir)\n        with open(new_path, 'wb') as new_file:\n            new_file.write(new_content)", "compilation_passed": true, "time": 0.0015532970428466797, "tests_passed": false, "error": "TypeError"}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(\"Source directory does not exist\")\n\n    hash_files = []\n\n    # Create the target directory if it doesn't exist\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # Calculate MD5 hashes of all files in the source directory\n    for filename in glob.glob(os.path.join(source_dir, '**/*'), recursive=True):\n        if not os.path.isfile(filename):\n            continue\n        with open(filename, 'rb') as f:\n            md5_hash = hashlib.md5()\n            while True:\n                data = f.read(4096)\n                if not data:\n                    break\n                md5_hash.update(data)\n            hash_filename = os.path.join(target_dir, f'{prefix}{md5_hash.hexdigest()}.{os.path.basename(filename)}')\n            hash_files.append(hash_filename)\n            with", "compilation_passed": false, "time": 0.00024390220642089844, "tests_passed": false, "error": "SyntaxError"}
{"code": "if not os.path.isdir(source_dir):\n        raise FileNotFoundError('Source directory does not exist!')\n\n    md5_list = []\n    for f in glob.glob(os.path.join(source_dir, '*')):\n        md5_list.append(hashlib.md5(open(f, 'rb').read()).hexdigest())\n\n    for i, md5 in enumerate(md5_list):\n        md5_list[i] = f'{prefix}{md5}'\n\n    target_dir = target_dir if os.path.isdir(target_dir) else os.path.join(target_dir, '')\n    for i, f in enumerate(md5_list):\n        os.rename(f, os.path.join(target_dir, f.split(':')[-1]))\n\n    return md5_list", "compilation_passed": true, "time": 0.001657724380493164, "tests_passed": false, "error": "FileNotFoundError"}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(f'Source directory does not exist: {source_dir}')\n    if not os.path.isdir(source_dir):\n        raise ValueError(f'Source directory is not a directory: {source_dir}')\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    if not os.path.isdir(target_dir):\n        raise ValueError(f'Target directory is not a directory: {target_dir}')\n    md5_hash_list = []\n    for source_file in glob.glob(os.path.join(source_dir, '*')):\n        with open(source_file, 'r', encoding='utf-8') as f:\n            source_file_contents = f.read()\n            md5_hash = hashlib.md5(source_file_contents.encode('utf-8')).hexdigest()\n            md5_hash_list.append(os.path.join(target_dir, f'{md5_", "compilation_passed": false, "time": 0.00010228157043457031, "tests_passed": false, "error": "SyntaxError"}
{"code": "if not os.path.isdir(source_dir):\n        raise FileNotFoundError(f\"{source_dir} is not a directory\")\n\n    all_hash_files = []\n\n    for src_file_path in glob.glob(os.path.join(source_dir, \"*.txt\")):\n        with open(src_file_path, \"r\") as src_file:\n            content = prefix + hashlib.md5(src_file.read().encode()).hexdigest() + \"\\n\" + src_file.read()\n\n        target_file_path = os.path.join(target_dir, os.path.basename(src_file_path))\n        all_hash_files.append(target_file_path)\n\n        with open(target_file_path, \"w\") as target_file:\n            target_file.write(content)\n\n    return all_hash_files", "compilation_passed": true, "time": 0.003075122833251953, "tests_passed": true, "error": null}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError\n    files = []\n    for fname in glob.glob(os.path.join(source_dir, '*.*')):\n        md5 = hashlib.md5()\n        with open(fname, 'rb') as f:\n            md5.update(f.read())\n        md5_hex = md5.hexdigest()\n        filename = md5_hex + os.path.basename(fname)\n        path = os.path.join(target_dir, filename)\n        with open(path, 'wb') as f:\n            f.write(bytes(f'{prefix}{md5_hex}', 'utf-8'))\n            f.write(os.path.getsize(fname) * b' ')\n        files.append(path)\n    return files", "compilation_passed": true, "time": 0.004014730453491211, "tests_passed": true, "error": null}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(f'Source directory \"{source_dir}\" does not exist.')\n\n    paths = glob.glob(os.path.join(source_dir, '**/*'))\n    with multiprocessing.Pool() as p:\n        result = p.map(hash_file_content, paths)\n\n    os.makedirs(target_dir, exist_ok=True)\n    for path, hash_val in result:\n        new_path = os.path.join(target_dir, f'{prefix}{hash_val}{os.path.basename(path)}')\n        with open(new_path, 'w') as file:\n            file.write(prefix + hash_val + os.linesep + os.linesep + path_content)\n\n    return [new_path for _, new_path in result]", "compilation_passed": true, "time": 0.0018019676208496094, "tests_passed": false, "error": "NameError"}
{"code": "if not os.path.isdir(source_dir):\n        raise FileNotFoundError(f\"{source_dir} is not a valid directory!\")\n\n    def hash_file(path):\n        \"\"\"\n        Computes the MD5 hash of a file and returns the path to a new file with the hash prepended.\n\n        Parameters:\n        - path (str): The path to the file to hash.\n\n        Returns:\n        - str: The path to the new file with the hash prepended.\n\n        Requirements:\n        - glob\n        - hashlib\n\n        Raises:\n        FileNotFoundError if the file does not exist.\n        \"\"\"\n        if not os.path.exists(path):\n            raise FileNotFoundError(f\"{path} is not a valid file!\")\n\n        with open(path, 'rb') as f:\n            hash_object = hashlib.md5()\n            hash_object.update(f.read())\n            file_name, extension = os.path.splitext(path)\n            hash_path = f\"{file_name}", "compilation_passed": false, "time": 8.58306884765625e-05, "tests_passed": false, "error": "SyntaxError"}
{"code": "try:\n        files = glob.glob(source_dir + '/*')\n    except FileNotFoundError:\n        return []\n\n    hashed_files = []\n    for file in files:\n        hash_value = hashlib.md5()\n        with open(file, 'rb') as file_to_hash:\n            for chunk in iter(lambda: file_to_hash.read(4096), b\"\"):\n                hash_value.update(chunk)\n        with open(os.path.join(target_dir, '{0}{1}'.format(prefix, os.path.basename(file))), 'wb') as new_file:\n            new_file.write(hash_value.hexdigest().encode() + os.linesep)\n            new_file.write(file_to_hash.read())\n        hashed_files.append(os.path.join(target_dir, '{0}{1}'.format(prefix, os.path.basename(file))))\n\n    return hashed_files", "compilation_passed": true, "time": 0.0018668174743652344, "tests_passed": false, "error": "TypeError"}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(f'Source directory \"{source_dir}\" does not exist.')\n\n    for filename in glob.glob(os.path.join(source_dir, '**/*'), recursive=True):\n        with open(filename, 'rb') as file:\n            # The entire file is hashed.\n            hash_object = hashlib.md5()\n            hash_object.update(file.read())\n            # The content of the file is hashed as well.\n            content = hash_object.hexdigest()\n\n        # Write the modified content.\n        new_filename = f'{os.path.join(target_dir, prefix)} {os.path.basename(filename)}'\n        with open(new_filename, 'w') as file:\n            file.write(content)", "compilation_passed": true, "time": 0.00186920166015625, "tests_passed": false, "error": "TypeError"}
{"code": "if not os.path.isdir(source_dir):\n        raise FileNotFoundError(f'Source directory {source_dir} does not exist')\n\n    file_paths = glob.glob(f'{source_dir}/*.txt')\n    result = []\n\n    for file_path in file_paths:\n        with open(file_path, 'r') as f:\n            content = f.read()\n        hash_obj = hashlib.md5(content.encode())\n        hash_value = hash_obj.hexdigest()\n        new_content = f'{prefix}{hash_value}\\n{content}'\n        new_file = f'{target_dir}/{os.path.basename(file_path)}'\n        with open(new_file, 'w') as new_f:\n            new_f.write(new_content)\n        result.append(new_file)\n    return result", "compilation_passed": true, "time": 0.006319999694824219, "tests_passed": true, "error": null}
{"code": "if not os.path.isdir(source_dir):\n        raise FileNotFoundError(source_dir)\n\n    hash_list = []\n\n    for f in glob.glob(source_dir + '/*'):\n        with open(f, \"rb\") as file:\n            hash_list.append(f)\n            hash_obj = hashlib.md5(file.read())\n            hash_str = f\"{prefix}{hash_obj.hexdigest()}\"\n            new_file = f\"{target_dir}/{os.path.basename(f)}\"\n            with open(new_file, \"w+\") as new_file:\n                new_file.write(hash_str)\n    return hash_list", "compilation_passed": true, "time": 0.002025127410888672, "tests_passed": true, "error": null}
{"code": "try:\n        os.mkdir(target_dir)\n    except FileExistsError:\n        pass\n    for src_file in glob.glob(os.path.join(source_dir, '**/*.txt')):\n        with open(src_file, 'rb') as f:\n            content = f.read()\n            hash = hashlib.md5()\n            hash.update(content)\n            dest_file = os.path.join(target_dir, f'MD5_{prefix}{hash.hexdigest()}.txt')\n            print(dest_file)\n            with open(dest_file, 'wb') as out:\n                out.write(f'#MD5: {hash.hexdigest()}\\n{content}')\n    return [dest_file for dest_file in glob.glob(os.path.join(target_dir, 'MD5_*.txt'))]", "compilation_passed": true, "time": 0.0027730464935302734, "tests_passed": true, "error": null}
{"code": "if not os.path.exists(source_dir):\n        raise FileNotFoundError(f\"Source directory not found: {source_dir}\")\n\n    # List all the files in the source directory\n    file_paths = glob.glob(os.path.join(source_dir, '*'))\n\n    # Iterate over each file and compute its MD5 hash and write it to a new file with the prefix prepended\n    hashed_paths = []\n    for file_path in file_paths:\n        # Compute MD5 hash for file content\n        md5_hash = hashlib.md5()\n        with open(file_path, 'rb') as file:\n            chunk_size = 64 * 1024  # 64 kB chunks for faster reading\n            chunk = file.read(chunk_size)\n            while chunk:\n                md5_hash.update(chunk)\n                chunk = file.read(chunk_size)\n        md5_hash = md5_hash.hexdigest()\n        # Generate path for the hashed file\n        hashed_file_path = os.path.", "compilation_passed": false, "time": 0.0002200603485107422, "tests_passed": false, "error": "SyntaxError"}
{"code": "try:\n        os.makedirs(target_dir, exist_ok=True)\n    except FileExistsError:\n        pass\n\n    target_files = []\n\n    for filename in glob.glob(os.path.join(source_dir, '*')):\n        with open(filename, 'rb') as f:\n            data = f.read()\n            md5 = hashlib.md5(data).hexdigest()\n            hash_str = f'{prefix}{md5}'\n\n            new_filename = f'{md5}.txt'\n            new_filepath = os.path.join(target_dir, new_filename)\n\n            with open(new_filepath, 'w') as new_f:\n                new_f.write(hash_str + '\\n')\n                new_f.write(data.decode('utf-8'))\n\n            target_files.append(new_filepath)\n\n    return target_files", "compilation_passed": true, "time": 0.0038301944732666016, "tests_passed": true, "error": null}
