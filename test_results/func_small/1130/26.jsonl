{"code": "path_obj = Path(directory)\n    path_obj.mkdir(exist_ok=True)\n\n    path_obj = Path(directory)\n    files = path_obj.rglob('*')\n\n    hash_map = {}\n    for f in files:\n        hash_map[f.name] = hashlib.sha256(f.read_bytes()).hexdigest()\n\n    with open(os.path.join(directory, \"hashes.json\"), \"w\") as f:\n        json.dump(hash_map, f, indent=2)\n\n    return os.path.join(directory, \"hashes.json\")", "compilation_passed": true, "time": 0.001931905746459961, "tests_passed": false, "error": "KeyError"}
{"code": "directory = Path(directory)\n    json_file = str(directory / \"hashes.json\")\n    hash_file = hashlib.sha256()\n    hashes = {}\n    for file in directory.rglob(\"*\"):\n        if file.is_file():\n            with open(file, \"rb\") as f:\n                for chunk in f:\n                    hash_file.update(chunk)\n            hashes[file.name] = hash_file.hexdigest()\n    with open(json_file, \"w\") as f:\n        json.dump(hashes, f)\n    return json_file", "compilation_passed": true, "time": 0.001863718032836914, "tests_passed": false, "error": "KeyError"}
{"code": "hashes: Dict[str, List[str]] = {}\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            filepath = os.path.join(root, file)\n            with open(filepath, \"rb\") as fp:\n                hash_value = hashlib.sha256(fp.read()).hexdigest()\n                hashes.setdefault(file, []).append(hash_value)\n\n    json_file = Path(directory) / \"hashes.json\"\n    with open(json_file, \"w\") as fp:\n        json.dump(hashes, fp)\n\n    return str(json_file)", "compilation_passed": true, "time": 0.0017287731170654297, "tests_passed": false, "error": "KeyError"}
{"code": "file_path = Path(directory) / \"hashes.json\"\n\n    hashes = {}\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(root, file)\n            file_hash = hashlib.sha256(open(file_path, 'rb').read()).hexdigest()\n            hashes[file_path] = file_hash\n    with open(file_path, \"w\") as f:\n        json.dump(hashes, f)\n\n    return str(file_path)", "compilation_passed": true, "time": 0.001753091812133789, "tests_passed": true, "error": null}
{"code": "json_file = None\n    directory = Path(directory)\n    if not directory.exists():\n        return json_file\n\n    hash_dict = {}\n\n    for file in directory.iterdir():\n        if file.is_file():\n            hash_dict[file.name] = get_file_hash(file)\n\n    with open(directory / \"hashes.json\", \"w\") as f:\n        json_file = directory / \"hashes.json\"\n        json.dump(hash_dict, f, indent=4)\n\n    return str(json_file)", "compilation_passed": true, "time": 0.0014698505401611328, "tests_passed": false, "error": "NameError"}
{"code": "file_hashes = {}\n\n    for path, dirs, files in os.walk(directory):\n        for file in files:\n            full_path = os.path.join(path, file)\n            hash = hashlib.sha256()\n            with open(full_path, 'rb') as f:\n                for chunk in iter(lambda: f.read(4096), b''):\n                    hash.update(chunk)\n            file_hashes[os.path.relpath(full_path, directory)] = hash.hexdigest()\n    json.dump(file_hashes, open(os.path.join(directory, 'hashes.json'), 'w'))\n    return os.path.join(directory, 'hashes.json')", "compilation_passed": true, "time": 0.0017080307006835938, "tests_passed": false, "error": "KeyError"}
{"code": "file_hashes: dict[str, str] = {}\n    path = Path(directory)\n    for dir_path, subdirs, files in os.walk(path):\n        for file in files:\n            abs_path = os.path.join(dir_path, file)\n            with open(abs_path, \"rb\") as f:\n                hash_object = hashlib.sha256()\n                hash_object.update(f.read())\n                hash_value = hash_object.hexdigest()\n                file_hashes[file] = hash_value\n    json_file = os.path.join(directory, \"hashes.json\")\n    with open(json_file, \"w\") as f:\n        json.dump(file_hashes, f, indent=2)\n    return json_file", "compilation_passed": true, "time": 0.0017058849334716797, "tests_passed": false, "error": "KeyError"}
{"code": "directory = Path(directory)\n    files = list(directory.rglob(\"*\"))\n    hashes = []\n    for file in files:\n        file_hash = hashlib.sha256()\n        with open(file, \"rb\") as f:\n            while True:\n                chunk = f.read(4096)\n                if not chunk:\n                    break\n                file_hash.update(chunk)\n        hashes.append((str(file), file_hash.hexdigest()))\n    hash_dict = dict(hashes)\n    hash_file = \"hashes.json\"\n    json_file = directory.joinpath(hash_file)\n    with open(json_file, \"w\") as f:\n        json.dump(hash_dict, f, indent=4)\n    print(f\"Hashes saved at: {json_file}\")\n    return str(json_file)", "compilation_passed": true, "time": 0.0033516883850097656, "tests_passed": false, "error": "IsADirectoryError"}
{"code": "# create a Path instance from the given directory path\n    directory_path = Path(directory)\n\n    # create an empty dictionary to store hashes\n    hashes = {}\n\n    # check if the given path is a valid directory\n    if not directory_path.is_dir():\n        raise ValueError(\"Given path is not a directory\")\n\n    # loop through all files in the directory and create a hash for each file\n    for file_path in directory_path.rglob(\"*\"):\n        if not file_path.is_file():\n            continue  # skip directories\n\n        # read the file\n        with open(file_path, \"rb\") as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n            hashes[file_path.name] = file_hash\n\n    # write the hashes to a JSON file\n    json_file_path = directory_path / \"hashes.json\"\n    with open(json_file_path, \"w\") as f:\n        json.dump(hashes, f, indent=4", "compilation_passed": false, "time": 0.00021409988403320312, "tests_passed": false, "error": "SyntaxError"}
{"code": "path_to_save = Path(directory) / \"hashes.json\"\n    hashes_dict = {}\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for f in filenames:\n            with open(os.path.join(dirpath, f), 'rb') as fp:\n                hashes_dict[os.path.join(dirpath, f)] = hashlib.sha256(fp.read()).hexdigest()\n\n    with open(path_to_save, 'w', encoding='utf-8') as f:\n        json.dump(hashes_dict, f, indent=4, sort_keys=True, ensure_ascii=False)\n\n    return path_to_save", "compilation_passed": true, "time": 0.0038099288940429688, "tests_passed": true, "error": null}
{"code": "path = Path(directory)\n\n    with path.joinpath(\"hashes.json\").open(\"w\") as file:\n        hashes = {\n            str(path.resolve()): hashlib.sha256(str(path.resolve()).encode()).hexdigest()\n        }\n        for subdirectory in path.rglob(\"*\"):\n            if not subdirectory.is_dir():\n                hashes[str(subdirectory.resolve())] = hashlib.sha256(str(subdirectory.resolve()).encode()).hexdigest()\n\n        json.dump(hashes, file, indent=4)\n        return str(path.joinpath(\"hashes.json\"))", "compilation_passed": true, "time": 0.0019998550415039062, "tests_passed": false, "error": "KeyError"}
{"code": "# Define a list to store all the files to be hashed\n    files_to_hash = []\n\n    # Recursively traverse all the files in the directory\n    for dirpath, dirnames, filenames in os.walk(directory):\n        # Add the files to the list\n        files_to_hash.extend([os.path.join(dirpath, file) for file in filenames])\n\n    # Sort the files by path\n    files_to_hash.sort()\n\n    # Convert the paths to file paths\n    file_paths = [Path(file) for file in files_to_hash]\n\n    # Generate the hashes for each file\n    hashes = {str(file): hashlib.sha256(file.read_bytes()).hexdigest() for file in file_paths}\n\n    # Write the hashes to a JSON file\n    with open(Path(directory, \"hashes.json\"), \"w\") as json_file:\n        json.dump(hashes, json_file)\n\n    # Return the path of the JSON file\n    return str(Path", "compilation_passed": false, "time": 0.00020599365234375, "tests_passed": false, "error": "SyntaxError"}
{"code": "directory = Path(directory)\n    hashes = {}\n\n    for path in directory.rglob(\"*\"):\n        if path.is_file():\n            with open(path, \"rb\") as file:\n                hashes[path] = hashlib.sha256(file.read()).hexdigest()\n\n    json_file = directory / \"hashes.json\"\n    with json_file.open(\"w\") as file:\n        json.dump(hashes, file, indent=2)\n\n    return str(json_file)", "compilation_passed": true, "time": 0.0017621517181396484, "tests_passed": false, "error": "TypeError"}
{"code": "directory = Path(directory)\n    with directory.joinpath(\"hashes.json\").open(\"w\") as file:\n        json.dump(\n            {\n                filename.stem: hashlib.sha256(\n                    filename.read_bytes()\n                ).hexdigest()\n                for filename in sorted(directory.iterdir())\n                if not filename.is_dir()\n            },\n            file,\n            indent=4,\n        )\n    return directory.joinpath(\"hashes.json\").resolve()", "compilation_passed": true, "time": 0.0017652511596679688, "tests_passed": false, "error": "KeyError"}
{"code": "result = []\n    for file in Path(directory).rglob('*'):\n        if file.is_dir():\n            continue\n        try:\n            with open(file, 'rb') as f:\n                h = hashlib.sha256(f.read()).hexdigest()\n                result.append({'filename': str(file), 'hash': h})\n        except PermissionError:\n            continue\n    with open(Path(directory) / 'hashes.json', 'w') as f:\n        json.dump(result, f, indent=4)\n    return os.path.abspath(Path(directory) / 'hashes.json')", "compilation_passed": true, "time": 0.0017969608306884766, "tests_passed": false, "error": "TypeError"}
{"code": "with os.scandir(directory) as entries:\n        hashes = {}\n        for entry in entries:\n            if entry.is_file():\n                with open(entry.path, \"rb\") as f:\n                    file_hash = hashlib.sha256(f.read()).hexdigest()\n                    hashes[entry.name] = file_hash\n        with open(directory + \"/hashes.json\", \"w\") as f:\n            json.dump(hashes, f)\n\n    return directory + \"/hashes.json\"", "compilation_passed": true, "time": 0.0016951560974121094, "tests_passed": false, "error": "KeyError"}
{"code": "with os.scandir(directory) as d:\n        for entry in d:\n            if entry.is_dir():\n                task_func(entry.path)\n            elif entry.is_file():\n                with open(entry.path, \"rb\") as f:\n                    print(hashlib.sha256(f.read()).hexdigest())", "compilation_passed": true, "time": 0.001432180404663086, "tests_passed": false, "error": "TypeError"}
{"code": "directory = os.path.abspath(directory)\n    path = Path(directory)\n    json_file = path / 'hashes.json'\n    hashes: dict[str, str] = {}\n\n    for file in path.rglob('*'):\n        if file.is_file():\n            hashes[file.name] = hashlib.sha256(\n                file.read_bytes()).hexdigest()\n\n    with json_file.open(mode='w', encoding='utf-8') as f:\n        json.dump(hashes, f, sort_keys=True, indent=4)\n\n    return os.path.abspath(str(json_file))", "compilation_passed": true, "time": 0.0017211437225341797, "tests_passed": false, "error": "KeyError"}
{"code": "# Create a list to store all hash values\n    hash_list = []\n\n    # Get the absolute path of the given directory\n    directory = Path(directory).absolute()\n\n    # Create a dictionary to store the hash values\n    hashes = {}\n\n    # Iterate through all files in the given directory and subdirectories\n    for file in directory.rglob(\"*\"):\n        # Skip files with a . directory file extension\n        if os.path.isfile(file):\n            # Calculate the MD5 hash of the file and append it to the list\n            md5_hash = hashlib.md5()\n            with open(file, \"rb\") as file_obj:\n                for chunk in iter(lambda: file_obj.read(4096), b\"\"):\n                    md5_hash.update(chunk)\n            hash_list.append(md5_hash.hexdigest())\n            hashes[file.name] = md5_hash.hexdigest()\n\n    # Create a JSON file with the hash values\n    json_file = directory / 'hashes.json", "compilation_passed": false, "time": 8.58306884765625e-05, "tests_passed": false, "error": "SyntaxError"}
{"code": "json_file = Path(directory) / 'hashes.json'\n\n    # check if the directory is a valid directory\n    if not Path(directory).is_dir():\n        raise ValueError(f\"The path '{directory}' does not specify an existing directory.\")\n\n    # check if the file 'hashes.json' exists and if it does, delete it\n    if json_file.exists():\n        json_file.unlink()\n\n    # check if the file 'hashes.json' exists and if it does, delete it\n    if json_file.exists():\n        json_file.unlink()\n\n    # create a directory for storing the hash files\n    if not Path(directory).joinpath('hashes').is_dir():\n        Path(directory).joinpath('hashes').mkdir()\n\n    # create a directory for storing the hash files\n    if not Path(directory).joinpath('hashes').is_dir():\n        Path(directory).joinpath('hashes').mkdir()\n\n    # create a dictionary to store hashes\n    hashes", "compilation_passed": true, "time": 0.0016469955444335938, "tests_passed": false, "error": "NameError"}
{"code": "directory = Path(directory)\n    if not directory.is_dir():\n        raise ValueError(f\"Directory '{directory}' does not exist or is not a directory.\")\n\n    hashes = {}\n\n    for subdir, subdirs, files in os.walk(directory):\n        for f in files:\n            path = os.path.join(subdir, f)\n            with open(path, 'rb') as file:\n                hashes[path] = hashlib.sha256(file.read()).hexdigest()\n\n    json_file = f\"{directory.parent.resolve()}/hashes.json\"\n    with open(json_file, 'w') as file:\n        json.dump(hashes, file, indent=4)\n\n    return json_file", "compilation_passed": true, "time": 0.0036852359771728516, "tests_passed": true, "error": null}
{"code": "directory = Path(directory)\n    if not directory.is_dir():\n        raise ValueError(f\"'{directory}' is not a directory.\")\n\n    file_hashes = {}\n    for dir_path, dir_names, file_names in os.walk(directory):\n        for filename in file_names:\n            file_path = os.path.join(dir_path, filename)\n            with open(file_path, \"rb\") as f:\n                file_hashes[filename] = hashlib.sha256(f.read()).hexdigest()\n\n    with open(directory / \"hashes.json\", \"w\") as f:\n        json.dump(file_hashes, f, indent=4)\n\n    return directory / \"hashes.json\"", "compilation_passed": true, "time": 0.001802206039428711, "tests_passed": false, "error": "KeyError"}
{"code": "hashes = {}\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            path = os.path.join(dirpath, filename)\n            with open(path, \"rb\") as f:\n                hashes[filename] = hashlib.sha256(f.read()).hexdigest()\n\n    json_file = Path(directory) / \"hashes.json\"\n    with open(json_file, \"w\") as f:\n        json.dump(hashes, f, indent=4)\n    return str(json_file)", "compilation_passed": true, "time": 0.0016758441925048828, "tests_passed": false, "error": "KeyError"}
{"code": "hashes = {}\n    for dir_path, subdirs, files in os.walk(directory):\n        for file in files:\n            file_path = os.path.join(dir_path, file)\n            with open(file_path, 'rb') as f:\n                file_hash = hashlib.sha256(f.read()).hexdigest()\n                hashes[file] = file_hash\n\n    # Write the hashes to the JSON file\n    json_file = Path(directory).joinpath(\"hashes.json\")\n    with open(json_file, 'w') as f:\n        json.dump(hashes, f)\n\n    return str(json_file)", "compilation_passed": true, "time": 0.0016999244689941406, "tests_passed": false, "error": "KeyError"}
{"code": "json_file = Path(directory) / \"hashes.json\"\n\n    if not json_file.exists():\n        os.makedirs(directory, exist_ok=True)\n        hashes = {}\n        for root, _, files in os.walk(directory):\n            for file in files:\n                file_path = os.path.join(root, file)\n                hash_obj = hashlib.sha256()\n                with open(file_path, 'rb') as f:\n                    for chunk in iter(lambda: f.read(4096), b\"\"):\n                        hash_obj.update(chunk)\n                hashes[file] = hash_obj.hexdigest()\n        json_file.write_text(json.dumps(hashes))\n    return str(json_file)", "compilation_passed": true, "time": 0.0017578601837158203, "tests_passed": false, "error": "KeyError"}
{"code": "# create a list to store the paths of files to be hashed\n    paths_list = []\n\n    # get the absolute path of the directory and convert it to a Path object\n    directory = Path(directory)\n\n    # recursively iterate through all files and subdirectories in the given directory\n    for path in directory.rglob(\"*\"):\n        # if the path is a file, append it to the paths_list\n        if path.is_file():\n            paths_list.append(path)\n\n    # create a JSON file object\n    json_file = Path(directory, \"hashes.json\")\n    # create a JSON object with the file name as the key\n    json_data = {\"file_name\": json_file.name}\n\n    # if the directory already contains a hashes.json file, delete it\n    if json_file.exists():\n        os.remove(json_file)\n\n    # create a list of the SHA256 hashes of the files to be hashed\n    hash_list = []\n\n    # create an empty list to store all hashes\n    has", "compilation_passed": true, "time": 0.0015590190887451172, "tests_passed": false, "error": "NameError"}
{"code": "# create a JSON file with the name 'hashes.json'\n    hashes_file = Path(directory).joinpath(\"hashes.json\")\n    with hashes_file.open(mode=\"w\") as f:\n        json.dump([], f)\n\n    # create a dictionary that will contain hashes and their file names\n    hashes = {}\n\n    # traverse the directory recursively\n    for path, dirs, files in os.walk(directory):\n        # ignore directories starting with '.'\n        dirs[:] = [d for d in dirs if not d.startswith('.')]\n\n        # iterate over all files in the directory\n        for f in files:\n            if f.startswith('.'):\n                continue  # skip files starting with '.'\n\n            # calculate the SHA256 hash of the file\n            hash_ = hashlib.sha256()\n            with open(os.path.join(path, f), \"rb\") as file:\n                for chunk in iter(lambda: file.read(4096", "compilation_passed": false, "time": 0.0002079010009765625, "tests_passed": false, "error": "SyntaxError"}
{"code": "dir_path = Path(directory)\n\n    hash_dict = {}\n\n    for dir_path, _, filenames in os.walk(directory):\n        for filename in filenames:\n            file_path = os.path.join(dir_path, filename)\n\n            # Compute hash for the file\n            with open(file_path, \"rb\") as file:\n                hash_dict[file_path] = hashlib.sha256(file.read()).hexdigest()\n\n    # Write hashes to JSON file\n    hashes_json_file = Path(dir_path, \"hashes.json\")\n    with open(hashes_json_file, \"w\") as file:\n        json.dump(hash_dict, file, indent=4)\n\n    return str(hashes_json_file)", "compilation_passed": true, "time": 0.003905057907104492, "tests_passed": true, "error": null}
{"code": "file_list = []\n\n    # get all files in directory\n    for root, _, files in os.walk(directory):\n        for file in files:\n            file_list.append(os.path.join(root, file))\n\n    # hash files using SHA256\n    hasher = hashlib.sha256()\n    for file in file_list:\n        with open(file, 'rb') as f:\n            hasher.update(f.read())\n        filename = os.path.basename(file)\n        hashes[filename] = hasher.hexdigest()\n\n    # save hashes to json file\n    with open(\"hashes.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(hashes, f, ensure_ascii=False, indent=2)\n        return \"hashes.json\"", "compilation_passed": true, "time": 0.001641988754272461, "tests_passed": false, "error": "NameError"}
{"code": "directory_path = Path(directory)\n\n    # Ensure the given directory path exists and is a valid directory\n    if not directory_path.is_dir():\n        raise ValueError(f\"Invalid directory: {directory}\")\n\n    # Create a list to store hashed files\n    hashed_files = []\n\n    # Iterate over all files and subdirectories\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Calculate the file path relative to the directory\n            file_path = Path(root) / file\n\n            # Skip the file if it is a directory\n            if file_path.is_dir():\n                continue\n\n            # Calculate the SHA256 hash of the file\n            with open(file_path, \"rb\") as f:\n                hash_ = hashlib.sha256(f.read())\n                hashed_files.append((hash_.hexdigest(), str(file_path)))\n\n    # Save the hashed files as JSON\n    with open(directory_path / \"hashes.json\", \"w\"", "compilation_passed": false, "time": 0.00021505355834960938, "tests_passed": false, "error": "SyntaxError"}
