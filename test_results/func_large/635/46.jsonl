{"code": "# Check input type\n    if not isinstance(text, str):\n        raise TypeError(\"text must be a string\")\n\n    # Check input value\n    if n < 2 or n > 10:\n        raise ValueError(\"n must be between 2 and 10\")\n\n    # Import the required libraries\n    stop_words = stopwords.words()\n\n    # Remove duplicate words and stopwords from text\n    text = re.sub(r'(\\b\\w+\\b)(\\s+\\1)+', r'\\1', text.strip())\n    text_tokens = [w for w in text.split(' ') if w not in stop_words]\n\n    # Generate n-grams from tokens\n    ngrams = zip(*[text_tokens[i:] for i in range(n)])\n\n    # Vectorize the n-grams\n    vectorizer = CountVectorizer()\n    counts = vectorizer.fit_transform(ngrams)\n\n    # Create a square matrix of co-occurrences\n    matrix = np.array(pd.DataFrame(counts.toarray(), columns=", "compilation_passed": false, "time": 0.00027108192443847656, "tests_passed": false, "error": "SyntaxError"}
{"code": "# Tokenizing the text\n    text = re.sub(r'[^a-zA-Z0-9 ]', ' ', text)\n    text = text.lower()\n    tokens = [t for t in text.split()]\n\n    # Removing the duplicates\n    tokens = [t for i, t in enumerate(tokens) if i == 0 or tokens[i - 1] != t]\n\n    # Removing the stop words\n    stop_words = set(stopwords.words(\"english\"))\n    tokens = [t for t in tokens if t not in stop_words]\n\n    # Generating the square co-occurrence matrix\n    vectorizer = CountVectorizer(ngram_range=(n, n), binary=True)\n    counts = vectorizer.fit_transform(tokens)\n\n    # Creating the dataframe and plotting\n    co_occurrence = pd.DataFrame(counts.toarray(),\n                                index=vectorizer.get_feature_names(),\n                                columns=vectorizer.get_feature_names())\n    co_occur", "compilation_passed": true, "time": 0.0029571056365966797, "tests_passed": false, "error": "ValueError"}
{"code": "# Replace all consecutive whitespaces with a single space\n    # and convert all characters to lowercase\n    text = re.sub(r'\\s{2,}', ' ', text).lower()\n    # Initialize an empty set of stopwords\n    stop = set(stopwords.words('english'))\n    # Initialize the vectorizer with the maximum n-gram length of n\n    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words=stop, max_df=1.0, min_df=1)\n    # Fit the vectorizer on the text, which creates a vocabulary of words and their counts\n    vectorizer.fit([text])\n    # Extract the feature names, which are the vocabulary of words\n    feature_names = vectorizer.get_feature_names_out()\n    # Create an empty DataFrame with the feature names as columns and indices\n    df = pd.DataFrame(columns=feature_names, index=feature_names)\n    # Initialize an empty co-occurrence matrix\n    cooccurrences = np.zeros((len(feature_names), len(feature_names))", "compilation_passed": false, "time": 0.0001900196075439453, "tests_passed": false, "error": "SyntaxError"}
{"code": "# Removing duplicate consecutive words and stopwords defined by nltk.corpus\n    text = re.sub(r'(\\b\\w+)( \\1)+', r'\\1', text)\n    stop_words = stopwords.words('english')\n    words = text.lower().split(' ')\n    text = ' '.join([word for word in words if word not in stop_words])\n\n    # Generating a square co-occurrence matrix of words\n    cv = CountVectorizer(ngram_range=(n, n))\n    X = cv.fit_transform([' '.join(t) for t in cv.build_ngram_analyzer()(text)]).toarray()\n    co_occurrence_matrix = np.dot(X, X.T)\n    co_occurrence_matrix = pd.DataFrame(co_occurrence_matrix, index=cv.get_feature_names_out())\n\n    # Plotting the co-occurrence matrix\n    fig, ax = plt.subplots(figsize=(7, 7))\n    plt.title('", "compilation_passed": false, "time": 9.608268737792969e-05, "tests_passed": false, "error": "SyntaxError"}
{"code": "# removing duplicate consecutive words\n    text = re.sub(r'(\\b(\\w+)\\b\\s+)+\\b\\2\\b', r'\\2', text)\n\n    # removing stopwords defined by nltk.corpus\n    text = ' '.join(\n        [word for word in text.split() if word not in stopwords.words('english')]\n    )\n\n    # generating n-grams\n    cv = CountVectorizer(ngram_range=(n, n))\n\n    # fitting and transforming\n    count_data = cv.fit_transform([text]).toarray()\n\n    # generating square co-occurrence matrix of words\n    df = pd.DataFrame(count_data, columns=cv.get_feature_names_out())\n\n    # plotting co-occurrence matrix\n    ax = df.iloc[0].plot.bar())\n\n    return df, ax", "compilation_passed": false, "time": 7.581710815429688e-05, "tests_passed": false, "error": "SyntaxError"}
{"code": "# Removing duplicate consecutive words using regular expressions\n    pattern = r'\\b(\\w+)\\b(?=.*\\b\\1\\b)'\n    text_unique_words = re.sub(pattern, '', text)\n\n    # Removing stopwords from the text using nltk.corpus\n    stop_words = set(stopwords.words('english'))\n    tokens = text_unique_words.split()\n    cleaned_words = [word for word in tokens if word.lower() not in stop_words]\n    words = \" \".join(cleaned_words)\n\n    # Generate n-grams (n = 2 by default) using CountVectorizer from sklearn\n    cv = CountVectorizer(ngram_range=(n, n), token_pattern=r'\\b\\w+\\b')\n    ngrams_matrix = cv.fit_transform([words]).toarray()\n\n    # Extract the unique n-grams (non-zero values) and their frequencies\n    ngrams = cv.get_feature_names_out()\n    ngrams_freq = ngrams_matrix.flatten()\n    ng", "compilation_passed": true, "time": 0.0011670589447021484, "tests_passed": false, "error": "NameError"}
{"code": "# Convert text to lowercase and tokenize it into a list of words\n    text = text.lower()\n    words = text.split()\n\n    # Remove stop words from the list of words using nltk.corpus\n    stop_words = set(stopwords.words())\n    words = [word for word in words if word not in stop_words]\n\n    # Create a list of n-grams using the words from the text\n    ngrams = [\" \".join(words[i:i + n]) for i in range(len(words) - n + 1)]\n\n    # Create a set of unique words from the n-grams\n    unique_words = set(re.findall(r'\\b\\w+\\b', \" \".join(ngrams)))\n\n    # Initialize an empty count matrix for the unique words\n    count_matrix = np.zeros((len(unique_words), len(unique_words)))\n\n    # Convert the n-grams to vectors of binary values to indicate the presence of each word in each n-gram\n    vectorizer = CountVectorizer(binary=True, token_pattern=", "compilation_passed": false, "time": 0.00021195411682128906, "tests_passed": false, "error": "SyntaxError"}
{"code": "text = re.sub(r\"(\\b\\w+)( \\1)+\", r\"\\1\", text)\n    cv = CountVectorizer(\n        ngram_range=(n, n),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        stop_words=stopwords.words(\"english\"),\n        vocabulary=None,\n        binary=True,\n        dtype=int,\n    )\n    mat = cv.fit_transform([text]).toarray().flatten()\n\n    # create square matrix from vector\n    n_grams = (\n        cv.build_analyzer()(\" \".join(cv.get_feature_names())) if mat.size else [\" \"]\n    )\n    mat = np.reshape(mat, (len(n_grams), -1))\n\n    # plot the matrix\n    ax = plt.axes() if mat.size else ax\n    df = pd.DataFrame(mat)\n    ax.set_xticks(np.arange(0, len(n", "compilation_passed": false, "time": 0.0002319812774658203, "tests_passed": false, "error": "SyntaxError"}
{"code": "if not isinstance(text, str):\n        raise ValueError(\"Input should be a string\")\n    if not isinstance(n, int):\n        raise ValueError(\"n should be an integer\")\n    if n < 1:\n        raise ValueError(\"n should be an integer >= 1\")\n\n    # Replace punctuation characters with space, and remove duplicate consecutive whitespace characters\n    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    # Tokenize text and remove stopwords\n    tokenized_text = text.lower().split()\n    tokenized_text = [w for w in tokenized_text if w not in stopwords.words('english')]\n\n    # Remove duplicate consecutive words\n    new_text = []\n    for i in range(1, len(tokenized_text)):\n        if tokenized_text[i] != tokenized_text[i-1]:\n            new_text.append(tokenized_text[i])\n\n", "compilation_passed": true, "time": 0.0010561943054199219, "tests_passed": false, "error": "TypeError"}
{"code": "text = re.sub(r'(\\b\\w+\\b)(\\s+\\1)+', r'\\1', text.lower())  # Removes consecutive duplicates\n    text = \" \".join(re.sub(r'\\b(' + r'|'.join(stopwords.words(\"english\")) + r')\\b\\s*', r'', text).split())  # Removes stopwords\n    vec = CountVectorizer(ngram_range=(n, n)).fit(np.array([text]))\n    # Creates a count vectorizer which creates n-grams (as many words in a row)\n    # In the following we create a DataFrame of the n-grams and the count of each of them.\n    # The n-gram itself is a row and a column.\n    df = pd.DataFrame(vec.transform(np.array([text])).toarray(), columns=vec.get_feature_names())\n    # Creating the plot object using matplotlib's imshow function.\n    # Note: This returns a tuple of the plot object and the image object. We only care about the plot object.", "compilation_passed": true, "time": 0.001764059066772461, "tests_passed": false, "error": "AttributeError"}
{"code": "# Remove duplicate consecutive words using regular expressions\n    text = re.sub(r'(\\b\\w+)\\s+\\1', r'\\1', text)\n\n    # Convert the text to lowercase and split it into a list of words\n    words = text.lower().split()\n\n    # Remove stopwords using the NLTK corpus\n    words = [word for word in words if word not in stopwords.words()]\n\n    # Generate the n-grams using a sliding window of size n\n    ngrams = [(words[i:i+n], i) for i in range(len(words)-n+1)]\n\n    # Create a list of co-occurrence pairs based on the sliding window\n    pairs = [(ngram, ngrams[i+n][0]) for ngram, i in ngrams if i+n < len(ngrams)]\n\n    # Create a list of unique words\n    unique_words = list(set([word for pair in pairs for ngram in pair for word in ngram]))\n\n    # Create a dictionary that maps words to indices\n    word2index = {word: i", "compilation_passed": false, "time": 0.00022482872009277344, "tests_passed": false, "error": "SyntaxError"}
{"code": "# Cleaning the text by removing punctuation and special characters\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    text = re.sub(r'\\n', ' ', text)\n\n    # Remove duplicate consecutive words\n    text = re.sub(r'\\b(\\w+)\\b\\s+(?=\\1\\b)', '', text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = text.split()\n    filtered_words = [word for word in words if word not in stop_words]\n\n    # Generating the square co-occurrence matrix\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    X = vectorizer.fit_transform(filtered_words)\n    cooccurrence_matrix = X.T.dot(X).toarray()\n    cooccurrence_df = pd.DataFrame(cooccurrence_matrix, index=vectorizer.get_feature_names_out(), columns=vectorizer.get_feature_names_", "compilation_passed": false, "time": 0.00020575523376464844, "tests_passed": false, "error": "SyntaxError"}
{"code": "# Removing duplicate consecutive words\n    text = re.sub(r'\\b(\\w+)\\b(?=.*\\b\\1\\b)', '', text)\n\n    # Removing stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in text.split() if token.lower() not in stop_words]\n    text = ' '.join(tokens)\n\n    # Generating the co-occurrence matrix\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    X = vectorizer.fit_transform([text])\n    df = pd.DataFrame(X.toarray()[0], index=vectorizer.get_feature_names_out(), columns=['Count'])\n    df = df.drop(df.index[0])  # drop the first row\n    co_occurrence_matrix = df.pivot_table(index=df.index, columns=df.index, values='Count', aggfunc='sum')\n\n    # Plotting the matrix\n    co_occurrence_matrix_plot =", "compilation_passed": false, "time": 0.00020599365234375, "tests_passed": false, "error": "SyntaxError"}
{"code": "# Cleaning and removing duplicate words\n    text = re.sub(r'\\b(\\w+)\\b(?=.*\\1\\b)', \"\", text)\n    # Cleaning and removing stopwords\n    text = re.sub(r'[^\\w\\s]', ' ', text)\n    text = re.sub(r'\\b\\w{1,3}\\b\\s*', ' ', text)\n    text = re.sub(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*', '', text)\n\n    # Vectorizing the text using n-grams\n    vectorizer = CountVectorizer(ngram_range=(n, n))\n    vectorized_text = vectorizer.fit_transform([text])\n    vectorized_text = pd.DataFrame(vectorized_text.toarray(), columns=vectorizer.get_feature_names_out())\n\n    # Creating the co-occurrence matrix\n    co_occurrence_matrix = vectorized_text.T @ vectorized_text\n\n    # Plotting", "compilation_passed": true, "time": 0.0010101795196533203, "tests_passed": false, "error": "ValueError"}
{"code": "# Define the n-grams we are interested in (using regex).\n    # Note that the regex is different for 1-grams and n-grams (when n>1)\n    if n == 1:\n        pattern = r'\\b(\\w+)\\b'\n        n_gram_words = re.findall(pattern, text)\n    else:\n        pattern = r'\\b(\\w+\\s' + r'\\w+\\s'.join(['' for _ in range(n - 1)]) + r')\\b'\n        n_gram_words = re.findall(pattern, text)\n\n    # Remove stopwords and duplicate n-grams\n    # We define a function to do this, as we will need to do this for both the training and test sets.\n    # Note that we remove stopwords here, as doing it later will result in a matrix with a lot of zero rows\n    # and zero columns, which will result in a matrix with very little colour.\n    stop_words = stopwords.words('english')\n    filtered_n_grams = []\n    for n_gram in n_gram_words:\n", "compilation_passed": false, "time": 0.00014591217041015625, "tests_passed": false, "error": "IndentationError"}
{"code": "text = re.sub(r'(\\b\\w+) \\1+', r'\\1', text) # Removing duplicate consecutive words in text\n    stop_words = set(stopwords.words(\"english\")) # Getting the list of stopwords from nltk library\n    text_words = re.findall(r'\\b\\w+\\b', text) # Finding all words in the text\n    words = [word for word in text_words if word not in stop_words] # Removing stopwords from text\n    vectorizer = CountVectorizer(ngram_range=(n,n)) # CountVectorizer object to create the co-occurrence matrix\n    X = vectorizer.fit_transform(words) # Fitting the text to the vectorizer and generating the matrix\n    ngram_col_names = vectorizer.get_feature_names_out() # Generating the column names of the co-occurrence matrix\n    df = pd.DataFrame(X.toarray(), columns=ngram_col_names, index=ngram_col_names).astype(int) # Creating dataframe of the co-", "compilation_passed": true, "time": 0.0009832382202148438, "tests_passed": false, "error": "ValueError"}
{"code": "# Remove consecutive duplicate words\n    text = re.sub(r'(\\b[a-zA-Z\\']+\\b)( +\\1\\b)+', r'\\1', text)\n    # Remove stopwords\n    stop = stopwords.words('english')\n    text = ' '.join([word for word in text.split() if word not in stop])\n    # Create co-occurrence matrix\n    cv = CountVectorizer(ngram_range=(n, n))\n    count_matrix = cv.fit_transform([text]).toarray()[0]\n    df = pd.DataFrame(count_matrix, columns=cv.get_feature_names())\n    # Plot matrix\n    ax = df.plot.pcolor(cmap='Blues')\n    ax.set_yticklabels(df.index, minor=False)\n    ax.set_xticklabels(df.columns, minor=False)\n    return df, ax", "compilation_passed": true, "time": 0.001062154769897461, "tests_passed": false, "error": "AttributeError"}
{"code": "# Remove duplicate consecutive words and stopwords\n    text_list = re.sub(r\"\\s+\", \" \", text.strip()).split()\n    stop_words = set(stopwords.words(\"english\"))\n    text_list_filtered = [\n        word for word in text_list if word not in stop_words\n    ]  # Remove stopwords\n    text_filtered = \" \".join(text_list_filtered)  # Join words back into a string\n\n    # Generate co-occurrence matrix\n    cv = CountVectorizer(ngram_range=(n, n))\n    # Convert text to a data frame of n-grams\n    text_series = pd.Series([text_filtered])\n    text_df = cv.fit_transform(text_series).todense()\n    # Extract the column names (n-grams) and convert to a data frame\n    col_names = cv.get_feature_names_out()\n    df = pd.DataFrame(text_df, columns=col_names)\n    # Convert values to integers\n    df = df.astype(", "compilation_passed": false, "time": 0.00019407272338867188, "tests_passed": false, "error": "SyntaxError"}
{"code": "# removing duplicated consecutive words and stopwords\n    text_wo_duplicates = remove_duplicates(text)\n    # replacing special symbols with a space\n    text_wo_duplicates_wo_symb = re.sub(r'[^\\w\\s]', ' ', text_wo_duplicates)\n    # vectorize text\n    vectorizer = CountVectorizer(ngram_tokenizer(n=n), preprocessor=preprocessor)\n    vectorizer.fit_transform([text_wo_duplicates_wo_symb])\n    # make square matrix\n    words = vectorizer.get_feature_names_out()\n    matrix = np.zeros((len(words), len(words)), dtype=int)\n    for i, word in enumerate(words):\n        for j, word2 in enumerate(words):\n            pattern = re.compile(rf\"(\\b{word}\\b).*\\b{word2}\\b\", re.IGNORECASE | re.DOTALL)\n            pattern_reverse = re.compile(rf\"(\\b{word2}\\b).*\\b{word", "compilation_passed": false, "time": 9.703636169433594e-05, "tests_passed": false, "error": "SyntaxError"}
{"code": "# removing punctuation marks\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # removing duplicate consecutive words\n    text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text)\n    # removing stopwords\n    text = text.lower()\n    stop_words = set(stopwords.words('english'))\n    filtered_text = ' '.join([word for word in text.split() if word not in stop_words])\n    # generate the square co-occurrence matrix of words\n    cv = CountVectorizer(ngram_range=(n, n), token_pattern=r'\\b\\w+\\b')\n    co_occurrence_matrix = cv.fit_transform([filtered_text]).toarray()\n    co_occurrence_df = pd.DataFrame(co_occurrence_matrix, columns=cv.get_feature_names_out(), index=cv.get_feature_names_out())\n    # plot this matrix\n    fig, ax = plt.subplots(figsize=(1", "compilation_passed": false, "time": 0.00019693374633789062, "tests_passed": false, "error": "SyntaxError"}
{"code": "stop_words = stopwords.words()\n\n    # remove punctuation and digits from the text\n    cleaned_text = re.sub(r'[^\\w\\s]', '', text.replace('\\n', ' '))\n    cleaned_text = re.sub(r'[0-9]+', '', cleaned_text)\n\n    # remove stopwords and duplicate words\n    cleaned_words = cleaned_text.split()\n    cleaned_words = [word for word in cleaned_words if word not in stop_words]\n    cleaned_text = ' '.join(cleaned_words)\n    # replace consecutive words with single occurrence of word\n    cleaned_text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', cleaned_text)\n\n    cv = CountVectorizer(analyzer='word', ngram_range=(n, n))\n    bow = cv.fit_transform([cleaned_text])\n    df = pd.DataFrame(bow.toarray().flatten(), index=cv.get_feature_names_out(), columns=['count'])\n", "compilation_passed": true, "time": 0.006840229034423828, "tests_passed": false, "error": "TypeError"}
{"code": "# Replace stop words with blank space and remove duplicate consecutive words\n    text = ' '.join(re.sub(r'(^\\s+|\\s+$)|(\\s\\s+)|(,\\s+)|(\\s,)|(\\.\\s+)', ' ', text).split())\n\n    # Convert text to n-grams and remove duplicates\n    n_grams = set([w1 + \" \" + w2 for w1, w2 in zip(text.split(' ')[:-1], text.split(' ')[1:])])\n\n    # Create square co-occurrence matrix of words\n    stop_words = set(stopwords.words('english'))\n    matrix = np.zeros((len(n_grams), len(n_grams)), dtype=np.int)\n    n_gram_index = {}\n    i = 0\n\n    for n_gram in n_grams:\n        # Store the n_gram and its index in the dictionary\n        if n_gram not in stop_words:\n            n_gram_index[n_gram] = i\n            i += ", "compilation_passed": false, "time": 0.00021791458129882812, "tests_passed": false, "error": "SyntaxError"}
{"code": "# Remove punctuation marks\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n\n    # Remove duplicate consecutive words\n    text = re.sub(r'(\\b\\w+)\\s+\\1', r'\\1', text)\n\n    # Generate co-occurrence matrix\n    cv = CountVectorizer(ngram_range=(n, n))\n    ngrams = cv.build_ngrams([text])\n    X = cv.transform([text], ngrams=ngrams)\n    df = pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())\n    df = df.transpose()\n    df.columns = [0]\n    df = df[df[0] != 0].drop(columns=[0])\n    df = df.T\n    df = df[df.T.sum()", "compilation_passed": false, "time": 0.00024008750915527344, "tests_passed": false, "error": "SyntaxError"}
{"code": "# Converts the text to lowercase and applies regex to remove consecutive and special characters.\n    # Stopwords are also removed using nltk.corpus.\n    stop_words = set(stopwords.words('english'))\n    text = re.sub(r'\\.', ' ', text)\n    text = text.lower()\n    text = re.sub(r'\\W', ' ', text)\n    text_words = [word for word in text.split() if word not in stop_words]\n\n    # Create an empty count matrix.\n    cv = CountVectorizer(ngram_range=(n, n))\n\n    # Generate a list of unique n-grams from the text.\n    grams = cv.fit_transform(text_words)\n    grams_list = cv.get_feature_names_out()\n\n    # Initialize the matrix as a pandas DataFrame.\n    df = pd.DataFrame(data=np.zeros((len(grams_list), len(grams_list)), dtype=int),\n                      index=grams_list, columns=grams_list)\n\n    # Iter", "compilation_passed": true, "time": 0.0011057853698730469, "tests_passed": false, "error": "ValueError"}
{"code": "# Creating a list of stopwords using nltk\n    stop_words = stopwords.words('english')\n\n    # Preprocessing the text to remove special characters and convert to lower case\n    preproc_text = re.sub(r'[^\\w\\s]', '', text.lower())\n\n    # Splitting the text into words and removing stopwords and duplicates\n    words = list(set([w for w in preproc_text.split() if w not in stop_words]))\n\n    # Counting the number of occurrences of each word pair\n    co_occurrence = CountVectorizer(ngram_range=(n, n))\n    co_occurrence_matrix = co_occurrence.fit_transform(words)\n\n    # Converting the co-occurrence matrix to a pandas dataframe\n    df = pd.DataFrame(co_occurrence_matrix.toarray().flatten(), co_occurrence.get_feature_names_out(), index=co_occurrence.get_feature_names_out())\n\n    # Setting up the plot with custom parameters\n    ", "compilation_passed": true, "time": 0.0009238719940185547, "tests_passed": false, "error": "ValueError"}
{"code": "# Regex pattern to match any word that is followed by itself within the next 5 words\n    pattern = r\"\\b(\\w+)\\b(?=\\s\\1{1,}{}(?=\\s|$))\".format(n - 1)\n\n    # Replace consecutive duplicate words within the next 5 words\n    replaced_text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n\n    # Convert to lowercase and remove stop words\n    stop_words = set(stopwords.words('english'))\n    words = replaced_text.lower().split()\n    filtered_text = \" \".join([word for word in words if word not in stop_words])\n\n    # Generate co-occurrence matrix\n    count_vect = CountVectorizer(ngram_range=(n, n))\n    tf_matrix = count_vect.fit_transform([filtered_text])\n    co_occurrence_matrix = pd.DataFrame(tf_matrix.toarray(), columns=count_vect.get_feature_names_out())\n    co_occurrence_matrix", "compilation_passed": true, "time": 0.0007266998291015625, "tests_passed": false, "error": "KeyError"}
{"code": "text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n    text = re.sub('\\n', ' ', text)\n    text = re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', text)\n    text = re.sub(r'[^a-zA-Z?.!\" ]', ' ', text)\n    text = re.sub(' +', ' ', text)\n    text = re.sub(r'([?.!\"])', r'\\1 ', text)\n    text = text.lower()\n    text = text.strip()\n    text_words = text.split(' ')\n    text_words_unique = set(text_words)\n    text_stopwords = stopwords.words('english')\n    text_words_without_stopwords_repetitions = [word for word in text_words if word not in text_stopwords and text_words.count(word) < 2]\n    text_words_", "compilation_passed": true, "time": 0.0009548664093017578, "tests_passed": false, "error": "NameError"}
{"code": "# Tokenize text and remove duplicate consecutive words\n    text = re.sub(r\"(?<!\\w)\\b(?!\\w)\", \"\", text)\n    text_list = text.split()\n    stop_words = set(stopwords.words('english'))\n    text_list_no_repeats = [text_list[i] for i in range(len(text_list)) if i == 0 or text_list[i] != text_list[i - 1] or text_list[i] in stop_words]\n\n    # Convert list of text to list of n-grams\n    text_ngram_list = [\" \".join(text_list_no_repeats[i:i+n]) for i in range(len(text_list_no_repeats) - n + 1)]\n    # Generate square co-occurrence matrix using scikit-learn's CountVectorizer\n    vectorizer = CountVectorizer(ngram_range=(n, n), dtype=np.float64)\n    cooccurrence_matrix", "compilation_passed": true, "time": 0.0008900165557861328, "tests_passed": false, "error": "NameError"}
{"code": "if text is None or text.strip() == \"\":\n        return None\n    # Regex pattern to match alphanumeric characters, underscore and hyphen.\n    pattern = r'[\\W]+'\n\n    # Removing the matches with empty spaces.\n    text = re.sub(pattern, ' ', text)\n    # Remove duplicate consecutive words from the text\n    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n\n    # Remove stopwords from the text\n    stop_words = set(stopwords.words('english'))\n    word_list = [word for word in text.split() if word not in stop_words]\n\n    # Create square co-occurrence matrix for n-grams\n    cooc_matrix = {}\n    for i in range(len(word_list)):\n        for j in range(i, len(word_list)):\n            if abs(i - j) <= n:\n                w1 = word_list[i]\n                w2 = word_list[j]\n                if (w1, w", "compilation_passed": false, "time": 0.0002560615539550781, "tests_passed": false, "error": "SyntaxError"}
{"code": "# Clean the text from symbols and split on spaces\n    words = re.sub(r'[^\\w\\s]', '', text).split()\n    stop_words = set(stopwords.words('english'))  # Load stop words from nltk.corpus\n    # Remove stop words and duplicates\n    words = [word for i, word in enumerate(words) if word not in stop_words and words.index(word) == i]\n    # Generate square co-occurrence matrix of words\n    cv = CountVectorizer(ngram_range=(n, n))\n    word_count_vector = cv.fit_transform(words)\n    co_occurrence_matrix = pd.DataFrame(word_count_vector.toarray(), columns=cv.get_feature_names_out())\n    # Plot the co-occurrence matrix\n    fig, ax = plt.subplots(figsize=(8, 8))\n    im = ax.imshow(co_occurrence_matrix.to_numpy())\n    # Add tick labels to the plot\n    ax.set_xticks(np", "compilation_passed": false, "time": 0.0002009868621826172, "tests_passed": false, "error": "SyntaxError"}
