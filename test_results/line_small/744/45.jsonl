{"code": " nltk.word_tokenize(text)", "compilation_passed": true, "time": 0.013612031936645508, "tests_passed": false, "error": ""}
{"code": " nltk.tokenize.word_tokenize(text)", "compilation_passed": true, "time": 0.011291027069091797, "tests_passed": false, "error": ""}
{"code": " nltk.tokenize.wordpunct_tokenize(text)", "compilation_passed": true, "time": 0.010328054428100586, "tests_passed": false, "error": "AttributeError"}
{"code": " nltk.tokenize.RegexpTokenizer('\\w+')", "compilation_passed": true, "time": 0.008099079132080078, "tests_passed": true, "error": null}
{"code": " nltk.Tokenizer()", "compilation_passed": true, "time": 0.012419939041137695, "tests_passed": false, "error": "AttributeError"}
{"code": " nltk.wordpunct_tokenize(text)", "compilation_passed": true, "time": 0.010519027709960938, "tests_passed": false, "error": "AttributeError"}
{"code": " nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text)", "compilation_passed": true, "time": 0.011897087097167969, "tests_passed": false, "error": "AttributeError"}
{"code": " nltk.tokenize.WordPunctTokenizer()", "compilation_passed": true, "time": 0.007786989212036133, "tests_passed": true, "error": null}
