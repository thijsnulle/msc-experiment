{"code": " TfidfVectorizer()", "compilation_passed": true, "time": 0.02372002601623535, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(stop_words='english')", "compilation_passed": true, "time": 0.022463083267211914, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(max_df=1.0, stop_words=STOPWORDS)", "compilation_passed": true, "time": 0.022895097732543945, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(analyzer='char', tokenizer=lambda s: s.split(),", "compilation_passed": false, "time": 0.0003199577331542969, "tests_passed": false, "error": "SyntaxError"}
{"code": " TfidfVectorizer(max_features=10000)", "compilation_passed": true, "time": 0.021617889404296875, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(stop_words='english', token_pattern='[^\\W\\d]+')", "compilation_passed": true, "time": 0.022715330123901367, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(max_features=30000)", "compilation_passed": true, "time": 0.02169966697692871, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(tokenizer=lambda x: x.split())", "compilation_passed": true, "time": 0.0217440128326416, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(analyzer='word', lowercase=False, token_pattern=' ')", "compilation_passed": true, "time": 0.007949113845825195, "tests_passed": true, "error": null}
