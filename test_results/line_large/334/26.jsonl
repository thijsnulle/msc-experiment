{"code": " TfidfVectorizer()", "compilation_passed": true, "time": 0.009713888168334961, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(lowercase=True)", "compilation_passed": true, "time": 0.00883793830871582, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(tokenizer=word_tokenize)", "compilation_passed": true, "time": 0.009409904479980469, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(tokenizer=word_tokenize, lowercase=True)", "compilation_passed": true, "time": 0.009571075439453125, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(lowercase=True, stop_words=None, token_pattern='[A-Za-z]+', ngram_range=(1, 1), max_df=1.0, min_df=1)", "compilation_passed": true, "time": 0.00865793228149414, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(stop_words='english')", "compilation_passed": true, "time": 0.008310079574584961, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(tokenizer=word_tokenize, dtype=float)", "compilation_passed": true, "time": 0.010119915008544922, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(lowercase=True, stop_words=None, tokenizer=word_tokenize)", "compilation_passed": true, "time": 0.009495019912719727, "tests_passed": true, "error": null}
{"code": " TfidfVectorizer(sublinear_tf=True)", "compilation_passed": true, "time": 0.009341001510620117, "tests_passed": true, "error": null}
